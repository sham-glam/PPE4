{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c87f6f-2996-401e-8543-b682d17c7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import utils_TP4 as utils\n",
    "import solution_TP4 as solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b41f198-bf2d-4fce-8911-12fd49d6f0ee",
   "metadata": {},
   "source": [
    "# Utilisation de Dataset (en gardant les anciens modèle et TextVectorizationLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d35a66-f44e-4517-b0b8-a3f680c5eb32",
   "metadata": {},
   "source": [
    "### Récupération et affichage d'une instance dans le corpus pour vérification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52859290-c295-4b4c-9222-150641025670",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_valid = keras.utils.text_dataset_from_directory(\n",
    "    \"Corpus\",\n",
    "    seed=42,\n",
    "    validation_split=0.3,\n",
    "    subset='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f7fa9-e0ac-405e-b6ed-d2bc693f838e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3931b5b7-dc5c-4c9a-b3b2-6d1bd8cba143",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_elem = ds_train.unbatch().take(1).get_single_element()\n",
    "un_elem   # equivalent of tst_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b491a3d0-de75-4290-8ca1-8ad51cef618d",
   "metadata": {},
   "source": [
    "### Vectorisation du corpus\n",
    "\n",
    "On `adapt()` le text_vectorizer en laissant de côté les `y` avec la fonction lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b8d196-3147-4285-89ac-cd25a610311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = solution.get_text_vectorizer_from_config(solution.ExpeConfig(\"whitespace\",None,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305a0e93-ad5e-4a1f-aa11-e414cfc0e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv.adapt(ds_train.map(lambda x,y: x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016f8c13-4615-4bfc-9494-b6e2d870ba4e",
   "metadata": {},
   "source": [
    "#### Vérification des structures de données obtenues (on vérifie les types et les shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40704ba0-ca58-4385-abf7-23b7679317f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train.map(lambda x,y: (tv(x),y)).take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96588e-67d6-4d07-9b2f-685cc2211eba",
   "metadata": {},
   "source": [
    "### Création et entraînement du modèle\n",
    "(et fonction de preprocessing qui peut remplacer la lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed752a7-9b84-432d-8ef3-6eb1f96ba6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = utils.PerceptronModelSparseCategorical(tv, list(range(7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead739d-47ea-40c0-a56a-2b29dd462e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(x,y):\n",
    "    return tv(x),y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91705c9-9c66-4729-be60-4f43313c6917",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(ds_train.map(preproc), validation_data=ds_valid.map(preproc), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf0ab8-2e47-45fc-acb1-c9f3bc116746",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv.vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85d284-5136-4297-93fb-ef2d137bc2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5169c2b-bef2-4b00-a10d-7b03b51444c6",
   "metadata": {},
   "source": [
    "# Utilisation de plongements (Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c49fed-05a6-4eaf-b041-d5b4de8fa430",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_int = text_vectorizer = keras.layers.TextVectorization(\n",
    "    max_tokens=3000, # taille du vocabulaire conservé\n",
    "    output_sequence_length=100, # taille des séquences (tronquées ou en ajoutant du padding)\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"whitespace\",\n",
    "    ngrams=None,\n",
    "    output_mode=\"int\") # changement : \"int\" au lieu de \"count\" pour un encodage un token -> un entier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac09df1c-b122-4d22-97dd-0a4faeffe885",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_int.adapt(ds_train.map(lambda x,y:x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b41bf-9324-41b3-817f-5179759d1fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_x = ds_train.unbatch().map(lambda x,y:x).map(tv_int).take(1).get_single_element()\n",
    "one_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511d871c-c121-4207-bbfe-05a40cb14a40",
   "metadata": {},
   "source": [
    "### On peut vérifier qu'on est capable de réencoder un document pour voir si tout se passe comme prévu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac71808-3f6d-4ceb-b705-5fb044329a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tv_int.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78102d24-4f91-47f8-86b3-183a4390e530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[vocab[i] for i in one_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7214b3a-780d-4219-8bfe-7e2f98a07584",
   "metadata": {},
   "source": [
    "# Une couche d'embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d025b6f-79e1-4138-9073-43906ac79057",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings = keras.layers.Embedding(\n",
    "    tv_int.vocabulary_size(),\n",
    "    3, # longueur des vecteurs\n",
    "    mask_zero=True # important si padding\n",
    ")\n",
    "embeddings(one_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c95782-c1c3-4f1d-92fa-e9a86b3eb7b1",
   "metadata": {},
   "source": [
    "# Un modèle avec une couche d'embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1ef4f5-ccd0-4694-861a-e18598b232ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(tv, emb_dim, nb_classes):\n",
    "    inputs = keras.layers.Input(shape=(100,))\n",
    "    embeddings = keras.layers.Embedding(\n",
    "        tv.vocabulary_size(),\n",
    "        emb_dim,\n",
    "        mask_zero=True,\n",
    "        name=\"emb\"\n",
    "    )(inputs)\n",
    "    embeddings = keras.layers.Dropout(rate=0.2)(embeddings)\n",
    "    pooling = keras.layers.GlobalMaxPooling1D()(embeddings)\n",
    "    classif = keras.layers.Dense(nb_classes, activation=\"softmax\", use_bias=True)(pooling)\n",
    "    model = keras.Model(inputs=inputs, outputs=classif)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e749c5-a78d-48e7-aa21-22f8797c5123",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(tv_int, 300, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e13a2b0-c0fb-4acb-a87f-5fae3e9c95a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4d97c-fa5a-43c9-9d4f-ffd148054353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_int(x,y):\n",
    "    return tv_int(x),y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b8bd4-6160-4b6a-abec-4bbd1e4b338b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(ds_train.map(preproc_int),  validation_data=ds_valid.map(preproc_int), epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809aa427-630f-4cf3-97e9-cdd85cd31dd1",
   "metadata": {},
   "source": [
    "# visualisation\n",
    "\n",
    "création de fichiers tsv prêts à être chargés sur https://projector.tensorflow.org/ \n",
    "(cf le code dans utils_TP5.py pour l'extraction des poids qui correspondent aux vecteurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da5f852-90d9-432e-9bec-bd51db5dac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_TP5 import write_vectors_proj_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0ed69-8eed-4319-82c7-0f740f307de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vectors_proj_format(model, tv_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274a7da6-fca5-479a-b085-4b669b1ff17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf551cdc-d12e-45cb-abb3-7c3714a35543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397eddf3-5ef9-4f52-9fd2-57efe1d67c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14ec7234-c973-4842-8392-1f4f736eefa0",
   "metadata": {},
   "source": [
    "# Debut TP 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a583a-3c1f-4aac-90f0-bf31675d32c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TP 7 \n",
    "# Corpus avec étiquette morphosyntaxique\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import TextLineDataset\n",
    "\n",
    "ds=TextLineDataset(\"aij-wikiner-fr-wp2\")\n",
    "\n",
    "ds  # entire dataset object  with token|POS tag | BIO tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2668f9-7234-4473-98ae-bef492eb8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_ds = ds.skip(1).take(1).get_single_element()\n",
    "tst_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489db6a6-10be-4d59-8fcd-e8d84c5e4e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_split(tensor):\n",
    "    # return tf.strings.split(tensor, sep=\"|\", maxsplit=-1, name=None)\n",
    "    t = tf.strings.split(tensor)\n",
    "    # return t\n",
    "    X,y = tf.strings.split(t, sep=\"|\", maxsplit=-1, name=None)[:, :1], tf.strings.split(t, sep=\"|\", maxsplit=-1, name=None)[:, 1:2]\n",
    "    # print(X)\n",
    "    # print(y)\n",
    "    return X,y\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9973c8-7a22-4d98-96ad-1dda9637c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y  # ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ac120-9034-4479-8506-a9da63302efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste = [ ]\n",
    "l1= []\n",
    "for item in ds:\n",
    "    l1.append(item)\n",
    "\n",
    "print(f'(Taille du corpus : {len(l1)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264e4c9-768c-4c46-adb1-1baf0fd7678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste[:5]\n",
    "l1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38cc032-b59c-4991-af28-d8300f418e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# je prends les 1000 premiers sinon hyper long\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "X_valid=[]\n",
    "y_valid=[]\n",
    "\n",
    "def create_tensors(datalist):\n",
    "    for instance in datalist:    \n",
    "        X_ds, y_ds = tensor_split(item)\n",
    "        X.extend(X_ds)\n",
    "        y.extend(y_ds)\n",
    "\n",
    "        X_tensor = tf.convert_to_tensor(X)\n",
    "        y_tensor = tf.convert_to_tensor(y)\n",
    "\n",
    "\n",
    "    return X_tensor, y_tensor\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209becee-1f1b-42f4-a0fd-5a9103f05e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = create_tensors(l1[:1000])\n",
    "X_valid, y_valid = create_tensors(l1[1000:1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc74c0e-4932-4144-8bf7-a7c16389a8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc267d-28ed-403b-ab37-121459589677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vérif\n",
    "for x_sample, y_sample in zip(X_train, y_train):\n",
    "    print(x_sample, y_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1cbc4a-b5a1-4559-adf3-125c56055cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X)), print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c960c73e-f268-411c-ba1c-5ba8a3da82e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469e68f5-dabd-4eee-ad3e-228a0443c060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy code\n",
    "\n",
    "# ds_train, ds_valid = keras.utils.text_dataset_from_directory(\n",
    "#     \"Corpus\",\n",
    "#     seed=42,\n",
    "#     validation_split=0.3,\n",
    "#     subset='both')\n",
    "\n",
    "tv = solution.get_text_vectorizer_from_config(solution.ExpeConfig(\"whitespace\",None,1000))\n",
    "# tv.adapt(ds_train.map(lambda x,y: x))\n",
    "tv.adapt(X_tensor)\n",
    "# ds_train.map(lambda x,y: (tv(x),y)).take(1).get_single_element()\n",
    "\n",
    "# model = utils.PerceptronModelSparseCategorical(tv, list(range(7)))\n",
    "\n",
    "# def preproc(x,y):\n",
    "#     return tv(x),y\n",
    "\n",
    "\n",
    "# model.fit(ds_train.map(preproc), validation_data=ds_valid.map(preproc), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617bc6a8-435f-45cc-bfcb-8da925175c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv.vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4bc1cc-1a86-4dd5-8b95-b9f9742811a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = utils.PerceptronModelSparseCategorical(tv, list(range(7)))\n",
    "\n",
    "def preproc(x,y):\n",
    "    return tv(x),y\n",
    "\n",
    "\n",
    "model.fit(X_tensor.map(preproc), validation_data=ds_valid.map(preproc), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80f1828-849f-4c30-8399-7c754d165398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
