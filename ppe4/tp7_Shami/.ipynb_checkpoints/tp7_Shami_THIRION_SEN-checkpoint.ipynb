{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc45bb23-862b-4812-92b2-a57b1b2da9a7",
   "metadata": {},
   "source": [
    "# TP 7 - preparation des données annotées \n",
    "### regarder « Début TP 7 - cellule 25 »\n",
    "- P.S. J'ai extrait que les nom et les étiquettes (en bas du notebook) , je continuerai avec l'entraînement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c87f6f-2996-401e-8543-b682d17c7ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 15:25:19.503027: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-26 15:25:19.555863: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-26 15:25:19.555890: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-26 15:25:19.556847: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-26 15:25:19.562343: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-26 15:25:19.562717: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-26 15:25:20.421044: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import utils_TP4 as utils\n",
    "import solution_TP4 as solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b41f198-bf2d-4fce-8911-12fd49d6f0ee",
   "metadata": {},
   "source": [
    "# Utilisation de Dataset (en gardant les anciens modèle et TextVectorizationLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d35a66-f44e-4517-b0b8-a3f680c5eb32",
   "metadata": {},
   "source": [
    "### Récupération et affichage d'une instance dans le corpus pour vérification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52859290-c295-4b4c-9222-150641025670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3498 files belonging to 7 classes.\n",
      "Using 2449 files for training.\n",
      "Using 1049 files for validation.\n"
     ]
    }
   ],
   "source": [
    "ds_train, ds_valid = keras.utils.text_dataset_from_directory(\n",
    "    \"Corpus\",\n",
    "    seed=42,\n",
    "    validation_split=0.3,\n",
    "    subset='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f7fa9-e0ac-405e-b6ed-d2bc693f838e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3931b5b7-dc5c-4c9a-b3b2-6d1bd8cba143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=string, numpy=b\"   Les Combattants est un film fran\\xc3\\xa7ais r\\xc3\\xa9alis\\xc3\\xa9 par Thomas Cailley , sorti\\n   le 20 ao\\xc3\\xbbt 2014 .\\n   R\\xc3\\xa9alisation Sc\\xc3\\xa9nario Acteurs principaux Pays de production Genre Dur\\xc3\\xa9e\\n   Sortie\\n\\n   Les Combattants est un film fran\\xc3\\xa7ais r\\xc3\\xa9alis\\xc3\\xa9 par Thomas Cailley , sorti\\n   le 20 ao\\xc3\\xbbt 2014 .\\n   R\\xc3\\xa9alisation Sc\\xc3\\xa9nario Acteurs principaux Pays de production Genre Dur\\xc3\\xa9e\\n   Sortie\\n\\n   Les Combattants d'Afrique est une exposition temporaire qui s'est\\n   d\\xc3\\xa9roul\\xc3\\xa9e du 15 juin 2010 au 31 octobre 2010 au Centre national\\n   Jean-Moulin de Bordeaux . Elle s'inscrit dans le cadre de la\\n   comm\\xc3\\xa9moration du 70 ^e anniversaire de l' Appel du 18 Juin 1940 par le\\n   g\\xc3\\xa9n\\xc3\\xa9ral de Gaulle , et du Cinquantenaire des Ind\\xc3\\xa9pendances et des\\n   actions engag\\xc3\\xa9es par la municipalit\\xc3\\xa9 de Bordeaux en direction des\\n   anciens combattants d'Afrique. L'exposition est pr\\xc3\\xa9sent\\xc3\\xa9e comme un\\n   hommage de la ville de Bordeaux, du Centre Jean-Moulin et du Minist\\xc3\\xa8re\\n   de la D\\xc3\\xa9fense aux anciens combattants d'Afrique.\\n   Type Pays Localisation Commissaire Date d'ouverture Date de cl\\xc3\\xb4ture\\n   Organisateur(s)\\n\\n   Les Combattants de Serkos est un roman de science-fiction d' Andr\\xc3\\xa9\\n   Caroff , paru en 1978 .\\n   Auteur Pays Genre Version originale Langue Date de parution Version\\n   fran\\xc3\\xa7aise \\xc3\\x89diteur Collection Date de parution\\n\\n\">,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=6>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "un_elem = ds_train.unbatch().take(1).get_single_element()\n",
    "un_elem   # equivalent of tst_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b491a3d0-de75-4290-8ca1-8ad51cef618d",
   "metadata": {},
   "source": [
    "### Vectorisation du corpus\n",
    "\n",
    "On `adapt()` le text_vectorizer en laissant de côté les `y` avec la fonction lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01b8d196-3147-4285-89ac-cd25a610311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = solution.get_text_vectorizer_from_config(solution.ExpeConfig(\"whitespace\",None,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "305a0e93-ad5e-4a1f-aa11-e414cfc0e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv.adapt(ds_train.map(lambda x,y: x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016f8c13-4615-4bfc-9494-b6e2d870ba4e",
   "metadata": {},
   "source": [
    "#### Vérification des structures de données obtenues (on vérifie les types et les shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40704ba0-ca58-4385-abf7-23b7679317f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 1000), dtype=float32, numpy=\n",
       " array([[143.,  20.,  13., ...,   0.,   0.,   0.],\n",
       "        [  7.,   2.,   0., ...,   0.,   0.,   0.],\n",
       "        [111.,  12.,   8., ...,   0.,   0.,   0.],\n",
       "        ...,\n",
       "        [ 48.,  10.,   2., ...,   0.,   0.,   0.],\n",
       "        [ 21.,   6.,   3., ...,   0.,   0.,   0.],\n",
       "        [132.,  12.,   0., ...,   0.,   0.,   0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([4, 6, 4, 1, 5, 0, 0, 2, 1, 6, 1, 6, 6, 6, 6, 1, 0, 6, 2, 5, 1, 3,\n",
       "        6, 2, 4, 0, 2, 5, 5, 5, 6, 6], dtype=int32)>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.map(lambda x,y: (tv(x),y)).take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96588e-67d6-4d07-9b2f-685cc2211eba",
   "metadata": {},
   "source": [
    "### Création et entraînement du modèle\n",
    "(et fonction de preprocessing qui peut remplacer la lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ed752a7-9b84-432d-8ef3-6eb1f96ba6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = utils.PerceptronModelSparseCategorical(tv, list(range(7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eead739d-47ea-40c0-a56a-2b29dd462e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(x,y):\n",
    "    return tv(x),y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a91705c9-9c66-4729-be60-4f43313c6917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "77/77 [==============================] - 1s 5ms/step - loss: 1.3700 - accuracy: 0.6260 - val_loss: 0.8998 - val_accuracy: 0.8151\n",
      "Epoch 2/10\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.7127 - accuracy: 0.8571 - val_loss: 0.6581 - val_accuracy: 0.8503\n",
      "Epoch 3/10\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4938 - accuracy: 0.8881 - val_loss: 0.5418 - val_accuracy: 0.8761\n",
      "Epoch 4/10\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3697 - accuracy: 0.9122 - val_loss: 0.4952 - val_accuracy: 0.8856\n",
      "Epoch 5/10\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.3008 - accuracy: 0.9265 - val_loss: 0.4868 - val_accuracy: 0.8866\n",
      "Epoch 6/10\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2564 - accuracy: 0.9367 - val_loss: 0.4888 - val_accuracy: 0.8837\n",
      "Epoch 7/10\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2268 - accuracy: 0.9424 - val_loss: 0.4855 - val_accuracy: 0.8837\n",
      "Epoch 8/10\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1977 - accuracy: 0.9551 - val_loss: 0.4866 - val_accuracy: 0.8827\n",
      "Epoch 9/10\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1748 - accuracy: 0.9604 - val_loss: 0.4886 - val_accuracy: 0.8875\n",
      "Epoch 10/10\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1556 - accuracy: 0.9653 - val_loss: 0.4957 - val_accuracy: 0.8827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x721f4053b190>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ds_train.map(preproc), validation_data=ds_valid.map(preproc), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ddf0ab8-2e47-45fc-acb1-c9f3bc116746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method TextVectorization.vocabulary_size of <keras.src.layers.preprocessing.text_vectorization.TextVectorization object at 0x721f44bf0fd0>>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv.vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c85d284-5136-4297-93fb-ef2d137bc2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " tokens_count (InputLayer)   [(None, 1000)]            0         \n",
      "                                                                 \n",
      " normalizer (Normalization)  (None, 1000)              2001      \n",
      "                                                                 \n",
      " hidden (Dense)              (None, 14)                14014     \n",
      "                                                                 \n",
      " sortie (Dense)              (None, 7)                 105       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16120 (62.97 KB)\n",
      "Trainable params: 14119 (55.15 KB)\n",
      "Non-trainable params: 2001 (7.82 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5169c2b-bef2-4b00-a10d-7b03b51444c6",
   "metadata": {},
   "source": [
    "# Utilisation de plongements (Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93c49fed-05a6-4eaf-b041-d5b4de8fa430",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_int = text_vectorizer = keras.layers.TextVectorization(\n",
    "    max_tokens=3000, # taille du vocabulaire conservé\n",
    "    output_sequence_length=100, # taille des séquences (tronquées ou en ajoutant du padding)\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"whitespace\",\n",
    "    ngrams=None,\n",
    "    output_mode=\"int\") # changement : \"int\" au lieu de \"count\" pour un encodage un token -> un entier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac09df1c-b122-4d22-97dd-0a4faeffe885",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_int.adapt(ds_train.map(lambda x,y:x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc9b41bf-9324-41b3-817f-5179759d1fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
       "array([   5, 1953,   65,    2,  122,    6,    8,   65,    1,   72,  120,\n",
       "          5,   78,    6,    9,  219,  131,    4,    1,   92, 1905,    2,\n",
       "          1,  906,    3,  228,    1,   39,    3, 1899,    2,    1, 2077,\n",
       "          3,  237,  335,    5,   21,    2, 1953,   65,    2,  122,   12,\n",
       "        444,   11,   65,   63,    1,    9,    3,    1,    2,    3, 1899,\n",
       "          2,    1,    7,    9,    1,    2,    1,   85,  877,   48,  202,\n",
       "        750,  503, 1930,   78,   16,   62,  107,   36,   40,    1, 2950,\n",
       "         12,    1,   11, 1953,   65,    2,  122,   63,   12, 1324,    1,\n",
       "          2,  511,    1,   10,    5, 1953,   65,    2,  122,   73,  106,\n",
       "         85])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_x = ds_train.unbatch().map(lambda x,y:x).map(tv_int).take(1).get_single_element()\n",
    "one_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511d871c-c121-4207-bbfe-05a40cb14a40",
   "metadata": {},
   "source": [
    "### On peut vérifier qu'on est capable de réencoder un document pour voir si tout se passe comme prévu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fac71808-3f6d-4ceb-b705-5fb044329a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tv_int.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78102d24-4f91-47f8-86b3-183a4390e530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['le',\n",
       " 'yacht',\n",
       " 'club',\n",
       " 'de',\n",
       " 'france',\n",
       " 'est',\n",
       " 'un',\n",
       " 'club',\n",
       " '[UNK]',\n",
       " 'français',\n",
       " 'dont',\n",
       " 'le',\n",
       " 'siège',\n",
       " 'est',\n",
       " 'à',\n",
       " 'paris',\n",
       " 'créée',\n",
       " 'en',\n",
       " '[UNK]',\n",
       " 'sous',\n",
       " 'légide',\n",
       " 'de',\n",
       " '[UNK]',\n",
       " 'iii',\n",
       " 'la',\n",
       " 'société',\n",
       " '[UNK]',\n",
       " 'pour',\n",
       " 'la',\n",
       " 'navigation',\n",
       " 'de',\n",
       " '[UNK]',\n",
       " 'prend',\n",
       " 'la',\n",
       " 'même',\n",
       " 'année',\n",
       " 'le',\n",
       " 'titre',\n",
       " 'de',\n",
       " 'yacht',\n",
       " 'club',\n",
       " 'de',\n",
       " 'france',\n",
       " 'les',\n",
       " 'activités',\n",
       " 'du',\n",
       " 'club',\n",
       " 'sont',\n",
       " '[UNK]',\n",
       " 'à',\n",
       " 'la',\n",
       " '[UNK]',\n",
       " 'de',\n",
       " 'la',\n",
       " 'navigation',\n",
       " 'de',\n",
       " '[UNK]',\n",
       " 'et',\n",
       " 'à',\n",
       " '[UNK]',\n",
       " 'de',\n",
       " '[UNK]',\n",
       " 'fondation',\n",
       " 'sigle',\n",
       " 'type',\n",
       " 'forme',\n",
       " 'juridique',\n",
       " 'domaine',\n",
       " 'dactivité',\n",
       " 'siège',\n",
       " 'pays',\n",
       " 'coordonnées',\n",
       " 'président',\n",
       " 'site',\n",
       " 'web',\n",
       " '[UNK]',\n",
       " 'siren',\n",
       " 'les',\n",
       " '[UNK]',\n",
       " 'du',\n",
       " 'yacht',\n",
       " 'club',\n",
       " 'de',\n",
       " 'france',\n",
       " 'sont',\n",
       " 'les',\n",
       " 'différentes',\n",
       " '[UNK]',\n",
       " 'de',\n",
       " 'course',\n",
       " '[UNK]',\n",
       " 'par',\n",
       " 'le',\n",
       " 'yacht',\n",
       " 'club',\n",
       " 'de',\n",
       " 'france',\n",
       " 'depuis',\n",
       " 'sa',\n",
       " 'fondation']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[i] for i in one_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7214b3a-780d-4219-8bfe-7e2f98a07584",
   "metadata": {},
   "source": [
    "# Une couche d'embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d025b6f-79e1-4138-9073-43906ac79057",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 3), dtype=float32, numpy=\n",
       "array([[-0.01148372,  0.01190931, -0.04015596],\n",
       "       [ 0.0323239 ,  0.02460972, -0.01484894],\n",
       "       [ 0.00819312, -0.03362774,  0.02862711],\n",
       "       [ 0.04613415, -0.00540604,  0.03390976],\n",
       "       [-0.03456982, -0.03946806,  0.00779904],\n",
       "       [-0.01972048,  0.01023448, -0.04731094],\n",
       "       [ 0.00637228,  0.0467762 ,  0.03507707],\n",
       "       [ 0.00819312, -0.03362774,  0.02862711],\n",
       "       [-0.04955391, -0.02809662,  0.01035144],\n",
       "       [ 0.00612235, -0.00105518, -0.04868677],\n",
       "       [ 0.00056183,  0.04279524, -0.04607042],\n",
       "       [-0.01148372,  0.01190931, -0.04015596],\n",
       "       [ 0.03516417, -0.03554715, -0.0034003 ],\n",
       "       [-0.01972048,  0.01023448, -0.04731094],\n",
       "       [-0.00825735, -0.03048357, -0.03032469],\n",
       "       [-0.01103293,  0.04750839, -0.02430751],\n",
       "       [-0.04801531, -0.01972827,  0.04865581],\n",
       "       [-0.02868931,  0.00074333, -0.0248952 ],\n",
       "       [-0.04955391, -0.02809662,  0.01035144],\n",
       "       [-0.01114261, -0.00291865,  0.00793708],\n",
       "       [ 0.03063767,  0.04874828,  0.0406857 ],\n",
       "       [ 0.04613415, -0.00540604,  0.03390976],\n",
       "       [-0.04955391, -0.02809662,  0.01035144],\n",
       "       [ 0.0465047 ,  0.00865318, -0.03772582],\n",
       "       [ 0.02070942,  0.04497761,  0.01937712],\n",
       "       [-0.03332944,  0.03039921, -0.02855349],\n",
       "       [-0.04955391, -0.02809662,  0.01035144],\n",
       "       [ 0.0294687 ,  0.01604052,  0.01760009],\n",
       "       [ 0.02070942,  0.04497761,  0.01937712],\n",
       "       [ 0.03237916, -0.04240305,  0.04266826],\n",
       "       [ 0.04613415, -0.00540604,  0.03390976],\n",
       "       [-0.04955391, -0.02809662,  0.01035144],\n",
       "       [-0.00319514, -0.044619  ,  0.04873331],\n",
       "       [ 0.02070942,  0.04497761,  0.01937712],\n",
       "       [ 0.04148154, -0.01588888, -0.01234385],\n",
       "       [ 0.02422333, -0.00032488, -0.04586134],\n",
       "       [-0.01148372,  0.01190931, -0.04015596],\n",
       "       [-0.03717141,  0.02728565,  0.00015445],\n",
       "       [ 0.04613415, -0.00540604,  0.03390976],\n",
       "       [ 0.0323239 ,  0.02460972, -0.01484894],\n",
       "       [ 0.00819312, -0.03362774,  0.02862711],\n",
       "       [ 0.04613415, -0.00540604,  0.03390976],\n",
       "       [-0.03456982, -0.03946806,  0.00779904],\n",
       "       [ 0.00321136, -0.02538314,  0.01091949],\n",
       "       [ 0.0035299 ,  0.0152385 , -0.01502805],\n",
       "       [-0.014403  ,  0.02096591, -0.03823143],\n",
       "       [ 0.00819312, -0.03362774,  0.02862711],\n",
       "       [ 0.01786827,  0.04150604,  0.01527673],\n",
       "       [-0.04955391, -0.02809662,  0.01035144],\n",
       "       [-0.00825735, -0.03048357, -0.03032469],\n",
       "       [ 0.02070942,  0.04497761,  0.01937712],\n",
       "       [-0.04955391, -0.02809662,  0.01035144],\n",
       "       [ 0.04613415, -0.00540604,  0.03390976],\n",
       "       [ 0.02070942,  0.04497761,  0.01937712],\n",
       "       [ 0.03237916, -0.04240305,  0.04266826],\n",
       "       [ 0.04613415, -0.00540604,  0.03390976],\n",
       "       [-0.04955391, -0.02809662,  0.01035144],\n",
       "       [-0.01417664, -0.01916506,  0.00281687],\n",
       "       [-0.00825735, -0.03048357, -0.03032469],\n",
       "       [-0.04955391, -0.02809662,  0.01035144],\n",
       "       [ 0.04613415, -0.00540604,  0.03390976],\n",
       "       [-0.04955391, -0.02809662,  0.01035144],\n",
       "       [ 0.0064388 , -0.01281643,  0.01602585],\n",
       "       [ 0.04092507, -0.03454223, -0.04566924],\n",
       "       [ 0.03176136,  0.0256117 , -0.03837691],\n",
       "       [ 0.00855403, -0.00993161,  0.02659656],\n",
       "       [ 0.00908099,  0.02568891,  0.00415703],\n",
       "       [-0.03775292,  0.01991155,  0.01701082],\n",
       "       [-0.01914519,  0.04130517,  0.02853448],\n",
       "       [ 0.03516417, -0.03554715, -0.0034003 ],\n",
       "       [ 0.04751979, -0.01374707, -0.04306545],\n",
       "       [ 0.00865301,  0.04287937,  0.0440728 ],\n",
       "       [ 0.04376705,  0.02075562, -0.0333161 ],\n",
       "       [ 0.00296491,  0.04666385, -0.02268302],\n",
       "       [ 0.00913597, -0.04544365, -0.04030091],\n",
       "       [-0.04955391, -0.02809662,  0.01035144],\n",
       "       [ 0.0057606 , -0.00203941, -0.03667953],\n",
       "       [ 0.00321136, -0.02538314,  0.01091949],\n",
       "       [-0.04955391, -0.02809662,  0.01035144],\n",
       "       [-0.014403  ,  0.02096591, -0.03823143],\n",
       "       [ 0.0323239 ,  0.02460972, -0.01484894],\n",
       "       [ 0.00819312, -0.03362774,  0.02862711],\n",
       "       [ 0.04613415, -0.00540604,  0.03390976],\n",
       "       [-0.03456982, -0.03946806,  0.00779904],\n",
       "       [ 0.01786827,  0.04150604,  0.01527673],\n",
       "       [ 0.00321136, -0.02538314,  0.01091949],\n",
       "       [-0.04328123,  0.02202146, -0.00709643],\n",
       "       [-0.04955391, -0.02809662,  0.01035144],\n",
       "       [ 0.04613415, -0.00540604,  0.03390976],\n",
       "       [ 0.04133889,  0.00187879, -0.02201916],\n",
       "       [-0.04955391, -0.02809662,  0.01035144],\n",
       "       [ 0.00251329, -0.02111355, -0.04836694],\n",
       "       [-0.01148372,  0.01190931, -0.04015596],\n",
       "       [ 0.0323239 ,  0.02460972, -0.01484894],\n",
       "       [ 0.00819312, -0.03362774,  0.02862711],\n",
       "       [ 0.04613415, -0.00540604,  0.03390976],\n",
       "       [-0.03456982, -0.03946806,  0.00779904],\n",
       "       [-0.01266738, -0.04947863, -0.02824714],\n",
       "       [-0.03578011, -0.04341228,  0.00030907],\n",
       "       [ 0.0064388 , -0.01281643,  0.01602585]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = keras.layers.Embedding(\n",
    "    tv_int.vocabulary_size(),\n",
    "    3, # longueur des vecteurs\n",
    "    mask_zero=True # important si padding\n",
    ")\n",
    "embeddings(one_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c95782-c1c3-4f1d-92fa-e9a86b3eb7b1",
   "metadata": {},
   "source": [
    "# Un modèle avec une couche d'embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c1ef4f5-ccd0-4694-861a-e18598b232ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(tv, emb_dim, nb_classes):\n",
    "    inputs = keras.layers.Input(shape=(100,))\n",
    "    embeddings = keras.layers.Embedding(\n",
    "        tv.vocabulary_size(),\n",
    "        emb_dim,\n",
    "        mask_zero=True,\n",
    "        name=\"emb\"\n",
    "    )(inputs)\n",
    "    embeddings = keras.layers.Dropout(rate=0.2)(embeddings)\n",
    "    pooling = keras.layers.GlobalMaxPooling1D()(embeddings)\n",
    "    classif = keras.layers.Dense(nb_classes, activation=\"softmax\", use_bias=True)(pooling)\n",
    "    model = keras.Model(inputs=inputs, outputs=classif)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4e749c5-a78d-48e7-aa21-22f8797c5123",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(tv_int, 300, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e13a2b0-c0fb-4acb-a87f-5fae3e9c95a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " emb (Embedding)             (None, 100, 300)          900000    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 300)          0         \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 300)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 7)                 2107      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 902107 (3.44 MB)\n",
      "Trainable params: 902107 (3.44 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46b4d97c-fa5a-43c9-9d4f-ffd148054353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_int(x,y):\n",
    "    return tv_int(x),y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "676b8bd4-6160-4b6a-abec-4bbd1e4b338b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.fit(ds_train.map(preproc_int),  validation_data=ds_valid.map(preproc_int), epochs=10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809aa427-630f-4cf3-97e9-cdd85cd31dd1",
   "metadata": {},
   "source": [
    "# visualisation\n",
    "\n",
    "création de fichiers tsv prêts à être chargés sur https://projector.tensorflow.org/ \n",
    "(cf le code dans utils_TP5.py pour l'extraction des poids qui correspondent aux vecteurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7da5f852-90d9-432e-9bec-bd51db5dac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_TP5 import write_vectors_proj_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ad0ed69-8eed-4319-82c7-0f740f307de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vectors_proj_format(model, tv_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274a7da6-fca5-479a-b085-4b669b1ff17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf551cdc-d12e-45cb-abb3-7c3714a35543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397eddf3-5ef9-4f52-9fd2-57efe1d67c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14ec7234-c973-4842-8392-1f4f736eefa0",
   "metadata": {},
   "source": [
    "# Debut TP 7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a3a583a-3c1f-4aac-90f0-bf31675d32c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TextLineDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TP 7 \n",
    "# Corpus avec étiquette morphosyntaxique\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import TextLineDataset\n",
    "\n",
    "ds=TextLineDataset(\"aij-wikiner-fr-wp2\")\n",
    "\n",
    "ds  # entire dataset object  with token|POS tag | BIO tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d2668f9-7234-4473-98ae-bef492eb8b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b\"Il|PRO:PER|O assure|VER:pres|O \\xc3\\xa0|VER:pper|O la|DET:ART|O suite|NOM|O de|PRP|I-PER Saussure|NAM|I-PER le|DET:ART|O cours|NOM|O de|PRP|O grammaire|NOM|O compar\\xc3\\xa9e|ADJ|O ,|PUN|O qu'|PRO:REL|O il|PRO:PER|O compl\\xc3\\xa8te|VER:subp|O \\xc3\\xa0|VER:pper|O partir|VER:infi|O de|PRP|O 1894|NUM|O par|PRP|O une|DET:ART|O conf\\xc3\\xa9rence|NOM|O sur|PRP|O l'|DET:ART|O iranien|ADJ|O .|SENT|O\">"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_ds = ds.skip(1).take(1).get_single_element()\n",
    "tst_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "489db6a6-10be-4d59-8fcd-e8d84c5e4e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_split(tensor):\n",
    "    # return tf.strings.split(tensor, sep=\"|\", maxsplit=-1, name=None)\n",
    "    t = tf.strings.split(tensor)\n",
    "    # return t\n",
    "    X,y = tf.strings.split(t, sep=\"|\", maxsplit=-1, name=None)[:, :1], tf.strings.split(t, sep=\"|\", maxsplit=-1, name=None)[:, 1:2]\n",
    "    \n",
    "    return X, y\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ac699d4-1c69-4c79-a9c2-a3a491e191d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_empty_tensors(x, y):\n",
    "    return tf.size(x) > 0 and tf.size(y) > 0\n",
    "\n",
    "# Apply the filter function before mapping\n",
    "# filtered_X_y = X_y.filter(filter_empty_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f9973c8-7a22-4d98-96ad-1dda9637c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y = ds.map(tensor_split)\n",
    "\n",
    "X_y = X_y.filter(filter_empty_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3380b4b-97ea-4786-a1ee-7b2e5565229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction du texte\n",
    "X_data = X_y.map(lambda x, y: x) # data \n",
    "y_labels = X_y.map(lambda x, y: y) # lables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9d19da2-a766-4589-b235-72420b27e839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (text): <tf.RaggedTensor [[b'Il'],\n",
      " [b'assure'],\n",
      " [b'\\xc3\\xa0'],\n",
      " [b'la'],\n",
      " [b'suite'],\n",
      " [b'de'],\n",
      " [b'Saussure'],\n",
      " [b'le'],\n",
      " [b'cours'],\n",
      " [b'de'],\n",
      " [b'grammaire'],\n",
      " [b'compar\\xc3\\xa9e'],\n",
      " [b','],\n",
      " [b\"qu'\"],\n",
      " [b'il'],\n",
      " [b'compl\\xc3\\xa8te'],\n",
      " [b'\\xc3\\xa0'],\n",
      " [b'partir'],\n",
      " [b'de'],\n",
      " [b'1894'],\n",
      " [b'par'],\n",
      " [b'une'],\n",
      " [b'conf\\xc3\\xa9rence'],\n",
      " [b'sur'],\n",
      " [b\"l'\"],\n",
      " [b'iranien'],\n",
      " [b'.']]>\n",
      "X (text): <tf.RaggedTensor [[b'En'],\n",
      " [b'1905'],\n",
      " [b','],\n",
      " [b'il'],\n",
      " [b'occupe'],\n",
      " [b'la'],\n",
      " [b'chaire'],\n",
      " [b'de'],\n",
      " [b'grammaire'],\n",
      " [b'compar\\xc3\\xa9e'],\n",
      " [b'au'],\n",
      " [b'Coll\\xc3\\xa8ge'],\n",
      " [b'de'],\n",
      " [b'France'],\n",
      " [b','],\n",
      " [b'o\\xc3\\xb9'],\n",
      " [b'il'],\n",
      " [b'consacre'],\n",
      " [b'ses'],\n",
      " [b'cours'],\n",
      " [b'\\xc3\\xa0'],\n",
      " [b\"l'\"],\n",
      " [b'histoire'],\n",
      " [b'et'],\n",
      " [b'\\xc3\\xa0'],\n",
      " [b'la'],\n",
      " [b'structure'],\n",
      " [b'des'],\n",
      " [b'langues'],\n",
      " [b'indo-europ\\xc3\\xa9ennes'],\n",
      " [b'.']]>\n",
      "X (text): <tf.RaggedTensor [[b'Il'],\n",
      " [b'a'],\n",
      " [b'form\\xc3\\xa9'],\n",
      " [b'toute'],\n",
      " [b'une'],\n",
      " [b'g\\xc3\\xa9n\\xc3\\xa9ration'],\n",
      " [b'de'],\n",
      " [b'linguistes'],\n",
      " [b'fran\\xc3\\xa7ais'],\n",
      " [b','],\n",
      " [b'parmi'],\n",
      " [b'lesquels'],\n",
      " [b'Emile'],\n",
      " [b'Benveniste'],\n",
      " [b','],\n",
      " [b'Marcel'],\n",
      " [b'Cohen'],\n",
      " [b','],\n",
      " [b'Georges'],\n",
      " [b'Dum\\xc3\\xa9zil'],\n",
      " [b','],\n",
      " [b'Andr\\xc3\\xa9'],\n",
      " [b'Martinet'],\n",
      " [b','],\n",
      " [b'Aur\\xc3\\xa9lien'],\n",
      " [b'Sauvageot'],\n",
      " [b','],\n",
      " [b'Lucien'],\n",
      " [b'Tesni\\xc3\\xa8re'],\n",
      " [b','],\n",
      " [b'Joseph'],\n",
      " [b'Vendryes'],\n",
      " [b'.']]>\n",
      "X (text): <tf.RaggedTensor [[b'Il'],\n",
      " [b'devait'],\n",
      " [b'diriger'],\n",
      " [b'la'],\n",
      " [b'th\\xc3\\xa8se'],\n",
      " [b'de'],\n",
      " [b'Jean'],\n",
      " [b'Paulhan'],\n",
      " [b'sur'],\n",
      " [b'la'],\n",
      " [b's\\xc3\\xa9mantique'],\n",
      " [b'du'],\n",
      " [b'proverbe'],\n",
      " [b'et'],\n",
      " [b\"c'\"],\n",
      " [b'est'],\n",
      " [b'lui'],\n",
      " [b'qui'],\n",
      " [b'd\\xc3\\xa9couvrit'],\n",
      " [b'Gustave'],\n",
      " [b'Guillaume'],\n",
      " [b'.']]>\n",
      "X (text): <tf.RaggedTensor [[b'Il'],\n",
      " [b'est'],\n",
      " [b'notamment'],\n",
      " [b\"l'\"],\n",
      " [b'inspirateur'],\n",
      " [b'de'],\n",
      " [b'la'],\n",
      " [b'd\\xc3\\xa9finition'],\n",
      " [b'de'],\n",
      " [b'la'],\n",
      " [b'phrase'],\n",
      " [b'adopt\\xc3\\xa9e'],\n",
      " [b'par'],\n",
      " [b'le'],\n",
      " [b'linguiste'],\n",
      " [b'am\\xc3\\xa9ricain'],\n",
      " [b'Leonard'],\n",
      " [b'Bloomfield'],\n",
      " [b'.']]>\n"
     ]
    }
   ],
   "source": [
    "for X in X_data.take(5):  # first 5 text elements\n",
    "    print(\"X (text):\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d088fc8f-d626-4f45-b7ae-5161a49de7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels (text): <tf.RaggedTensor [[b'PRO:PER'],\n",
      " [b'VER:pres'],\n",
      " [b'VER:pper'],\n",
      " [b'DET:ART'],\n",
      " [b'NOM'],\n",
      " [b'PRP'],\n",
      " [b'NAM'],\n",
      " [b'DET:ART'],\n",
      " [b'NOM'],\n",
      " [b'PRP'],\n",
      " [b'NOM'],\n",
      " [b'ADJ'],\n",
      " [b'PUN'],\n",
      " [b'PRO:REL'],\n",
      " [b'PRO:PER'],\n",
      " [b'VER:subp'],\n",
      " [b'VER:pper'],\n",
      " [b'VER:infi'],\n",
      " [b'PRP'],\n",
      " [b'NUM'],\n",
      " [b'PRP'],\n",
      " [b'DET:ART'],\n",
      " [b'NOM'],\n",
      " [b'PRP'],\n",
      " [b'DET:ART'],\n",
      " [b'ADJ'],\n",
      " [b'SENT']]>\n",
      "Labels (text): <tf.RaggedTensor [[b'PRP'],\n",
      " [b'NUM'],\n",
      " [b'PUN'],\n",
      " [b'PRO:PER'],\n",
      " [b'VER:pres'],\n",
      " [b'DET:ART'],\n",
      " [b'NOM'],\n",
      " [b'PRP'],\n",
      " [b'NOM'],\n",
      " [b'ADJ'],\n",
      " [b'PRP:det'],\n",
      " [b'NAM'],\n",
      " [b'PRP'],\n",
      " [b'NAM'],\n",
      " [b'PUN'],\n",
      " [b'VER:pper'],\n",
      " [b'PRO:PER'],\n",
      " [b'VER:pres'],\n",
      " [b'DET:POS'],\n",
      " [b'NOM'],\n",
      " [b'VER:pper'],\n",
      " [b'DET:ART'],\n",
      " [b'NOM'],\n",
      " [b'KON'],\n",
      " [b'NOM'],\n",
      " [b'DET:ART'],\n",
      " [b'NOM'],\n",
      " [b'PRP:det'],\n",
      " [b'NOM'],\n",
      " [b'ADJ'],\n",
      " [b'SENT']]>\n",
      "Labels (text): <tf.RaggedTensor [[b'PRO:PER'],\n",
      " [b'VER:pres'],\n",
      " [b'VER:pper'],\n",
      " [b'PRO:IND'],\n",
      " [b'DET:ART'],\n",
      " [b'NOM'],\n",
      " [b'PRP'],\n",
      " [b'NOM'],\n",
      " [b'ADJ'],\n",
      " [b'PUN'],\n",
      " [b'PRP'],\n",
      " [b'PRO:REL'],\n",
      " [b'NAM'],\n",
      " [b'NAM'],\n",
      " [b'PUN'],\n",
      " [b'NAM'],\n",
      " [b'NAM'],\n",
      " [b'PUN'],\n",
      " [b'NAM'],\n",
      " [b'NAM'],\n",
      " [b'PUN'],\n",
      " [b'NAM'],\n",
      " [b'NAM'],\n",
      " [b'PUN'],\n",
      " [b'NAM'],\n",
      " [b'NAM'],\n",
      " [b'PUN'],\n",
      " [b'NAM'],\n",
      " [b'NAM'],\n",
      " [b'PUN'],\n",
      " [b'NAM'],\n",
      " [b'NAM'],\n",
      " [b'SENT']]>\n",
      "Labels (text): <tf.RaggedTensor [[b'PRO:PER'],\n",
      " [b'VER:impf'],\n",
      " [b'VER:infi'],\n",
      " [b'DET:ART'],\n",
      " [b'NOM'],\n",
      " [b'PRP'],\n",
      " [b'NAM'],\n",
      " [b'NAM'],\n",
      " [b'PRP'],\n",
      " [b'DET:ART'],\n",
      " [b'NOM'],\n",
      " [b'PRP:det'],\n",
      " [b'NOM'],\n",
      " [b'KON'],\n",
      " [b'PRO:DEM'],\n",
      " [b'VER:pres'],\n",
      " [b'PRO:PER'],\n",
      " [b'PRO:REL'],\n",
      " [b'VER:simp'],\n",
      " [b'NAM'],\n",
      " [b'NAM'],\n",
      " [b'SENT']]>\n",
      "Labels (text): <tf.RaggedTensor [[b'PRO:PER'],\n",
      " [b'VER:pres'],\n",
      " [b'ADV'],\n",
      " [b'DET:ART'],\n",
      " [b'NOM'],\n",
      " [b'PRP'],\n",
      " [b'DET:ART'],\n",
      " [b'NOM'],\n",
      " [b'PRP'],\n",
      " [b'DET:ART'],\n",
      " [b'NOM'],\n",
      " [b'ADJ'],\n",
      " [b'PRP'],\n",
      " [b'DET:ART'],\n",
      " [b'NOM'],\n",
      " [b'ADJ'],\n",
      " [b'NAM'],\n",
      " [b'NAM'],\n",
      " [b'SENT']]>\n"
     ]
    }
   ],
   "source": [
    "for y in y_labels.take(5):  #  first 5 label elements\n",
    "    print(\"Labels (text):\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d4b631-5c2a-48c1-9735-145d6882a0c4",
   "metadata": {},
   "source": [
    "### A continuer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3dc74c0e-4932-4144-8bf7-a7c16389a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tv = solution.get_text_vectorizer_from_config(solution.ExpeConfig(\"whitespace\",None,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4bf687b-008b-4645-be8d-e4765af71d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preproc(x,y):\n",
    "#     return tv(x),y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0e26891-85f2-4576-9374-0a16846d600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = utils.PerceptronModelSparseCategorical(tv, list(range(7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31027d2-8ee4-4caa-ada7-9df4e0dca3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03f2901b-4ba5-470d-b9bf-4d5372a6ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tv.adapt(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "469e68f5-dabd-4eee-ad3e-228a0443c060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tv = solution.get_text_vectorizer_from_config(solution.ExpeConfig(\"whitespace\",None,1000))\n",
    "# tv.adapt(X_text)\n",
    "# ds_train.map(lambda x,y: (tv(x),y)).take(1).get_single_element()\n",
    "# X_y.map(lambda x,y: (tv(x),y)).take(1).get_single_element()\n",
    "# model = utils.PerceptronModelSparseCategorical(tv, list(range(7)))\n",
    "\n",
    "# def preproc(x,y):\n",
    "#     return tv(x),y(\n",
    "\n",
    "# model.fit(ds_train.map(preproc), validation_data=ds_valid.map(preproc), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617bc6a8-435f-45cc-bfcb-8da925175c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tv.vocabulary_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
