{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2362094b-9ef7-4eb9-88b1-797fae9aa3eb",
   "metadata": {},
   "source": [
    "# TP lemmatiseur et générateur en coréen\n",
    "`Shami THIRION SEN`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0924e06-9135-442b-9637-91a563a3a441",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Consignes:\n",
    "\n",
    "### 1. Données et Objectifs: <br>\n",
    "\n",
    "Dans ce TP, on va réaliser un lemmatiseur/segmenteur morphosyntaxique pour le\n",
    "coréen.\n",
    "- Les données sont celles issues de UD (corpus KAIST).\n",
    "- Celles-ci sont déjà divisées en train/valid/test.\n",
    "(Mais on pensera à limiter la taille du train pour développer le modèle !)\n",
    "- On pourra tester avec et sans normalisation+décomposition Unicode\n",
    "(paquet python unicodedata)\n",
    "- Il est conseillé d’extraire « manuellement » le vocabulaire.\n",
    "\n",
    "  ---\n",
    "\n",
    "### 2. Architectures neuronales:\n",
    "- Les architectures que l’on va explorer avec ce TP sont des ”sequence to sequence”, et\n",
    "plus précisement des Encodeur-Décodeur.\n",
    "- Encodeur : Un premier RNN ”encode” l’information lue et la résume en un unique\n",
    "vecteur (seq2vec)\n",
    "- Décodeur : Un second RNN ”décode” et génère la cible à partir\n",
    "- Plusieurs modèles à envisager. La partie encodeur ne change pas, mais ensuite la séquence générée par le décodeur peut l’être :\n",
    "    - **modèle basique** : uniquement à partir du vecteur encodé. Il faudra répéter celui-ci avec une couche RepeatVector.\n",
    "    - **modèle avancé** : on démarre avec le vecteur encodé, puis on poursuit en utilisant comme entrée du décodeur ce qui vient d’être généré par le décodeur à t-1\n",
    "- **Bonus** : On peut essayer de combiner les deux approche en combinant le vecteur encodé et le vecteur décodé à t-1, par concaténation (layers.Concatenate) ou somme (layers.Add)\n",
    "- **Bonus2**: Les fichiers UD fournissent aussi un étiquetage des morphèmes, on peut essayer de produire **une double sortie** : segmentation d’une part et étiquetage de l’autre.\n",
    "Attention : Pour pouvoir utiliser le modèle « avancé », il faudra fournir une classe Generator inspirée du TP “Brassens”, car contrairement à la phase d’entraînement, lors de l’utilisation (et d’un test rigoureux), la séquence produite n’est pas connue à priori.\n",
    "\n",
    "- Voir les graphes fournis qui décrivent les modèles à construire.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Tokenisation et Vectorisation\n",
    "Quelques consignes pour le TextVectorizer :\n",
    "- on tokenisera nous-même avec la fonction **tf.strings.unicodesplit**\n",
    "- on construira nous même la liste du vocabulaire de tous les caractères possibles en\n",
    "ajoutant les tokens [START] et [END]\n",
    "- on évitera les RaggedTensor en fixant la **longueur à 48 tokens** (il faudra tout de\n",
    "même faire attention au masking)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Construction des instances (X,Y)\n",
    "- Avec Keras, il possible de construire des données d’entraînement plus complexes qu’un simple couple de tenseur (X,Y).\n",
    "- On va pour cela fournir un **dictionnaire python** à la place du tenseur X.\n",
    "    - Les clefs du dictionnaire doivent porter le même nom que les couches ”Input Layer” du réseau, et les valeurs sont les tenseurs qui seront envoyées à ces entrées.\n",
    "    - Dans le cas présent, on va donc construire des dictionnaires avec les données suivantes :<br>\n",
    "    \n",
    "    **input_1** : mot-forme tel qu’observé dans le texte <br>\n",
    "    \n",
    "    **input_2** : mot-analysé qui arrivera en entrée du décodeur (donc décallé à droite en commençant par un token [START]\n",
    "    le Y sera simplement le mot analysé suivit du token [END]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c1ee0-09bf-48c1-bb8c-970ab32da801",
   "metadata": {},
   "source": [
    "### Import des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e116bafe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 08:40:36.111359: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-30 08:40:36.114986: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-30 08:40:36.159085: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-30 08:40:37.014842: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# %env TF_FORCE_GPU_ALLOW_GROWTH=true\n",
    "# %matplotlib widget\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.data import TextLineDataset\n",
    "from keras.layers import Input, LSTM, Dense, RepeatVector, Embedding, Bidirectional, Concatenate, Dropout, StringLookup, RNN, LayerNormalization\n",
    "from keras import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e9e732",
   "metadata": {},
   "source": [
    "### 1. Extraction des données\n",
    "- extraction du 1ère et 2ème colonnes\n",
    "- Nous allons d'abord expérimenter avec **1000** tokens, et ensuite avec 10000\n",
    "- Le résultat commence à être fiable qu'avec les **10000** tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c340ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_tokens, dev_morphs= read_csv_file('corpus_ko/dev.tsv', 0,1) # lecture colonnes 0 et 1\n",
    "test_tokens, test_morphs= read_csv_file('corpus_ko/test.tsv',0,1)\n",
    "train_tokens, train_morphs= read_csv_file('corpus_ko/train.tsv',0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cbd4def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=int32, numpy=25278>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=28366>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=296446>,\n",
       " 296446,\n",
       " list)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.size(dev_tokens), tf.size(test_tokens), tf.size(train_tokens), len(train_tokens), type(train_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02187e8-45f2-4148-8472-d47493db0456",
   "metadata": {},
   "source": [
    "### 1000 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f7d5be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1000 # finalise with train size 1000 5000 10000\n",
    "dev_size = int(train_size * 0.2)\n",
    "test_size = int(train_size * 0.2)\n",
    "\n",
    "train_X, train_Y = train_tokens[:train_size], train_morphs[:train_size]\n",
    "dev_X, dev_Y = dev_tokens[:dev_size], dev_morphs[:dev_size]\n",
    "test_X, test_Y = train_tokens[:test_size], train_morphs[:test_size]\n",
    "\n",
    "# extraction manuelle du vocabulaire à partir du corpus lemmatisé\n",
    "vocab = build_vocab(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b533f6f-1ff2-4aca-aa65-13735114af31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['하기야', '짐승도', '잘', '가르치기만', '하면', '어느', '정도는', '순치될', '수', '있다']\n",
      "['하기야', '짐승+도', '잘', '가르치+기+만', '하+면', '어느', '정도+는', '순치+되+ㄹ', '수', '있+다']\n"
     ]
    }
   ],
   "source": [
    "# Pour comparaison token et lemme\n",
    "print(test_X[:10])\n",
    "print(test_Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0231f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=398>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary size\n",
    "tf.size(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc38ec3",
   "metadata": {},
   "source": [
    "### 2. Creation des instances - Tokenisation et Vectorisation\n",
    " - fonctions dans ***utils.py***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0904cc4",
   "metadata": {},
   "source": [
    "#### Rappel\n",
    "Quelques consignes pour le TextVectorizer\n",
    "- on tokenisera nous-même avec la fonction **tf.strings.unicodesplit**\n",
    "- on construira nous même la liste du vocabulaire de tous les caractères possibles en ajoutant les tokens [START] et [END]\n",
    "- on évitera les RaggedTensor en fixant la longueur à 48 tokens (il faudra tout de même faire attention au masking) -> `mask_zero=True`\n",
    "- add extra tokens => `tf.constant(['[START]', '[END]']`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40db6140-a9f8-497c-bc8a-97240b7a89cb",
   "metadata": {},
   "source": [
    "#### Vectorization des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e716179",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 before split:  ['하기야', '짐승도']\n",
      "X1:  <tf.RaggedTensor [[b'\\xed\\x95\\x98', b'\\xea\\xb8\\xb0', b'\\xec\\x95\\xbc'],\n",
      " [b'\\xec\\xa7\\x90', b'\\xec\\x8a\\xb9', b'\\xeb\\x8f\\x84']]>\n",
      "\n",
      " X2 before split:  ['[START]하기야', '[START]짐승+도']\n",
      "\n",
      " X2:  <tf.RaggedTensor [[b'\\xed\\x95\\x98', b'\\xea\\xb8\\xb0', b'\\xec\\x95\\xbc'],\n",
      " [b'\\xec\\xa7\\x90', b'\\xec\\x8a\\xb9', b'+', b'\\xeb\\x8f\\x84']]>\n",
      "\n",
      "Y beofore split:  ['하기야[END]', '짐승+도[END]']\n",
      "\n",
      "Data Y <tf.RaggedTensor [[b'\\xed\\x95\\x98', b'\\xea\\xb8\\xb0', b'\\xec\\x95\\xbc'],\n",
      " [b'\\xec\\xa7\\x90', b'\\xec\\x8a\\xb9', b'+', b'\\xeb\\x8f\\x84']]>\n",
      "X1 before split:  ['내', '고향은']\n",
      "X1:  <tf.RaggedTensor [[b'\\xeb\\x82\\xb4'], [b'\\xea\\xb3\\xa0', b'\\xed\\x96\\xa5', b'\\xec\\x9d\\x80']]>\n",
      "\n",
      " X2 before split:  ['[START]내', '[START]고향+은']\n",
      "\n",
      " X2:  <tf.RaggedTensor [[b'\\xeb\\x82\\xb4'],\n",
      " [b'\\xea\\xb3\\xa0', b'\\xed\\x96\\xa5', b'+', b'\\xec\\x9d\\x80']]>\n",
      "\n",
      "Y beofore split:  ['내[END]', '고향+은[END]']\n",
      "\n",
      "Data Y <tf.RaggedTensor [[b'\\xeb\\x82\\xb4'],\n",
      " [b'\\xea\\xb3\\xa0', b'\\xed\\x96\\xa5', b'+', b'\\xec\\x9d\\x80']]>\n",
      "X1 before split:  ['하기야', '짐승도']\n",
      "X1:  <tf.RaggedTensor [[b'\\xed\\x95\\x98', b'\\xea\\xb8\\xb0', b'\\xec\\x95\\xbc'],\n",
      " [b'\\xec\\xa7\\x90', b'\\xec\\x8a\\xb9', b'\\xeb\\x8f\\x84']]>\n",
      "\n",
      " X2 before split:  ['[START]하기야', '[START]짐승+도']\n",
      "\n",
      " X2:  <tf.RaggedTensor [[b'\\xed\\x95\\x98', b'\\xea\\xb8\\xb0', b'\\xec\\x95\\xbc'],\n",
      " [b'\\xec\\xa7\\x90', b'\\xec\\x8a\\xb9', b'+', b'\\xeb\\x8f\\x84']]>\n",
      "\n",
      "Y beofore split:  ['하기야[END]', '짐승+도[END]']\n",
      "\n",
      "Data Y <tf.RaggedTensor [[b'\\xed\\x95\\x98', b'\\xea\\xb8\\xb0', b'\\xec\\x95\\xbc'],\n",
      " [b'\\xec\\xa7\\x90', b'\\xec\\x8a\\xb9', b'+', b'\\xeb\\x8f\\x84']]>\n"
     ]
    }
   ],
   "source": [
    "X1_train, X2_train, Y_train, vectorization = create_instances(train_X, train_Y,vocab)\n",
    "X1_dev, X2_dev, Y_dev, _ = create_instances(dev_X, dev_Y,vocab)\n",
    "X1_test, X2_test, Y_test, _ = create_instances(test_X, test_Y,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3f1f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(X1_train[5])\n",
    "# pprint(X2_train[5])\n",
    "# pprint(Y_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc82bd8",
   "metadata": {},
   "source": [
    "####   Formatter les données - modèle basique\n",
    "- préparation des données en entrée pour le modèle basique, sous le forme de dictionnaire <br>\n",
    "```x_train = {'input1': X1, 'intput_2': X2}   |    y_train = Y``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f522c77-f048-4972-9aef-4dc50c873336",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = {'input1': X1_train, 'input2': X2_train}\n",
    "y_train = Y_train\n",
    "\n",
    "x_dev = {'input1': X1_dev, 'input2': X2_dev}\n",
    "y_dev = Y_dev\n",
    "\n",
    "x_test = {'input1': X1_test, 'input2': X2_test}\n",
    "y_test = Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6617eb",
   "metadata": {},
   "source": [
    "### Basic model\n",
    "- les nom des input doivent être les mêmes que ceux du dictionnaire\n",
    "    - ici **'input1'** et **'input2'\n",
    "- On emploi Repeatvector du dernier état des encodeurs comme entrée du décodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e7cc251-6b6e-4091-b6ff-3935607c3ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_model():\n",
    "\n",
    "    # input1\n",
    "    input1 = Input(shape=(None,), name='input1')\n",
    "    emb1 = Embedding(vectorization.vocabulary_size(), 30, mask_zero=True, name='embedding1')(input1)\n",
    "    emb1 = Dropout(0.3)(emb1)\n",
    "    encoder_lstm1 = Bidirectional(LSTM(64, return_state=True, name='lstm1'))\n",
    "    encoder_outputs1, forward_h1, forward_c1, backward_h1, backward_c1 = encoder_lstm1(emb1)\n",
    "\n",
    "    # input2\n",
    "    input2 = Input(shape=(None,), name='input2')\n",
    "    emb2 = Embedding(vectorization.vocabulary_size(), 30, mask_zero=True, name='embedding2')(input2)\n",
    "    emb2 = Dropout(0.3)(emb2)\n",
    "    encoder_lstm2 = Bidirectional(LSTM(64, return_state=True, name='lstm2'))\n",
    "    encoder_outputs2, forward_h2, forward_c2, backward_h2, backward_c2 = encoder_lstm2(emb2)\n",
    "\n",
    "    # Concatenation des états\n",
    "    state_h = Concatenate()([forward_h1, backward_h1, forward_h2, backward_h2])\n",
    "    state_c = Concatenate()([forward_c1, backward_c1, forward_c2, backward_c2])\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # encoder_states` est l'état initial\n",
    "    decoder_inputs = RepeatVector(48)(state_h) # modele basique avec repeat 48\n",
    "    decoder_lstm = LSTM(256, return_sequences=True)\n",
    "    decoder_outputs = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(vectorization.vocabulary_size(), activation='softmax', name='output')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # définir the model\n",
    "    model = Model([input1, input2], decoder_outputs)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ac965aa-3631-4aec-838e-edc038739292",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = basic_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a177cdb-27fb-4d94-8ab2-51110651cfc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,000</span> │ input1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding2          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,000</span> │ input2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">48,640</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)]       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">48,640</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)]       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ repeat_vector       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ repeat_vector[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">102,800</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input1 (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input2 (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)  │     \u001b[38;5;34m12,000\u001b[0m │ input1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding2          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)  │     \u001b[38;5;34m12,000\u001b[0m │ input2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │     \u001b[38;5;34m48,640\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)]       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │     \u001b[38;5;34m48,640\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)]       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ bidirectional_1[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │                   │            │ bidirectional_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ repeat_vector       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mRepeatVector\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ bidirectional_1[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │                   │            │ bidirectional_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m525,312\u001b[0m │ repeat_vector[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m400\u001b[0m)   │    \u001b[38;5;34m102,800\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">749,392</span> (2.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m749,392\u001b[0m (2.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">749,392</span> (2.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m749,392\u001b[0m (2.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3973df0d-c3ff-482f-a3c1-bad97609d736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 120ms/step - accuracy: 0.8075 - loss: 3.6175 - val_accuracy: 0.9251 - val_loss: 0.5886\n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - accuracy: 0.9196 - loss: 0.5565 - val_accuracy: 0.9251 - val_loss: 0.5062\n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 0.9222 - loss: 0.4765 - val_accuracy: 0.9251 - val_loss: 0.4740\n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 126ms/step - accuracy: 0.9248 - loss: 0.4386 - val_accuracy: 0.9414 - val_loss: 0.4025\n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 125ms/step - accuracy: 0.9392 - loss: 0.3691 - val_accuracy: 0.9430 - val_loss: 0.3582\n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 136ms/step - accuracy: 0.9422 - loss: 0.3241 - val_accuracy: 0.9429 - val_loss: 0.3435\n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 131ms/step - accuracy: 0.9426 - loss: 0.3152 - val_accuracy: 0.9448 - val_loss: 0.3378\n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 123ms/step - accuracy: 0.9431 - loss: 0.3015 - val_accuracy: 0.9448 - val_loss: 0.3328\n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - accuracy: 0.9438 - loss: 0.2936 - val_accuracy: 0.9449 - val_loss: 0.3310\n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 122ms/step - accuracy: 0.9426 - loss: 0.2961 - val_accuracy: 0.9450 - val_loss: 0.3332\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, \n",
    "                    validation_data=(x_dev, y_dev),\n",
    "                    epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f814d-1407-4690-8a75-34c00d61a4c7",
   "metadata": {},
   "source": [
    "### Evaluation sur données test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "207672ec-71cb-4602-9947-acd595c99b4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9450 - loss: 0.2853\n",
      "Test Loss: 0.2887529134750366\n",
      "Test Accuracy: 0.9441667199134827\n"
     ]
    }
   ],
   "source": [
    "#evaluation sur des données test\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49465895-0804-4da8-aae6-b365a37c3518",
   "metadata": {},
   "source": [
    "### Regardons les séquences générées\n",
    "- création du dictionnaire **index_to_token - it2** et **token_to_index - t2i**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af7889a1-e4f2-41ff-a1fd-5a6024728c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vectorization.get_vocabulary()\n",
    "i2t = {i: tok for i, tok in enumerate(vocabulary)} # mappage des tokens et indice \n",
    "# print(i2t)\n",
    "\n",
    "t2i = {tok: i for i, tok in enumerate(vocabulary)}\n",
    "# print(t2i)\n",
    "\n",
    "# print(t2i.get('+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8519ef1a-4108-47aa-9ea0-69a08685b7dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step\n"
     ]
    }
   ],
   "source": [
    "## DECODE \n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b51b00-ef5d-4a68-882b-a22411b2b73e",
   "metadata": {},
   "source": [
    "#### Extraction des séquences prédites et comparaison avec séquences réelles\n",
    "- Avec peu de données 100, 1000 train tokens:\n",
    "    - beaucoup de répétition des mêmes caractères\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "483b407b-ef81-4934-82b7-d2e201fd8bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# J'ai pas trouvé la façon keras pour décoder... \n",
    "predicted_tokens = []\n",
    "\n",
    "for pred in predictions:\n",
    "    predicted = \"\"\n",
    "    for idx in pred:\n",
    "        idx=  tf.argmax(idx, axis=-1).numpy()\n",
    "        predicted+=i2t[idx] \n",
    "    predicted_tokens.append(predicted)\n",
    "\n",
    "# predicted_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75b4a427-1130-4817-9525-b8d9302bfef9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_tokens = [\"\".join(map(lambda idx: i2t[idx.numpy().astype(int) ] , test)) for test in y_test]\n",
    "# test_tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5987d01-82a1-4aaa-a716-e9f6f3023686",
   "metadata": {},
   "source": [
    "### Comparaison des tokens générés et réels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e185371-1f38-4467-84bf-75992de2b114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted =  ++\n",
      "real =  하기야 \n",
      "\n",
      "predicted =  +++\n",
      "real =  짐승+도 \n",
      "\n",
      "predicted =  .\n",
      "real =  잘 \n",
      "\n",
      "predicted =  그+++++\n",
      "real =  가르치+기+만 \n",
      "\n",
      "predicted =  +++\n",
      "real =  하+면 \n",
      "\n",
      "predicted =  +\n",
      "real =  어느 \n",
      "\n",
      "predicted =  +++\n",
      "real =  정도+는 \n",
      "\n",
      "predicted =  것++++\n",
      "real =  순치+되+ㄹ \n",
      "\n",
      "predicted =  .\n",
      "real =  수 \n",
      "\n",
      "predicted =  것+\n",
      "real =  있+다 \n",
      "\n",
      "Nombre de prédiction correcte: 55 \n",
      "Nombre de prediction fausse:  145\n"
     ]
    }
   ],
   "source": [
    "# intersection de predicted_tokens , test_tokens\n",
    "for p , t in zip(predicted_tokens[:10], test_tokens[:10]):\n",
    "    print(\"predicted = \", p)\n",
    "    print(\"real = \", t, \"\\n\")\n",
    "\n",
    "common = [x for x in predicted_tokens if x in test_tokens]\n",
    "print(\"Nombre de prédiction correcte:\",len(common), \"\\nNombre de prediction fausse: \", len(predicted_tokens)-len(common))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20007570-542c-47e9-8fd6-52b39e094f56",
   "metadata": {},
   "source": [
    "### Même entraînement - avec une augmentation des tokens à 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d01b181-e389-47cc-a9f8-f882c3e4d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 10000 # finalise with train size 1000 5000 10000\n",
    "dev_size = int(train_size * 0.2)\n",
    "test_size = int(train_size * 0.2)\n",
    "\n",
    "train_X, train_Y = train_tokens[:train_size], train_morphs[:train_size]\n",
    "dev_X, dev_Y = dev_tokens[:dev_size], dev_morphs[:dev_size]\n",
    "test_X, test_Y = train_tokens[:test_size], train_morphs[:test_size]\n",
    "\n",
    "# extraction manuelle du vocabulaire à partir du corpus lemmatisé\n",
    "vocab = build_vocab(train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72585c92-d503-442a-b71b-087ff0743d25",
   "metadata": {},
   "source": [
    "#### Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77aad66c-9399-4956-be18-764215f2cb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 before split:  ['하기야', '짐승도']\n",
      "X1:  <tf.RaggedTensor [[b'\\xed\\x95\\x98', b'\\xea\\xb8\\xb0', b'\\xec\\x95\\xbc'],\n",
      " [b'\\xec\\xa7\\x90', b'\\xec\\x8a\\xb9', b'\\xeb\\x8f\\x84']]>\n",
      "\n",
      " X2 before split:  ['[START]하기야', '[START]짐승+도']\n",
      "\n",
      " X2:  <tf.RaggedTensor [[b'\\xed\\x95\\x98', b'\\xea\\xb8\\xb0', b'\\xec\\x95\\xbc'],\n",
      " [b'\\xec\\xa7\\x90', b'\\xec\\x8a\\xb9', b'+', b'\\xeb\\x8f\\x84']]>\n",
      "\n",
      "Y beofore split:  ['하기야[END]', '짐승+도[END]']\n",
      "\n",
      "Data Y <tf.RaggedTensor [[b'\\xed\\x95\\x98', b'\\xea\\xb8\\xb0', b'\\xec\\x95\\xbc'],\n",
      " [b'\\xec\\xa7\\x90', b'\\xec\\x8a\\xb9', b'+', b'\\xeb\\x8f\\x84']]>\n",
      "X1 before split:  ['내', '고향은']\n",
      "X1:  <tf.RaggedTensor [[b'\\xeb\\x82\\xb4'], [b'\\xea\\xb3\\xa0', b'\\xed\\x96\\xa5', b'\\xec\\x9d\\x80']]>\n",
      "\n",
      " X2 before split:  ['[START]내', '[START]고향+은']\n",
      "\n",
      " X2:  <tf.RaggedTensor [[b'\\xeb\\x82\\xb4'],\n",
      " [b'\\xea\\xb3\\xa0', b'\\xed\\x96\\xa5', b'+', b'\\xec\\x9d\\x80']]>\n",
      "\n",
      "Y beofore split:  ['내[END]', '고향+은[END]']\n",
      "\n",
      "Data Y <tf.RaggedTensor [[b'\\xeb\\x82\\xb4'],\n",
      " [b'\\xea\\xb3\\xa0', b'\\xed\\x96\\xa5', b'+', b'\\xec\\x9d\\x80']]>\n",
      "X1 before split:  ['하기야', '짐승도']\n",
      "X1:  <tf.RaggedTensor [[b'\\xed\\x95\\x98', b'\\xea\\xb8\\xb0', b'\\xec\\x95\\xbc'],\n",
      " [b'\\xec\\xa7\\x90', b'\\xec\\x8a\\xb9', b'\\xeb\\x8f\\x84']]>\n",
      "\n",
      " X2 before split:  ['[START]하기야', '[START]짐승+도']\n",
      "\n",
      " X2:  <tf.RaggedTensor [[b'\\xed\\x95\\x98', b'\\xea\\xb8\\xb0', b'\\xec\\x95\\xbc'],\n",
      " [b'\\xec\\xa7\\x90', b'\\xec\\x8a\\xb9', b'+', b'\\xeb\\x8f\\x84']]>\n",
      "\n",
      "Y beofore split:  ['하기야[END]', '짐승+도[END]']\n",
      "\n",
      "Data Y <tf.RaggedTensor [[b'\\xed\\x95\\x98', b'\\xea\\xb8\\xb0', b'\\xec\\x95\\xbc'],\n",
      " [b'\\xec\\xa7\\x90', b'\\xec\\x8a\\xb9', b'+', b'\\xeb\\x8f\\x84']]>\n"
     ]
    }
   ],
   "source": [
    "X1_train, X2_train, Y_train, vectorization = create_instances(train_X, train_Y,vocab)\n",
    "X1_dev, X2_dev, Y_dev, _ = create_instances(dev_X, dev_Y,vocab)\n",
    "X1_test, X2_test, Y_test, _ = create_instances(test_X, test_Y,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b504c479-2c9b-4ed2-ab70-ee5ce881c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_model():\n",
    "\n",
    "    # input1\n",
    "    input1 = Input(shape=(None,), name='input1')\n",
    "    emb1 = Embedding(vectorization.vocabulary_size(), 30, mask_zero=True, name='embedding1')(input1)\n",
    "    emb1 = Dropout(0.3)(emb1)\n",
    "    encoder_lstm1 = Bidirectional(LSTM(64, return_state=True, name='lstm1'))\n",
    "    encoder_outputs1, forward_h1, forward_c1, backward_h1, backward_c1 = encoder_lstm1(emb1)\n",
    "\n",
    "    # input2\n",
    "    input2 = Input(shape=(None,), name='input2')\n",
    "    emb2 = Embedding(vectorization.vocabulary_size(), 30, mask_zero=True, name='embedding2')(input2)\n",
    "    emb2 = Dropout(0.3)(emb2)\n",
    "    encoder_lstm2 = Bidirectional(LSTM(64, return_state=True, name='lstm2'))\n",
    "    encoder_outputs2, forward_h2, forward_c2, backward_h2, backward_c2 = encoder_lstm2(emb2)\n",
    "\n",
    "    # Concatenation des états\n",
    "    state_h = Concatenate()([forward_h1, backward_h1, forward_h2, backward_h2])\n",
    "    state_c = Concatenate()([forward_c1, backward_c1, forward_c2, backward_c2])\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # encoder_states` est l'état initial\n",
    "    decoder_inputs = RepeatVector(48)(state_h) # modele basique avec repeat 48\n",
    "    decoder_lstm = LSTM(256, return_sequences=True)\n",
    "    decoder_outputs = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(vectorization.vocabulary_size(), activation='softmax', name='output')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # définir the model\n",
    "    model = Model([input1, input2], decoder_outputs)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1a94b5-1856-44c4-a7b4-21394516df5f",
   "metadata": {},
   "source": [
    "### Former le dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53a45088-1090-4416-8428-4675cb386804",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = {'input1': X1_train, 'input2': X2_train}\n",
    "y_train = Y_train\n",
    "\n",
    "x_dev = {'input1': X1_dev, 'input2': X2_dev}\n",
    "y_dev = Y_dev\n",
    "\n",
    "x_test = {'input1': X1_test, 'input2': X2_test}\n",
    "y_test = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b45077d1-c169-497f-a99c-a2e3f9b6523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 134ms/step - accuracy: 0.9114 - loss: 1.2958 - val_accuracy: 0.9404 - val_loss: 0.3428\n",
      "Epoch 2/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 136ms/step - accuracy: 0.9471 - loss: 0.2866 - val_accuracy: 0.9466 - val_loss: 0.2967\n",
      "Epoch 3/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 135ms/step - accuracy: 0.9536 - loss: 0.2385 - val_accuracy: 0.9511 - val_loss: 0.2666\n",
      "Epoch 4/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 136ms/step - accuracy: 0.9605 - loss: 0.2000 - val_accuracy: 0.9568 - val_loss: 0.2401\n",
      "Epoch 5/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 135ms/step - accuracy: 0.9654 - loss: 0.1727 - val_accuracy: 0.9606 - val_loss: 0.2154\n",
      "Epoch 6/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 135ms/step - accuracy: 0.9691 - loss: 0.1510 - val_accuracy: 0.9643 - val_loss: 0.1944\n",
      "Epoch 7/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 136ms/step - accuracy: 0.9739 - loss: 0.1238 - val_accuracy: 0.9675 - val_loss: 0.1772\n",
      "Epoch 8/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 136ms/step - accuracy: 0.9774 - loss: 0.1052 - val_accuracy: 0.9701 - val_loss: 0.1623\n",
      "Epoch 9/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 137ms/step - accuracy: 0.9807 - loss: 0.0884 - val_accuracy: 0.9724 - val_loss: 0.1461\n",
      "Epoch 10/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 136ms/step - accuracy: 0.9841 - loss: 0.0730 - val_accuracy: 0.9748 - val_loss: 0.1331\n"
     ]
    }
   ],
   "source": [
    "big_model=basic_model()\n",
    "history = big_model.fit(x_train, y_train, \n",
    "                    validation_data=(x_dev, y_dev),\n",
    "                    epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d90ae3-ef45-4516-a8ea-ecdcb96213d0",
   "metadata": {},
   "source": [
    "### Évaluation et comparaison des tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e36edd2-7184-4214-b1cf-c9335a2e0560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - accuracy: 0.9866 - loss: 0.0609\n",
      "Test Loss: 0.057124290615320206\n",
      "Test Accuracy: 0.9874478578567505\n"
     ]
    }
   ],
   "source": [
    "#evaluation sur des données test\n",
    "test_loss, test_accuracy = big_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a13fa6e9-bfe6-4bd5-9fde-5fedaf385aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vectorization.get_vocabulary()\n",
    "i2t = {i: tok for i, tok in enumerate(vocabulary)} # mappage des tokens et indice \n",
    "# print(i2t)\n",
    "\n",
    "t2i = {tok: i for i, tok in enumerate(vocabulary)}\n",
    "# print(t2i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9168a59a-f24e-4ef9-b367-772630bdc741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step\n"
     ]
    }
   ],
   "source": [
    "## DECODE \n",
    "predictions = big_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ab20ef4-eb70-420a-8f33-8c7d7c65d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # J'ai pas trouvé la façon keras pour décoder... \n",
    "predicted_tokens = []\n",
    "\n",
    "for pred in predictions:\n",
    "    predicted = \"\"\n",
    "    for idx in pred:\n",
    "        idx=  tf.argmax(idx, axis=-1).numpy()\n",
    "        predicted+=i2t[idx] \n",
    "    predicted_tokens.append(predicted)\n",
    "\n",
    "# predicted_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cde5a270-b12b-49ae-a2b3-101c2aec76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = [\"\".join(map(lambda idx: i2t[idx.numpy().astype(int) ] , test)) for test in y_test]\n",
    "# test_tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1466c51b-6b78-4e02-b42d-c9c0d6ba5524",
   "metadata": {},
   "source": [
    "### Comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f8ca3c9-3a23-480d-bdb3-cdd127d1905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted =  하기야\n",
      "real =  하기야 \n",
      "\n",
      "predicted =  도도+도\n",
      "real =  짐승+도 \n",
      "\n",
      "predicted =  잘\n",
      "real =  잘 \n",
      "\n",
      "predicted =  가가가+기+만\n",
      "real =  가르치+기+만 \n",
      "\n",
      "predicted =  하+면\n",
      "real =  하+면 \n",
      "\n",
      "predicted =  어느\n",
      "real =  어느 \n",
      "\n",
      "predicted =  정도+는\n",
      "real =  정도+는 \n",
      "\n",
      "predicted =  순치+되+ㄹ\n",
      "real =  순치+되+ㄹ \n",
      "\n",
      "predicted =  수\n",
      "real =  수 \n",
      "\n",
      "predicted =  있+다\n",
      "real =  있+다 \n",
      "\n",
      "Nombre de prédiction correcte: 1282 \n",
      "Nombre de prediction fausse:  718\n"
     ]
    }
   ],
   "source": [
    "# intersection de predicted_tokens , test_tokens\n",
    "for p , t in zip(predicted_tokens[:10], test_tokens[:10]):\n",
    "    print(\"predicted = \", p)\n",
    "    print(\"real = \", t, \"\\n\")\n",
    "\n",
    "common = [x for x in predicted_tokens if x in test_tokens]\n",
    "print(\"Nombre de prédiction correcte:\",len(common), \"\\nNombre de prediction fausse: \", len(predicted_tokens)-len(common))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3334f067-54c3-4bd1-a8db-43ef76d1b2e1",
   "metadata": {},
   "source": [
    "## Advanced Model\n",
    "#### on démarre avec le vecteur encodé, puis on poursuit en utilisant comme entrée du décodeur ce qui vient d’être généré par le décodeur à t-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa530e7-1980-4bae-8baa-a992aaaed632",
   "metadata": {},
   "source": [
    "Input1 (séquence) → Embedding → Dropout → Bidirectional LSTM → États encoder <br>\n",
    "Input2 (séquence) → Embedding → Dropout → Bidirectional LSTM → États encoder <br>\n",
    "                                                    ↓<br>\n",
    "                            Concaténation des états des deux encoders<br>\n",
    "                                                    ↓<br>\n",
    "                           Initialisation des états du décodeur <br>\n",
    "                                                    ↓ <br>\n",
    "                               LSTM Décodeur (avec embedding de Input1) <br>\n",
    "                                                    ↓<br>\n",
    "                               Normalisation des sorties du décodeur <br>\n",
    "                                                    ↓<br>\n",
    "                               Dense (Softmax) → Prédiction finale <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a110588f-592e-4a33-87e0-1bad47d7189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_advanced():\n",
    "    # Input1\n",
    "    input1 = Input(shape=(None,), name='input1')\n",
    "    emb1 = Embedding(vectorization.vocabulary_size(), 30, mask_zero=True, name='embedding1')(input1)\n",
    "    emb1 = Dropout(0.3)(emb1)\n",
    "    encoder_lstm1 = Bidirectional(LSTM(64, return_state=True, name='lstm1'))\n",
    "    encoder_outputs1, forward_h1, forward_c1, backward_h1, backward_c1 = encoder_lstm1(emb1)\n",
    "    \n",
    "    # Input2\n",
    "    input2 = Input(shape=(None,), name='input2')\n",
    "    emb2 = Embedding(vectorization.vocabulary_size(), 30, mask_zero=True, name='embedding2')(input2)\n",
    "    emb2 = Dropout(0.3)(emb2)\n",
    "    encoder_lstm2 = Bidirectional(LSTM(64, return_state=True, name='lstm2'))\n",
    "    encoder_outputs2, forward_h2, forward_c2, backward_h2, backward_c2 = encoder_lstm2(emb2)\n",
    "    \n",
    "    # Concatenate states\n",
    "    state_h = Concatenate()([forward_h1, backward_h1, forward_h2, backward_h2])\n",
    "    state_c = Concatenate()([forward_c1, backward_c1, forward_c2, backward_c2])\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Decoder\n",
    "    decoder_input_h = Input(shape=(None, ), name='decoder_input_h') # on utilise que l'état\n",
    "    decoder_input_c = Input(shape=(None, ), name='decoder_input_c') # on utilise que l'état\n",
    "    decoder_lstm = LSTM(256, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "    \n",
    "    # Initialisation avec les états\n",
    "    decoder_outputs, _, _ = decoder_lstm(emb1, initial_state=encoder_states) # encoder_states pour l'initialisation\n",
    "    decoder_outputs = LayerNormalization()(decoder_outputs)\n",
    "    \n",
    "    # Output \n",
    "    decoder_dense = Dense(vectorization.vocabulary_size(), activation='softmax', name='output')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    # modèle\n",
    "    model = Model([input1, input2], decoder_outputs)\n",
    "    \n",
    "    # Compile le modele\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d248e75b-0018-4c48-a146-1e433598e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = model_advanced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5eee8eb7-017f-44ba-bb53-42b426133d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">26,550</span> │ input1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding2          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">26,550</span> │ input2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_4         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_5         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_4     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">48,640</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │ not_equal_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)]       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_5     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">48,640</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │ not_equal_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)]       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ bidirectional_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ bidirectional_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ bidirectional_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ bidirectional_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ bidirectional_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ bidirectional_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">293,888</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ concatenate_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ concatenate_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">885</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">227,445</span> │ layer_normalizat… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input1 (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input2 (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)  │     \u001b[38;5;34m26,550\u001b[0m │ input1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding2          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)  │     \u001b[38;5;34m26,550\u001b[0m │ input2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_4         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_5         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_4     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │     \u001b[38;5;34m48,640\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │ not_equal_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)]       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_5     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │     \u001b[38;5;34m48,640\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │ not_equal_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)]       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bidirectional_4[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ bidirectional_4[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │                   │            │ bidirectional_5[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │                   │            │ bidirectional_5[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bidirectional_4[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ bidirectional_4[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │                   │            │ bidirectional_5[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │                   │            │ bidirectional_5[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m293,888\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ concatenate_4[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ concatenate_5[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m885\u001b[0m) │    \u001b[38;5;34m227,445\u001b[0m │ layer_normalizat… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">672,225</span> (2.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m672,225\u001b[0m (2.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">672,225</span> (2.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m672,225\u001b[0m (2.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b517b921-0c60-4f2b-a3c2-eb42203fd59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 113ms/step - accuracy: 0.0250 - loss: 4.0114 - val_accuracy: 0.0490 - val_loss: 1.5246\n",
      "Epoch 2/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 114ms/step - accuracy: 0.0516 - loss: 0.8753 - val_accuracy: 0.0624 - val_loss: 0.7026\n",
      "Epoch 3/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 114ms/step - accuracy: 0.0603 - loss: 0.2992 - val_accuracy: 0.0658 - val_loss: 0.4722\n",
      "Epoch 4/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 114ms/step - accuracy: 0.0643 - loss: 0.1309 - val_accuracy: 0.0832 - val_loss: 0.4026\n",
      "Epoch 5/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 116ms/step - accuracy: 0.0717 - loss: 0.0743 - val_accuracy: 0.0720 - val_loss: 0.3619\n",
      "Epoch 6/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 114ms/step - accuracy: 0.0699 - loss: 0.0468 - val_accuracy: 0.0723 - val_loss: 0.3579\n",
      "Epoch 7/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 114ms/step - accuracy: 0.0724 - loss: 0.0409 - val_accuracy: 0.0999 - val_loss: 0.3640\n",
      "Epoch 8/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 115ms/step - accuracy: 0.0808 - loss: 0.0379 - val_accuracy: 0.0744 - val_loss: 0.3634\n",
      "Epoch 9/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 114ms/step - accuracy: 0.0878 - loss: 0.0256 - val_accuracy: 0.0752 - val_loss: 0.3171\n",
      "Epoch 10/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 114ms/step - accuracy: 0.0867 - loss: 0.0313 - val_accuracy: 0.0811 - val_loss: 0.3361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x787960336770>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_train, y_train, \n",
    "                    validation_data=(x_dev, y_dev),\n",
    "                    epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb6f90-4a41-439d-a898-7d18b9c9276f",
   "metadata": {},
   "source": [
    "### Générateur \n",
    "- prend en entrée le modèle avancé et les dictionnaires token_to_index and index_to_tokens\n",
    "- génère la séquence suivante\n",
    "- '[UNK]' si non trouvé dans le l'index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "187f4a49-9699-4e48-88b9-0e92f0524e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator:\n",
    "    def __init__(self, model, t2i, i2t):\n",
    "        self.model = model\n",
    "        self.t2i = t2i\n",
    "        self.i2t = i2t\n",
    "\n",
    "        self.encoder = self.find_layer_by_name('bidirectional', 'encoder')\n",
    "        self.decoder = self.find_layer_by_name('decoder_lstm', 'decoder')\n",
    "        self.emb = self.find_layer_by_name('embedding', 'embedding')\n",
    "        self.rnn = self.decoder\n",
    "        self.classif = self.find_layer_by_name('output', 'output')\n",
    "\n",
    "    # affichage de noms de layers \n",
    "    def find_layer_by_name(self, name_keyword, layer_type):\n",
    "        for layer in self.model.layers:\n",
    "            if name_keyword in layer.name:\n",
    "                return layer\n",
    "        raise ValueError(f\"No {layer_type} layer found with keyword '{name_keyword}'\")\n",
    "\n",
    "    def _predict_next(self, last_char, state, encoder_output):\n",
    "        e = self.emb(last_char)\n",
    "        output_and_state = self.rnn(e, initial_state=state)  \n",
    "        output = output_and_state[0]  # extraction output tenseur\n",
    "        new_state = output_and_state[1:]  # extraction état\n",
    "        probs = self.classif(output)\n",
    "        next_char = tf.random.categorical(probs[-1, :, :], 1)\n",
    "        return next_char, new_state\n",
    "        \n",
    "    def predict_seq(self, starting_token):\n",
    "        result = []\n",
    "        current_token = starting_token\n",
    "        state = None\n",
    "        encoder_output = None\n",
    "        while current_token != '[END]' and len(result) <= 10:\n",
    "            char_index = self.t2i.get(current_token, self.t2i['[UNK]']) # par defaut [UNK]\n",
    "            char = tf.reshape(char_index, [1, 1])\n",
    "            encoder_output = self.encoder(self.emb(char)) if encoder_output is None else encoder_output\n",
    "            next_token, state = self._predict_next(char, state, encoder_output)\n",
    "            current_token = self.i2t.get(tf.squeeze(next_token).numpy(), '[UNK]') # par defaut [UNK]\n",
    "            if current_token != '[END]':\n",
    "                result.append(current_token)\n",
    "        return ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "331f90b7-f378-4e24-a0bd-0ff4c9005fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 116ms/step - accuracy: 0.0295 - loss: 3.9646 - val_accuracy: 0.0567 - val_loss: 1.5078\n",
      "Epoch 2/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 117ms/step - accuracy: 0.0611 - loss: 0.9029 - val_accuracy: 0.0869 - val_loss: 0.7116\n",
      "Epoch 3/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 117ms/step - accuracy: 0.0767 - loss: 0.3002 - val_accuracy: 0.0815 - val_loss: 0.4941\n",
      "Epoch 4/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 117ms/step - accuracy: 0.1011 - loss: 0.1343 - val_accuracy: 0.1091 - val_loss: 0.4276\n",
      "Epoch 5/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 118ms/step - accuracy: 0.0992 - loss: 0.0870 - val_accuracy: 0.4295 - val_loss: 0.3881\n",
      "Epoch 6/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 117ms/step - accuracy: 0.2412 - loss: 0.0536 - val_accuracy: 0.0980 - val_loss: 0.3692\n",
      "Epoch 7/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 117ms/step - accuracy: 0.0980 - loss: 0.0402 - val_accuracy: 0.0983 - val_loss: 0.3595\n",
      "Epoch 8/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 117ms/step - accuracy: 0.1175 - loss: 0.0298 - val_accuracy: 0.1044 - val_loss: 0.3798\n",
      "Epoch 9/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 117ms/step - accuracy: 0.1116 - loss: 0.0285 - val_accuracy: 0.1207 - val_loss: 0.3547\n",
      "Epoch 10/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 117ms/step - accuracy: 0.1158 - loss: 0.0256 - val_accuracy: 0.1051 - val_loss: 0.3702\n"
     ]
    }
   ],
   "source": [
    "model2 = model_advanced() # model2\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "history = model2.fit(x_train, y_train,\n",
    "                    validation_data=(x_dev, y_dev),\n",
    "                    epochs=10, batch_size=32, \n",
    "                    callbacks=[early_stopping, model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e5b59-8773-4f92-996f-cd4dcdd9e702",
   "metadata": {},
   "source": [
    "### Exemples des générations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdedeb0c-6864-4ff4-80c8-72af6db1e5ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "간 컵회뿌눈즐코수긋약틀샘\n"
     ]
    }
   ],
   "source": [
    "gen = Generator(model2,t2i, i2t)\n",
    "input_char = random.choice(vocab)\n",
    "# input_char = '한'  # input Korean character\n",
    "output_seq = gen.predict_seq(input_char)    \n",
    "print(input_char, output_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f94d6-7edc-4d5c-9e49-007e40d10b0d",
   "metadata": {},
   "source": [
    "### Donnée une séquence à partir des données testes, les tokens générés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69227da3-c990-410a-bb09-f4ba8f57c717",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하기야 위펼낭푸볕앉년남어얘것\n",
      "짐승도 힘문퓨혈놀빛맙쌀냄꺾든\n",
      "잘 똑늘강싶톳백둑흥층걸뛰\n",
      "가르치기만 신년터욕글꽤넉끝근일약\n",
      "하면 혈할량던황심눅휴률즘받\n",
      "어느 햇싣좌남경.갓짖쭐분팔\n",
      "정도는 허간쿵꿔판낮튼애온끗템\n",
      "순치될 묻믿능빌빠풀곰꼭왕촉뿌\n",
      "수 판욱매퉁념뻘윗즈업균렵\n",
      "있다 맛그번맘풀묽관닭꿰일매\n",
      ". 병임닝운딱나에쟁권데주\n",
      "사람이 밸갑황땟삶문프거잔흑꽃\n",
      "스스로 젖벽핵나첩끊끔꺼찍밭허\n",
      "만물의 알히슬왕당닉떡형액애임\n",
      "영장이라 찻쐬님까찬쳐륭썰베참님\n",
      "하고 덜츠검웅히멀개거품딩개\n",
      "우쭐대는 데람,악퍼토칭승리좁삶\n",
      "까닭이 권업관갚뻐궁재답쩐방랄\n",
      "여기에 약월못잣레심상은북잉외\n",
      "있다 맘죽찰죠맥빗꽤쉬람물타\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for input_token in test_X[:20]:\n",
    "    token = gen.predict_seq(input_token)\n",
    "    print(input_token, token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046edca-f7d6-4fe1-814e-1b9baddc694a",
   "metadata": {},
   "source": [
    "### MODELE AVANCÉ 2\n",
    "- Les états cachés et les états de cellule des LSTM bidirectionnels des deux séquences d'entrée sont concaténés pour former les états initiaux du décodeur\n",
    "- le décodeur est initialisé directement avec les états concaténés des encodeurs.\n",
    "- les embeddings de input1 sont utilisés comme entrée séquentielle du décodeur\n",
    "\n",
    "### Générateur:\n",
    "- génère une séquence à partir d'un token de départ ([START]) jusqu'à ce qu'un token de fin ([END]) soit atteint ou que la longueur maximale soit atteinte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b81fac3d-d457-4441-828c-6ab3b78f0ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_advanced2():\n",
    "    # Input1\n",
    "    input1 = Input(shape=(None,), name='input1')\n",
    "    emb1 = Embedding(vectorization.vocabulary_size(), 100, mask_zero=True, name='embedding1')(input1)\n",
    "    emb1 = Dropout(0.3)(emb1)\n",
    "    encoder_lstm1 = Bidirectional(LSTM(100, return_state=True, name='lstm1'))\n",
    "    encoder_outputs1, forward_h1, forward_c1, backward_h1, backward_c1 = encoder_lstm1(emb1)\n",
    "\n",
    "    # Input2\n",
    "    input2 = Input(shape=(None,), name='input2')\n",
    "    emb2 = Embedding(vectorization.vocabulary_size(), 100, mask_zero=True, name='embedding2')(input2)\n",
    "    emb2 = Dropout(0.3)(emb2)\n",
    "    encoder_lstm2 = Bidirectional(LSTM(100, return_state=True, name='lstm2'))\n",
    "    encoder_outputs2, forward_h2, forward_c2, backward_h2, backward_c2 = encoder_lstm2(emb2)\n",
    "\n",
    "    # Concatenate states\n",
    "    state_h = Concatenate()([forward_h1, backward_h1, forward_h2, backward_h2])\n",
    "    state_c = Concatenate()([forward_c1, backward_c1, forward_c2, backward_c2])\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Decoder\n",
    "    decoder_input_h = Input(shape=(None, 300), name='decoder_input_h') # on utilisera que l'état\n",
    "    decoder_input_c = Input(shape=(None, 300), name='decoder_input_c') # on utilisera que l'état\n",
    "    decoder_lstm = LSTM(400, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "\n",
    "    # Initialize decoder states with encoder states\n",
    "    decoder_outputs, _, _ = decoder_lstm(emb1, initial_state=encoder_states)\n",
    "    decoder_outputs = LayerNormalization()(decoder_outputs)\n",
    "\n",
    "    # Output layer\n",
    "    decoder_dense = Dense(vectorization.vocabulary_size(), activation='softmax', name='output')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model([input1, input2], decoder_outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "class Generator:\n",
    "    def __init__(self, model, t2i, i2t):\n",
    "        self.model = model\n",
    "        self.t2i = t2i\n",
    "        self.i2t = i2t\n",
    "\n",
    "        self.encoder = self.find_layer_by_name('bidirectional', 'encoder')\n",
    "        self.decoder = self.find_layer_by_name('decoder_lstm', 'decoder')\n",
    "        self.emb = self.find_layer_by_name('embedding', 'embedding')\n",
    "        self.rnn = self.decoder\n",
    "        self.classif = self.find_layer_by_name('output', 'output')\n",
    "\n",
    "    def find_layer_by_name(self, name_keyword, layer_type):\n",
    "        for layer in self.model.layers:\n",
    "            if name_keyword in layer.name:\n",
    "                return layer\n",
    "        raise ValueError(f\"No {layer_type} layer found with keyword '{name_keyword}'\")\n",
    "\n",
    "    def _predict_next(self, last_char, state, encoder_output):\n",
    "        e = self.emb(last_char)\n",
    "        output_and_state = self.rnn(e, initial_state=state)\n",
    "        output = output_and_state[0]\n",
    "        new_state = output_and_state[1:]\n",
    "        probs = self.classif(output)\n",
    "        next_char = tf.random.categorical(probs[-1, :, :], 1)\n",
    "        return next_char, new_state\n",
    "\n",
    "    def predict_seq(self, starting_token='[START]'):\n",
    "        result = []\n",
    "        current_token = starting_token\n",
    "        state = None\n",
    "        encoder_output = None\n",
    "        while current_token != '[END]' and len(result) <= 10:\n",
    "            char_index = self.t2i.get(current_token, self.t2i['[UNK]'])\n",
    "            char = tf.reshape(char_index, [1, 1])\n",
    "            encoder_output = self.encoder(self.emb(char)) if encoder_output is None else encoder_output\n",
    "            next_token, state = self._predict_next(char, state, encoder_output)\n",
    "            current_token = self.i2t.get(tf.squeeze(next_token).numpy(), '[UNK]')\n",
    "            if current_token != '[END]':\n",
    "                result.append(current_token)\n",
    "        return ''.join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3bf1f1a1-92a5-4dd2-a9db-e8f3ab9b00ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2= model_advanced2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3fa9a04-30fe-49be-aa5d-6cd300f87d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 330ms/step - accuracy: 0.0619 - loss: 3.7681 - val_accuracy: 0.0757 - val_loss: 1.2762\n",
      "Epoch 2/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 333ms/step - accuracy: 0.0892 - loss: 0.6573 - val_accuracy: 0.5339 - val_loss: 0.6441\n",
      "Epoch 3/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 331ms/step - accuracy: 0.4399 - loss: 0.2070 - val_accuracy: 0.0759 - val_loss: 0.4536\n",
      "Epoch 4/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 332ms/step - accuracy: 0.0840 - loss: 0.0801 - val_accuracy: 0.0856 - val_loss: 0.3870\n",
      "Epoch 5/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 332ms/step - accuracy: 0.1189 - loss: 0.0455 - val_accuracy: 0.0898 - val_loss: 0.3579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x78797ea558d0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod2.fit(x_train, y_train,\n",
    "        validation_data=(x_dev, y_dev),\n",
    "        epochs=10, batch_size=64, \n",
    "        callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f91f7-02b9-46da-832d-9a3ddc1ce581",
   "metadata": {},
   "source": [
    "### Exemples de générations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8bf5939b-3192-4ed4-a11c-54c547c5c24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슨덩쑤플볍빛엇얘떠관판\n"
     ]
    }
   ],
   "source": [
    "gen = Generator(mod2,t2i, i2t)\n",
    "input_char = '[START]'  # input Korean character\n",
    "output_seq = gen.predict_seq(input_char)\n",
    "    \n",
    "print(output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f821bbbe-8a2e-4000-914a-01085acd4303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하기야 슴돈노ㅆ당꿰토표럭의노\n",
      "짐승도 밧뿍은쿨툼벽참둘콧외들\n",
      "잘 땀님곡씻집린령냐준밥얘\n",
      "가르치기만 덮걱초노릇럼려종냐찰잉\n",
      "하면 맘론닭멀뱉벗밤맹꺾롭짖\n",
      "어느 쭉얗물짜외땀싶얇겁끊균\n",
      "정도는 심닐꺼터즙냉인음립곁푼\n",
      "순치될 \n",
      "수 향꿀메릎깥매쉽끊독딪참\n",
      "있다 섬웬책등짙는돈ㅆㅂ샘올\n",
      ". 운솔급막참싸아싶지에귀\n",
      "사람이 씌감야빨톱색촉쭈평맥칠\n",
      "스스로 햄추힘른레냐앉축괴팡흐\n",
      "만물의 강창듬블왕널카막픔탁욕\n",
      "영장이라 라함튼곁당임창느튀굽믿\n",
      "하고 존앉클클얗깎늦틈싹경울\n",
      "우쭐대는 찰넷찾틀황주끓풀챙에어\n",
      "까닭이 제홀훌걱렵끼놓낫묵잉칙\n",
      "여기에 슴넥볍,솜파출멸숫깊편\n",
      "있다 독뽑깨컨부떡튀키칡풀코\n"
     ]
    }
   ],
   "source": [
    "for input_token in test_X[:20]:\n",
    "    print(input_token, gen.predict_seq(input_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1659242-9cde-48b1-86f9-c5dcf670c5c9",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Multioutput \n",
    "Il s'agit des données multi-entrée et multiple-sorties\n",
    "A partir des données CoNLL-U du coréen, nous allons créer un modèle qui prend 3 entrées :\n",
    "- token\n",
    "- tokens lemmatisé (prefixé de [START])\n",
    "- étiquette POS des lemmes des tokens (prefixé de [START])\n",
    "\n",
    "Donne comme sortie:\n",
    "- tokens lemmatisé (suffixé de [END])\n",
    "- étiquette POS des lemmes des tokens (suffixé de [END])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "331c228f-0461-4b75-a745-262101daebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env TF_FORCE_GPU_ALLOW_GROWTH=true\n",
    "# %matplotlib widget\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.data import TextLineDataset\n",
    "from keras.layers import Input, LSTM, Dense, RepeatVector, Embedding, Bidirectional, Concatenate, Dropout, StringLookup, RNN, LayerNormalization\n",
    "from keras import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eac6e19-0d4d-46aa-b15d-8d3d40044195",
   "metadata": {},
   "source": [
    "#### Extraction des données\n",
    "\n",
    "fichier coNLU : \n",
    "    - col2 : tokens | col3 : lemmatized_tokens | col5 : lemma_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c5a8f9b-dd49-4de2-8b30-5aaabe493801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture \n",
    "df_train = parse_conllu_file('corpus_ko/ko_train.conllu')[:10000]\n",
    "df_dev = parse_conllu_file('corpus_ko/ko_dev.conllu')[:2000]\n",
    "df_test = parse_conllu_file('corpus_ko/ko_test.conllu')[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85685474-c31b-4531-8cf0-b347afb75ca3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>lemma_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>현대증권</td>\n",
       "      <td>현대증권</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>배성영</td>\n",
       "      <td>배성영</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>수석연구원은</td>\n",
       "      <td>수석+연구원+은</td>\n",
       "      <td>NNG+NNG+JX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"</td>\n",
       "      <td>\"</td>\n",
       "      <td>SS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>미국</td>\n",
       "      <td>미국</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>통화당국의</td>\n",
       "      <td>통화+당국+의</td>\n",
       "      <td>NNG+NNG+JKG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>경기부양</td>\n",
       "      <td>경기+부양</td>\n",
       "      <td>NNG+NNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>기조</td>\n",
       "      <td>기조</td>\n",
       "      <td>NNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>유지</td>\n",
       "      <td>유지</td>\n",
       "      <td>NNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>가능성과</td>\n",
       "      <td>가능+성+과</td>\n",
       "      <td>XR+XSN+JC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    token     lemma    lemma_tag\n",
       "0    현대증권      현대증권          NNP\n",
       "1     배성영       배성영          NNP\n",
       "2  수석연구원은  수석+연구원+은   NNG+NNG+JX\n",
       "3       \"         \"           SS\n",
       "4      미국        미국          NNP\n",
       "5   통화당국의   통화+당국+의  NNG+NNG+JKG\n",
       "6    경기부양     경기+부양      NNG+NNG\n",
       "7      기조        기조          NNG\n",
       "8      유지        유지          NNG\n",
       "9    가능성과    가능+성+과    XR+XSN+JC"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a1ed1b5-828f-4c48-8ef2-9f7127647d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['현대증권', '배성영', '수석+연구원+은', '\"', '미국', '통화+당국+의', '경기+부양', '기조', '유지', '가능+성+과', '주요+20+개국', '(', 'G+20', ')', '정상+회의+를', '앞두+ㄴ', '위안+화', '절상', '압력+의', '고조+로', '인하+아', '당분간', '원화', '강세', '압력+은', '더', '지속+되+ㄹ', '전망', '\"', '이']\n"
     ]
    }
   ],
   "source": [
    "print(list(df_test['lemma'])[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7240bc6b-f62c-4929-9c9a-38c9b3a5059b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NNP', 'NNP', 'NNG+NNG+JX', 'SS', 'NNP', 'NNG+NNG+JKG', 'NNG+NNG', 'NNG', 'NNG', 'XR+XSN+JC', 'NNG+SN+NNB', 'SS', 'SL+SN', 'SS', 'NNG+NNG+JKO', 'VV+ETM', 'NNG+XSN', 'NNG', 'NNG+JKG', 'NNG+JKB', 'VV+EC', 'MAG', 'NNG', 'NNG', 'NNG+JX', 'MAG', 'NNG+XSV+ETM', 'NNG', 'SS', 'VCP+EC']\n"
     ]
    }
   ],
   "source": [
    "print(list(df_test['lemma_tag'])[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0154a76-ea98-428c-a830-1b3dec13daec",
   "metadata": {},
   "source": [
    "### Creation de vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac5efe4b-478f-41b3-b8c0-f2ae4cef28ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'\\xea\\xb6\\x81', b'\\xeb\\x9e\\xab', b'\\xec\\x84\\xac', b'\\xea\\xb2\\xac', b'\\xe8\\xbb\\x8d', b'\\xea\\xba\\xbc', b'\\xed\\x97\\x9d', b'\\xeb\\x8d\\xb8', b'\\xeb\\xaa\\xbb', b'\\xeb\\x81\\x8c', b'\\xec\\xb0\\xac', b'\\xec\\xa0\\x84', b'\\xeb\\x8a\\xa6', b'\\xec\\x99\\x95', b'\\xeb\\x8b\\x99', b'\\xeb\\x82\\x98', b'\\xec\\xb9\\xa9', b'\\xeb\\x83\\x90', b'\\xeb\\x86\\x80', b'P', b'\\xec\\x9d\\xbd', b',', b'\\xe4\\xb9\\x9d', b'\\xeb\\x94\\x94', b'\\xec\\x86\\x9f', b'\\xeb\\xac\\x98', b'\\xeb\\x90\\x90', b'\\xec\\x98\\x81', b'\\xeb\\x89\\xb4', b'\\xeb\\xae\\xa4']\n"
     ]
    }
   ],
   "source": [
    "vocab = create_char_vocab(df_train)\n",
    "print(vocab[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe6f8cd-9fb5-4d34-bebc-e4469e057515",
   "metadata": {},
   "source": [
    "### Vectorisation des données CoNLL-U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "07866e9f-8711-46d5-a084-4789daf55692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        잡스는\n",
      "1     워즈니악에게\n",
      "2        보수를\n",
      "3         반씩\n",
      "4        나누는\n",
      "       ...  \n",
      "95     다이오드의\n",
      "96      본격적인\n",
      "97       보급이\n",
      "98     시작되었다\n",
      "99         .\n",
      "Name: token, Length: 100, dtype: object\n",
      "0         잡스+는\n",
      "1      워즈니악+에게\n",
      "2         보수+를\n",
      "3          반+씩\n",
      "4         나누+는\n",
      "        ...   \n",
      "95      다이오드+의\n",
      "96    본격+적+이+ㄴ\n",
      "97        보급+이\n",
      "98    시작+되+었+다\n",
      "99           .\n",
      "Name: lemma, Length: 100, dtype: object\n",
      "0             NNP+JX\n",
      "1            NNP+JKB\n",
      "2            NNG+JKO\n",
      "3            NNG+XSN\n",
      "4             VV+ETM\n",
      "           ...      \n",
      "95           NNG+JKG\n",
      "96    XR+XSN+VCP+ETM\n",
      "97           NNG+JKS\n",
      "98     NNG+XSV+EP+EF\n",
      "99                SF\n",
      "Name: lemma_tag, Length: 100, dtype: object\n",
      "X1:  <tf.RaggedTensor [[b'\\xec\\x9e\\xa1', b'\\xec\\x8a\\xa4', b'\\xeb\\x8a\\x94'],\n",
      " [b'\\xec\\x9b\\x8c', b'\\xec\\xa6\\x88', b'\\xeb\\x8b\\x88', b'\\xec\\x95\\x85',\n",
      "  b'\\xec\\x97\\x90', b'\\xea\\xb2\\x8c']                                  ]>\n",
      "X2:  <tf.RaggedTensor [[b'[', b'S', b'T', b'A', b'R', b'T', b']', b'\\xec\\x9e\\xa1',\n",
      "  b'\\xec\\x8a\\xa4', b'+', b'\\xeb\\x8a\\x94']                   ,\n",
      " [b'[', b'S', b'T', b'A', b'R', b'T', b']', b'\\xec\\x9b\\x8c',\n",
      "  b'\\xec\\xa6\\x88', b'\\xeb\\x8b\\x88', b'\\xec\\x95\\x85', b'+', b'\\xec\\x97\\x90',\n",
      "  b'\\xea\\xb2\\x8c']                                                         ]>\n",
      "X3:  <tf.RaggedTensor [[b'[', b'S', b'T', b'A', b'R', b'T', b']', b'N', b'N', b'P', b'+', b'J',\n",
      "  b'X']                                                                  ,\n",
      " [b'[', b'S', b'T', b'A', b'R', b'T', b']', b'N', b'N', b'P', b'+', b'J',\n",
      "  b'K', b'B']                                                            ]>\n",
      "0        제일\n",
      "1       가까운\n",
      "2     스타벅스가\n",
      "3        어디\n",
      "4        있지\n",
      "      ...  \n",
      "95       안양\n",
      "96        ,\n",
      "97       안산\n",
      "98      등지로\n",
      "99        갈\n",
      "Name: token, Length: 100, dtype: object\n",
      "0         제일\n",
      "1       가깝+ㄴ\n",
      "2     스타벅스+가\n",
      "3         어디\n",
      "4        있+지\n",
      "       ...  \n",
      "95        안양\n",
      "96         ,\n",
      "97        안산\n",
      "98      등지+로\n",
      "99       가+ㄹ\n",
      "Name: lemma, Length: 100, dtype: object\n",
      "0         NNG\n",
      "1      VA+ETM\n",
      "2     NNG+JKS\n",
      "3          NP\n",
      "4       VV+EC\n",
      "       ...   \n",
      "95        NNP\n",
      "96         SP\n",
      "97        NNP\n",
      "98    NNB+JKB\n",
      "99     VV+ETM\n",
      "Name: lemma_tag, Length: 100, dtype: object\n",
      "X1:  <tf.RaggedTensor [[b'\\xec\\xa0\\x9c', b'\\xec\\x9d\\xbc'],\n",
      " [b'\\xea\\xb0\\x80', b'\\xea\\xb9\\x8c', b'\\xec\\x9a\\xb4']]>\n",
      "X2:  <tf.RaggedTensor [[b'[', b'S', b'T', b'A', b'R', b'T', b']', b'\\xec\\xa0\\x9c',\n",
      "  b'\\xec\\x9d\\xbc']                                          ,\n",
      " [b'[', b'S', b'T', b'A', b'R', b'T', b']', b'\\xea\\xb0\\x80',\n",
      "  b'\\xea\\xb9\\x9d', b'+', b'\\xe3\\x84\\xb4']                   ]>\n",
      "X3:  <tf.RaggedTensor [[b'[', b'S', b'T', b'A', b'R', b'T', b']', b'N', b'N', b'G'],\n",
      " [b'[', b'S', b'T', b'A', b'R', b'T', b']', b'V', b'A', b'+', b'E', b'T',\n",
      "  b'M']                                                                  ]>\n",
      "0       현대증권\n",
      "1        배성영\n",
      "2     수석연구원은\n",
      "3          \"\n",
      "4         미국\n",
      "       ...  \n",
      "95       포함해\n",
      "96      10년을\n",
      "97      감옥에서\n",
      "98       보냈다\n",
      "99         .\n",
      "Name: token, Length: 100, dtype: object\n",
      "0         현대증권\n",
      "1          배성영\n",
      "2     수석+연구원+은\n",
      "3            \"\n",
      "4           미국\n",
      "        ...   \n",
      "95      포함+하+아\n",
      "96      10+년+을\n",
      "97       감옥+에서\n",
      "98      보내+었+다\n",
      "99           .\n",
      "Name: lemma, Length: 100, dtype: object\n",
      "0            NNP\n",
      "1            NNP\n",
      "2     NNG+NNG+JX\n",
      "3             SS\n",
      "4            NNP\n",
      "         ...    \n",
      "95    NNG+XSV+EC\n",
      "96    SN+NNB+JKO\n",
      "97       NNG+JKB\n",
      "98      VV+EP+EF\n",
      "99            SF\n",
      "Name: lemma_tag, Length: 100, dtype: object\n",
      "X1:  <tf.RaggedTensor [[b'\\xed\\x98\\x84', b'\\xeb\\x8c\\x80', b'\\xec\\xa6\\x9d', b'\\xea\\xb6\\x8c'],\n",
      " [b'\\xeb\\xb0\\xb0', b'\\xec\\x84\\xb1', b'\\xec\\x98\\x81']]>\n",
      "X2:  <tf.RaggedTensor [[b'[', b'S', b'T', b'A', b'R', b'T', b']', b'\\xed\\x98\\x84',\n",
      "  b'\\xeb\\x8c\\x80', b'\\xec\\xa6\\x9d', b'\\xea\\xb6\\x8c']        ,\n",
      " [b'[', b'S', b'T', b'A', b'R', b'T', b']', b'\\xeb\\xb0\\xb0',\n",
      "  b'\\xec\\x84\\xb1', b'\\xec\\x98\\x81']                         ]>\n",
      "X3:  <tf.RaggedTensor [[b'[', b'S', b'T', b'A', b'R', b'T', b']', b'N', b'N', b'P'],\n",
      " [b'[', b'S', b'T', b'A', b'R', b'T', b']', b'N', b'N', b'P']]>\n"
     ]
    }
   ],
   "source": [
    "# vectorisation\n",
    "X1_train, X2_train, X3_train, Y1_train, Y2_train, vectorization = conll_instances(df_train, vocab)\n",
    "X1_dev, X2_dev, X3_dev, Y1_dev, Y2_dev, _ = conll_instances(df_dev, vocab)\n",
    "X1_test, X2_test, X3_test, Y1_test, Y2_test, _ = conll_instances(df_test, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba10e9d1-c377-4ca7-96d7-5b31d6409f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creation dictionnaire inverse\n",
    "vocabulary2 = vectorization.get_vocabulary()\n",
    "reverse_vocab = {i: tok for i, tok in enumerate(vocabulary2)} # mappage des tokens et indice "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0744034-3e97-472e-b5d3-5a5a5ca8bc64",
   "metadata": {},
   "source": [
    "#### Le modèle de séquence à séquence multi-sorties encode des séquences de tokens, lemmes, et tags, concatène leurs états, et utilise ces états pour générer les séquences de lemmes et tags correspondants.\n",
    "\n",
    "**Entrées:**\n",
    "Token: Séquence d'entrée pour les tokens (input_token).\n",
    "Lemma: Séquence d'entrée pour les lemmes (input_lemma).\n",
    "Tag: Séquence d'entrée pour les tags (input_tag).\n",
    "\n",
    "**Encodeur:**\n",
    "Chaque séquence d'entrée passe par une couche d'embedding (taille 30) suivie d'un dropout (taux de 0.3).\n",
    "Chaque embedding est ensuite traité par un LSTM bidirectionnel avec 64 unités.\n",
    "Les états cachés et les états de cellule des LSTM bidirectionnels sont extraits pour chaque type d'entrée (token, lemma, tag).\n",
    "\n",
    "**Concatenation des États:**\n",
    "Les états cachés et les états de cellule des encodeurs sont concaténés pour former les états initiaux du décodeur.\n",
    "\n",
    "**Décodeur:**\n",
    "Les états concaténés (state_h) sont répétés pour correspondre à la longueur de séquence de sortie attendue (48).\n",
    "La séquence répétée est concaténée avec les embeddings des tokens pour former l'entrée du LSTM du décodeur.\n",
    "Le décodeur utilise un LSTM avec 384 unités pour générer des séquences de sortie.\n",
    "\n",
    "\n",
    "**Sorties:**\n",
    "Lemma: Une couche dense avec activation softmax génère les lemmes (output_lemma).\n",
    "Tag: Une autre couche dense avec activation softmax génère les tags (output_tag).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c82873dc-e658-466c-bb5e-4c872ff562e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_multioutput():\n",
    "    # Encoder for token\n",
    "    input_token = Input(shape=(None,), name='input_token')\n",
    "    emb_token = Embedding(vectorization.vocabulary_size(), 30, mask_zero=True, name='embedding_token')(input_token)\n",
    "    emb_token = Dropout(0.3)(emb_token)\n",
    "    encoder_lstm_token = Bidirectional(LSTM(64, return_state=True, name='lstm_token'))\n",
    "    _, forward_h_token, forward_c_token, backward_h_token, backward_c_token = encoder_lstm_token(emb_token)\n",
    "    \n",
    "    # Encoder for lemma\n",
    "    input_lemma = Input(shape=(None,), name='input_lemma')\n",
    "    emb_lemma = Embedding(vectorization.vocabulary_size(), 30, mask_zero=True, name='embedding_lemma')(input_lemma)\n",
    "    emb_lemma = Dropout(0.3)(emb_lemma)\n",
    "    encoder_lstm_lemma = Bidirectional(LSTM(64, return_state=True, name='lstm_lemma'))\n",
    "    _, forward_h_lemma, forward_c_lemma, backward_h_lemma, backward_c_lemma = encoder_lstm_lemma(emb_lemma)\n",
    "    \n",
    "    # Encoder for token tags\n",
    "    input_tag = Input(shape=(None,), name='input_tag')\n",
    "    emb_tag = Embedding(vectorization.vocabulary_size(), 30, mask_zero=True, name='embedding_tag')(input_tag)\n",
    "    emb_tag = Dropout(0.3)(emb_tag)\n",
    "    encoder_lstm_tag = Bidirectional(LSTM(64, return_state=True, name='lstm_tag'))\n",
    "    _, forward_h_tag, forward_c_tag, backward_h_tag, backward_c_tag = encoder_lstm_tag(emb_tag)\n",
    "    \n",
    "    # Concatenate all encoder states\n",
    "    state_h = Concatenate()([forward_h_token, backward_h_token, forward_h_lemma, backward_h_lemma, forward_h_tag, backward_h_tag])\n",
    "    state_c = Concatenate()([forward_c_token, backward_c_token, forward_c_lemma, backward_c_lemma, forward_c_tag, backward_c_tag])\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Decoder\n",
    "    dec_input_repeat = RepeatVector(48)(state_h)  \n",
    "    dec_lstm_input = Concatenate()([dec_input_repeat, emb_token]) \n",
    "    \n",
    "    dec_lstm = LSTM(384, return_sequences=True, return_state=True, name='dec_lstm')\n",
    "    dec_outputs, _, _ = dec_lstm(dec_lstm_input, initial_state=encoder_states)\n",
    "    \n",
    "    # Decoder output for lemma\n",
    "    dec_dense_lemma = Dense(vectorization.vocabulary_size(), activation='softmax', name='output_lemma')\n",
    "    dec_outputs_lemma = dec_dense_lemma(dec_outputs)\n",
    "    \n",
    "    # Decoder output for lemma tags\n",
    "    dec_dense_tag = Dense(vectorization.vocabulary_size(), activation='softmax', name='output_tag')\n",
    "    dec_outputs_tag = dec_dense_tag(dec_outputs)\n",
    "    \n",
    "    model = Model([input_token, input_lemma, input_tag], [dec_outputs_lemma, dec_outputs_tag])\n",
    "    model.compile(\n",
    "        loss=[\"sparse_categorical_crossentropy\", \"sparse_categorical_crossentropy\"], \n",
    "        optimizer=\"adam\", \n",
    "        metrics=[\"accuracy\", \"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "39796a93-9d59-41a6-b920-420965ae2c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_token         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_lemma         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_tag           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_token     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,730</span> │ input_token[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_lemma     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,730</span> │ input_lemma[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_tag       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,730</span> │ input_tag[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_token[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_10        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_token[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_lemma[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_11        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_lemma[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_12          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_tag[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_12        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_tag[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_10    │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">48,640</span> │ dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │ not_equal_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)]       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_11    │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">48,640</span> │ dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │ not_equal_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)]       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_12    │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">48,640</span> │ dropout_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │ not_equal_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>),       │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)]       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_10      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_10… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ bidirectional_10… │\n",
       "│                     │                   │            │ bidirectional_11… │\n",
       "│                     │                   │            │ bidirectional_11… │\n",
       "│                     │                   │            │ bidirectional_12… │\n",
       "│                     │                   │            │ bidirectional_12… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ repeat_vector_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_12      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">414</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ repeat_vector_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_11      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_10… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ bidirectional_10… │\n",
       "│                     │                   │            │ bidirectional_11… │\n",
       "│                     │                   │            │ bidirectional_11… │\n",
       "│                     │                   │            │ bidirectional_12… │\n",
       "│                     │                   │            │ bidirectional_12… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>), │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,227,264</span> │ concatenate_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>),      │            │ concatenate_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)]      │            │ concatenate_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_lemma        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1091</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">420,035</span> │ dec_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_tag (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1091</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">420,035</span> │ dec_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_token         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_lemma         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_tag           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_token     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)  │     \u001b[38;5;34m32,730\u001b[0m │ input_token[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_lemma     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)  │     \u001b[38;5;34m32,730\u001b[0m │ input_lemma[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_tag       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)  │     \u001b[38;5;34m32,730\u001b[0m │ input_tag[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding_token[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_10        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_token[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding_lemma[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_11        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_lemma[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_12          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding_tag[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_12        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_tag[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_10    │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │     \u001b[38;5;34m48,640\u001b[0m │ dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │ not_equal_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)]       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_11    │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │     \u001b[38;5;34m48,640\u001b[0m │ dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │ not_equal_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)]       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_12    │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │     \u001b[38;5;34m48,640\u001b[0m │ dropout_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │ not_equal_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m),       │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)]       │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_10      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bidirectional_10… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ bidirectional_10… │\n",
       "│                     │                   │            │ bidirectional_11… │\n",
       "│                     │                   │            │ bidirectional_11… │\n",
       "│                     │                   │            │ bidirectional_12… │\n",
       "│                     │                   │            │ bidirectional_12… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ repeat_vector_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m384\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ concatenate_10[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mRepeatVector\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_12      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m414\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ repeat_vector_2[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_11      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bidirectional_10… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ bidirectional_10… │\n",
       "│                     │                   │            │ bidirectional_11… │\n",
       "│                     │                   │            │ bidirectional_11… │\n",
       "│                     │                   │            │ bidirectional_12… │\n",
       "│                     │                   │            │ bidirectional_12… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_lstm (\u001b[38;5;33mLSTM\u001b[0m)     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m384\u001b[0m), │  \u001b[38;5;34m1,227,264\u001b[0m │ concatenate_12[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m),      │            │ concatenate_10[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)]      │            │ concatenate_11[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_lemma        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m1091\u001b[0m)  │    \u001b[38;5;34m420,035\u001b[0m │ dec_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_tag (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m1091\u001b[0m)  │    \u001b[38;5;34m420,035\u001b[0m │ dec_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,311,444</span> (8.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,311,444\u001b[0m (8.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,311,444</span> (8.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,311,444\u001b[0m (8.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model3 = create_model_multioutput()\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b599fea-e5b9-423e-aa15-6101064d673e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 298ms/step - loss: 2.6724 - output_lemma_accuracy: 0.8305 - output_tag_accuracy: 0.7986 - val_loss: 0.7470 - val_output_lemma_accuracy: 0.9298 - val_output_tag_accuracy: 0.9169\n",
      "Epoch 2/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 298ms/step - loss: 0.6759 - output_lemma_accuracy: 0.9319 - output_tag_accuracy: 0.9281 - val_loss: 0.5101 - val_output_lemma_accuracy: 0.9400 - val_output_tag_accuracy: 0.9555\n",
      "Epoch 3/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 299ms/step - loss: 0.4759 - output_lemma_accuracy: 0.9422 - output_tag_accuracy: 0.9597 - val_loss: 0.3930 - val_output_lemma_accuracy: 0.9468 - val_output_tag_accuracy: 0.9745\n",
      "Epoch 4/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 301ms/step - loss: 0.3675 - output_lemma_accuracy: 0.9495 - output_tag_accuracy: 0.9757 - val_loss: 0.3349 - val_output_lemma_accuracy: 0.9503 - val_output_tag_accuracy: 0.9819\n",
      "Epoch 5/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 299ms/step - loss: 0.3180 - output_lemma_accuracy: 0.9529 - output_tag_accuracy: 0.9803 - val_loss: 0.3019 - val_output_lemma_accuracy: 0.9532 - val_output_tag_accuracy: 0.9868\n",
      "Epoch 6/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 299ms/step - loss: 0.2638 - output_lemma_accuracy: 0.9581 - output_tag_accuracy: 0.9883 - val_loss: 0.2563 - val_output_lemma_accuracy: 0.9578 - val_output_tag_accuracy: 0.9906\n",
      "Epoch 7/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 299ms/step - loss: 0.2253 - output_lemma_accuracy: 0.9613 - output_tag_accuracy: 0.9910 - val_loss: 0.2378 - val_output_lemma_accuracy: 0.9598 - val_output_tag_accuracy: 0.9917\n",
      "Epoch 8/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 300ms/step - loss: 0.2082 - output_lemma_accuracy: 0.9630 - output_tag_accuracy: 0.9910 - val_loss: 0.2101 - val_output_lemma_accuracy: 0.9638 - val_output_tag_accuracy: 0.9926\n",
      "Epoch 9/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 300ms/step - loss: 0.1776 - output_lemma_accuracy: 0.9667 - output_tag_accuracy: 0.9933 - val_loss: 0.1895 - val_output_lemma_accuracy: 0.9667 - val_output_tag_accuracy: 0.9937\n",
      "Epoch 10/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 300ms/step - loss: 0.1558 - output_lemma_accuracy: 0.9707 - output_tag_accuracy: 0.9937 - val_loss: 0.1698 - val_output_lemma_accuracy: 0.9694 - val_output_tag_accuracy: 0.9947\n"
     ]
    }
   ],
   "source": [
    "history = model3.fit(\n",
    "    [X1_train, X2_train, X3_train],\n",
    "    [Y1_train, Y2_train],\n",
    "    validation_data=([X1_dev, X2_dev, X3_dev], [Y1_dev, Y2_dev]),\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9dcd51bf-48c3-4ab2-b1a6-caf800ef8031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 87ms/step - loss: 0.1682 - output_lemma_accuracy: 0.9708 - output_tag_accuracy: 0.9937\n",
      "\n",
      "Test Loss: 0.16909939050674438, \n",
      "Test Lemma Accuracy: 0.971281111240387, \n",
      "Test Tag Accuracy: 0.9938541650772095\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_lemma_accuracy, test_tag_accuracy = model3.evaluate(\n",
    "    [X1_test, X2_test, X3_test],\n",
    "    [Y1_test, Y2_test]\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Loss: {test_loss}, \\nTest Lemma Accuracy: {test_lemma_accuracy}, \\nTest Tag Accuracy: {test_tag_accuracy}\")\n",
    "# print(f\"\\nTest Lemma Accuracy: {test_lemma_accuracy}, \\nTest Tag Accuracy: {test_tag_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ed4ead-d7f6-4a12-9b98-7ba188f8abce",
   "metadata": {},
   "source": [
    "### Prédictions et comparaison avec des données tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4d0edea5-742c-454c-bbe5-a8a052d19b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 78ms/step\n",
      "Example 1:\n",
      "Predicted lemmas: 현현현가[UNK]END[UNK]\n",
      "Predicted tags: NNP[UNK]END[UNK]\n",
      "True lemmas: 현대증권[UNK]END[UNK]\n",
      "True tags: NNP[UNK]END[UNK]\n",
      "\n",
      "Example 2:\n",
      "Predicted lemmas: 강민민[UNK]END[UNK]\n",
      "Predicted tags: NNP[UNK]END[UNK]\n",
      "True lemmas: 배성영[UNK]END[UNK]\n",
      "True tags: NNP[UNK]END[UNK]\n",
      "\n",
      "Example 3:\n",
      "Predicted lemmas: 사원++원++은[UNK]END[UNK]\n",
      "Predicted tags: NNG+NNG+JX[UNK]END[UNK]\n",
      "True lemmas: 수석+연구원+은[UNK]END[UNK]\n",
      "True tags: NNG+NNG+JX[UNK]END[UNK]\n",
      "\n",
      "Example 4:\n",
      "Predicted lemmas: \"[UNK]END[UNK]\n",
      "Predicted tags: SS[UNK]END[UNK]\n",
      "True lemmas: \"[UNK]END[UNK]\n",
      "True tags: SS[UNK]END[UNK]\n",
      "\n",
      "Example 5:\n",
      "Predicted lemmas: 국국[UNK]END[UNK]\n",
      "Predicted tags: NNP[UNK]END[UNK]\n",
      "True lemmas: 미국[UNK]END[UNK]\n",
      "True tags: NNP[UNK]END[UNK]\n",
      "\n",
      "Example 6:\n",
      "Predicted lemmas: 대민+대++의[UNK]END[UNK]\n",
      "Predicted tags: NNG+NNG+JKG[UNK]END[UNK]\n",
      "True lemmas: 통화+당국+의[UNK]END[UNK]\n",
      "True tags: NNG+NNG+JKG[UNK]END[UNK]\n",
      "\n",
      "Example 7:\n",
      "Predicted lemmas: 경++목간[UNK]END[UNK]\n",
      "Predicted tags: NNG+NNG[UNK]END[UNK]\n",
      "True lemmas: 경기+부양[UNK]END[UNK]\n",
      "True tags: NNG+NNG[UNK]END[UNK]\n",
      "\n",
      "Example 8:\n",
      "Predicted lemmas: 기기[UNK]END[UNK]\n",
      "Predicted tags: NNG[UNK]END[UNK]\n",
      "True lemmas: 기조[UNK]END[UNK]\n",
      "True tags: NNG[UNK]END[UNK]\n",
      "\n",
      "Example 9:\n",
      "Predicted lemmas: 지지[UNK]END[UNK]\n",
      "Predicted tags: NNG[UNK]END[UNK]\n",
      "True lemmas: 유지[UNK]END[UNK]\n",
      "True tags: NNG[UNK]END[UNK]\n",
      "\n",
      "Example 10:\n",
      "Predicted lemmas: 가능+들+과[UNK]END[UNK]\n",
      "Predicted tags: NR+XSN+JC[UNK]END[UNK]\n",
      "True lemmas: 가능+성+과[UNK]END[UNK]\n",
      "True tags: XR+XSN+JC[UNK]END[UNK]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model3.predict([X1_test, X2_test, X3_test])\n",
    "\n",
    "# prediction à séquences\n",
    "predicted_lemma_sequences = []\n",
    "predicted_tag_sequences = []\n",
    "\n",
    "for pred_lemma, pred_tag in zip(predictions[0], predictions[1]):\n",
    "    predicted_lemma_sequence = ''.join([reverse_vocab.get(idx) for idx in tf.argmax(pred_lemma, axis=-1).numpy()])\n",
    "    predicted_tag_sequence = ''.join([reverse_vocab.get(idx) for idx in tf.argmax(pred_tag, axis=-1).numpy()])\n",
    "    predicted_lemma_sequences.append(predicted_lemma_sequence)\n",
    "    predicted_tag_sequences.append(predicted_tag_sequence)\n",
    "\n",
    "# Ensure Y1_test and Y2_test are numpy arrays\n",
    "Y1_test_np = Y1_test.numpy()\n",
    "Y2_test_np = Y2_test.numpy()\n",
    "\n",
    "# Convert true sequences to human-readable format\n",
    "true_lemma_sequences = []\n",
    "true_tag_sequences = []\n",
    "\n",
    "for y1, y2 in zip(Y1_test_np, Y2_test_np):\n",
    "    true_lemma_sequence = ''.join([reverse_vocab.get(idx) for idx in y1])\n",
    "    true_tag_sequence = ''.join([reverse_vocab.get(idx) for idx in y2])\n",
    "    true_lemma_sequences.append(true_lemma_sequence)\n",
    "    true_tag_sequences.append(true_tag_sequence)\n",
    "\n",
    "# Print the predicted and true sequences for the first few examples in the test set\n",
    "for i in range(10):\n",
    "    print(f\"Example {i + 1}:\")\n",
    "    print(\"Predicted lemmas:\", predicted_lemma_sequences[i])\n",
    "    print(\"Predicted tags:\", predicted_tag_sequences[i])\n",
    "    print(\"True lemmas:\", true_lemma_sequences[i])\n",
    "    print(\"True tags:\", true_tag_sequences[i])\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "82e75543-f9eb-4b61-9392-1b18911129b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison between Predicted and True Sequences:\n",
      "           Lemmas  Tokens\n",
      "                         \n",
      "Common        329     126\n",
      "Different    1085     125\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame to represent common and different elements\n",
    "data = {\n",
    "    '': ['Common', 'Different'],\n",
    "    'Lemmas': [common_lemma_count, len(predicted_lemma_set) - common_lemma_count],\n",
    "    'Tokens': [common_token_count, len(predicted_token_set) - common_token_count]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index('', inplace=True)\n",
    "\n",
    "# Display DataFrame\n",
    "print(\"Comparison between Predicted and True Sequences:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b2cc4-c526-4650-b159-be18748e4008",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017427d3-9fd6-495d-a091-4a16838f46de",
   "metadata": {},
   "source": [
    "- Si le modèle basique commence à générer des résultats à peu près fiable, \n",
    "malgré mes recherches, je n'ai pas réussi à créer un modèle recursive pour les modèles avancés.\n",
    "- En ce qui concerne la génération, je n'ai pas très bien compris, ce qu'il faut générer:\n",
    "    - pour Brassens, c'est des génération à partir des lettres de l'alphabet\n",
    "    - ici, la tâche étant lemmatisation, à partir d'un token, le générateur devrait générer la forme lemmatisée des tokens.\n",
    "- le modèle multi-output semble avoir des quelques résultats correctes, par contre, je suis pas sûr de la façon de décoder\n",
    "\n",
    "### Pour améliorer\n",
    "- essayer avec normalisation des tokens\n",
    "- faire varier les hyperparamètres pour comparer les performances\n",
    "- matrices de confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bfb698",
   "metadata": {},
   "source": [
    "### Sitographie \n",
    "- [Keras sequence-to-sequence learning](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)\n",
    "- [Character-level recurrent sequence-to-sequence model](https://keras.io/examples/nlp/lstm_seq2seq/)\n",
    "- [Tokens to sequence](https://medium.com/geekculture/nlp-with-tensorflow-keras-explanation-and-tutorial-cae3554b1290)\n",
    "- [keras tv tuto](https://www.youtube.com/watch?v=gjjAyZWFkds&t=37)\n",
    "- [multi-output model](https://stackoverflow.com/questions/66845924/multi-input-multi-output-model-with-keras-functional-api)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e18eb3-7514-4902-927d-710688e55950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
