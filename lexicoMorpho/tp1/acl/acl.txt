



 Activity detection for information access to oral communication  Oral communication is ubiquitous and carries important information yet it is also time consuming to document. Given the development of storage media and networks one could just record and store a conversation for documentation. The question is, however, how an interesting information piece would be found in a large database . Traditional information retrieval techniques use a histogram of keywords as the document representation but oral communication may offer additional indices such as the time and place of the rejoinder and the attendance. An alternative index could be the activity such as discussing, planning, informing, story-telling, etc. This paper addresses the problem of the automatic detection of those activities in meeting situation and everyday rejoinders. Several extensions of this basic idea are being discussed and/or evaluated: Similar to activities one can define subsets of larger database and detect those automatically which is shown on a large database of TV shows . Emotions and other indices such as the dominance distribution of speakers might be available on the surface and could be used directly. Despite the small size of the databases used some results about the effectiveness of these indices can be obtained. 




 Dialogue Interaction with the DARPA Communicator Infrastructure: The Development of Useful Software  To support engaging human users in robust, mixed-initiative speech dialogue interactions which reach beyond current capabilities in dialogue systems , the DARPA Communicator program [1] is funding the development of a distributed message-passing infrastructure for dialogue systems which all Communicator participants are using. In this presentation, we describe the features of and requirements for a genuinely useful software infrastructure for this purpose. 




 Interlingua-Based Broad-Coverage Korean-to-English Translation in CCLING  At MIT Lincoln Laboratory, we have been developing a Korean-to-English machine translation system CCLINC (Common Coalition Language System at Lincoln Laboratory) . The CCLINC Korean-to-English translation system consists of two core modules , language understanding and generation modules mediated by a language neutral meaning representation called a semantic frame . The key features of the system include: (i) Robust efficient parsing of Korean (a verb final language with overt case markers , relatively free word order , and frequent omissions of arguments ). (ii) High quality translation via word sense disambiguation and accurate word order generation of the target language . (iii) Rapid system development and porting to new domains via knowledge-based automated acquisition of grammars . Having been trained on Korean newspaper articles on missiles and chemical biological warfare, the system produces the translation output sufficient for content understanding of the original document . 




 Is That Your Final Answer?  The purpose of this research is to test the efficacy of applying automated evaluation techniques , originally devised for the evaluation of human language learners , to the output of machine translation (MT) systems . We believe that these evaluation techniques will provide information about both the human language learning process , the translation process and the development of machine translation systems . This, the first experiment in a series of experiments, looks at the intelligibility of MT output . A language learning experiment showed that assessors can differentiate native from non-native language essays in less than 100 words . Even more illuminating was the factors on which the assessors made their decisions. We tested this to see if similar criteria could be elicited from duplicating the experiment using machine translation output . Subjects were given a set of up to six extracts of translated newswire text . Some of the extracts were expert human translations , others were machine translation outputs . The subjects were given three minutes per extract to determine whether they believed the sample output to be an expert human translation or a machine translation . Additionally, they were asked to mark the word at which they made this decision. The results of this experiment, along with a preliminary analysis of the factors involved in the decision making process will be presented here. 




 Listen-Communicate-Show (LCS): Spoken Language Command of Agent-based Remote Information Access  Listen-Communicate-Show (LCS) is a new paradigm for human interaction with data sources . We integrate a spoken language understanding system with intelligent mobile agents that mediate between users and information sources . We have built and will demonstrate an application of this approach called LCS-Marine . Using LCS-Marine , tactical personnel can converse with their logistics system to place a supply or information request. The request is passed to a mobile, intelligent agent for execution at the appropriate database . Requestors can also instruct the system to notify them when the status of a request changes or when a request is complete. We have demonstrated this capability in several field exercises with the Marines and are currently developing applications of this technology in new domains . 




 On Combining Language Models : Oracle Approach  In this paper, we address the problem of combining several language models (LMs) . We find that simple interpolation methods , like log-linear and linear interpolation , improve the performance but fall short of the performance of an oracle . The oracle knows the reference word string and selects the word string with the best performance (typically, word or semantic error rate ) from a list of word strings , where each word string has been obtained by using a different LM . Actually, the oracle acts like a dynamic combiner with hard decisions using the reference . We provide experimental results that clearly show the need for a dynamic language model combination to improve the performance further . We suggest a method that mimics the behavior of the oracle using a neural network or a decision tree . The method amounts to tagging LMs with confidence measures and picking the best hypothesis corresponding to the LM with the best confidence . 




 Towards an Intelligent Multilingual Keyboard System  This paper proposes a practical approach employing n-gram models and error-correction rules for Thai key prediction and Thai-English language identification . The paper also proposes rule-reduction algorithm applying mutual information to reduce the error-correction rules . Our algorithm reported more than 99% accuracy in both language identification and key prediction . 




 SPoT: A Trainable Sentence Planner  Sentence planning is a set of inter-related but distinct tasks, one of which is sentence scoping , i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into one or more sentences . In this paper, we present SPoT , a sentence planner , and a new methodology for automatically training SPoT on the basis of feedback provided by human judges . We reconceptualize the task into two distinct phases. First, a very simple, randomized sentence-plan-generator (SPG) generates a potentially large list of possible sentence plans for a given text-plan input . Second, the sentence-plan-ranker (SPR) ranks the list of output sentence plans , and then selects the top-ranked plan . The SPR uses ranking rules automatically learned from training data . We show that the trained SPR learns to select a sentence plan whose rating on average is only 5% worse than the top human-ranked sentence plan . 




 Low-cost, High-performance Translation Retrieval: Dumber is Better  In this paper, we compare the relative effects of segment order , segmentation and segment contiguity on the retrieval performance of a translation memory system . We take a selection of both bag-of-words and segment order-sensitive string comparison methods , and run each over both character- and word-segmented data , in combination with a range of local segment contiguity models (in the form of N-grams ). Over two distinct datasets , we find that indexing according to simple character bigrams produces a retrieval accuracy superior to any of the tested word N-gram models . Further,in their optimum configuration , bag-of-words methods are shown to be equivalent to segment order-sensitive methods in terms of retrieval accuracy , but much faster. We also provide evidence that our findings are scalable. 




 Guided Parsing of Range Concatenation Languages  The theoretical study of the range concatenation grammar [RCG] formalism has revealed many attractive properties which may be used in NLP . In particular, range concatenation languages [RCL] can be parsed in polynomial time and many classical grammatical formalisms can be translated into equivalent RCGs without increasing their worst-case parsing time complexity . For example, after translation into an equivalent RCG , any tree adjoining grammar can be parsed in O(n6) time . In this paper, we study a parsing technique whose purpose is to improve the practical efficiency of RCL parsers . The non-deterministic parsing choices of the main parser for a language L are directed by a guide which uses the shared derivation forest output by a prior RCL parser for a suitable superset of L . The results of a practical evaluation of this method on a wide coverage English grammar are given. 




 Extracting Paraphrases from a Parallel Corpus  While paraphrasing is critical both for interpretation and generation of natural language , current systems use manual or semi-automatic methods to collect paraphrases . We present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple English translations of the same source text . Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases . 




 Alternative Phrases and Natural Language Information Retrieval  This paper presents a formal analysis for a large class of words called alternative markers , which includes other (than) , such (as) , and besides . These words appear frequently enough in dialog to warrant serious attention , yet present natural language search engines perform poorly on queries containing them. I show that the performance of a search engine can be improved dramatically by incorporating an approximation of the formal analysis that is compatible with the search engine &apos;s operational semantics . The value of this approach is that as the operational semantics of natural language applications improve, even larger improvements are possible. 




 Extending Lambek grammars: a logical account of minimalist grammars  We provide a logical definition of Minimalist grammars , that are Stabler&apos;s formalization of Chomsky&apos;s minimalist program . Our logical definition leads to a neat relation to categorial grammar , (yielding a treatment of Montague semantics ), a parsing-as-deduction in a resource sensitive logic , and a learning algorithm from structured data (based on a typing-algorithm and type-unification ). Here we emphasize the connection to Montague semantics which can be viewed as a formal computation of the logical form . 




 Evaluating a Trainable Sentence Planner for a Spoken Dialogue System  Techniques for automatically training modules of a natural language generator have recently been proposed, but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rule-based approaches . In this paper We experimentally evaluate a trainable sentence planner for a spoken dialogue system by eliciting subjective human judgments . In order to perform an exhaustive comparison, we also evaluate a hand-crafted template-based generation component , two rule-based sentence planners , and two baseline sentence planners . We show that the trainable sentence planner performs better than the rule-based systems and the baselines , and as well as the hand-crafted system . 




 Using Machine Learning Techniques to Interpret WH-questions  We describe a set of supervised machine learning experiments centering on the construction of statistical models of WH-questions . These models , which are built from shallow linguistic features of questions , are employed to predict target variables which represent a user&apos;s informational goals . We report on different aspects of the predictive performance of our models , including the influence of various training and testing factors on predictive performance , and examine the relationships among the target variables. 




 Effective Utterance Classification with Unsupervised Phonotactic Models  This paper describes a method for utterance classification that does not require manual transcription of training data . The method combines domain independent acoustic models with off-the-shelf classifiers to give utterance classification performance that is surprisingly close to what can be achieved using conventional word-trigram recognition requiring manual transcription . In our method, unsupervised training is first used to train a phone n-gram model for a particular domain ; the output of recognition with this model is then passed to a phone-string classifier . The classification accuracy of the method is evaluated on three different spoken language system domains . 




 In Question Answering, Two Heads Are Better Than One  Motivated by the success of ensemble methods in machine learning and other areas of natural language processing , we developed a multi-strategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora . The answering agents adopt fundamentally different strategies, one utilizing primarily knowledge-based mechanisms and the other adopting statistical techniques . We present our multi-level answer resolution algorithm that combines results from the answering agents at the question, passage, and/or answer levels . Experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0% relative improvement over our baseline system in the number of questions correctly answered , and a 32.8% improvement according to the average precision metric . 




 Semantic Coherence Scoring Using an Ontology  In this paper we present ONTOSCORE , a system for scoring sets of concepts on the basis of an ontology . We apply our system to the task of scoring alternative speech recognition hypotheses (SRH) in terms of their semantic coherence . We conducted an annotation experiment and showed that human annotators can reliably differentiate between semantically coherent and incoherent speech recognition hypotheses . An evaluation of our system against the annotated data shows that, it successfully classifies 73.2% in a German corpus of 2.284 SRHs as either coherent or incoherent (given a baseline of 54.55%). 




 Statistical Phrase-Based Translation  We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models . Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models . Our empirical results, which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations . Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems. 




 A Generative Probabilistic OCR Model for NLP Applications  In this paper, we introduce a generative probabilistic optical character recognition (OCR) model that describes an end-to-end process in the noisy channel framework , progressing from generation of true text through its transformation into the noisy output of an OCR system . The model is designed for use in error correction , with a focus on post-processing the output of black-box OCR systems in order to make it more useful for NLP tasks . We present an implementation of the model based on finite-state models , demonstrate the model &apos;s ability to significantly reduce character and word error rate , and provide evaluation results involving automatic extraction of translation lexicons from printed text . 




 Statistical Sentence Condensation using Ambiguity Packing and Stochastic Disambiguation Methods for Lexical-Functional Grammar  We present an application of ambiguity packing and stochastic disambiguation techniques for Lexical-Functional Grammars (LFG) to the domain of sentence condensation . Our system incorporates a linguistic parser/generator for LFG , a transfer component for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output selection . Furthermore, we propose the use of standard parser evaluation methods for automatically evaluating the summarization quality of sentence condensation systems . An experimental evaluation of summarization quality shows a close correlation between the automatic parse-based evaluation and a manual evaluation of generated strings . Overall summarization quality of the proposed system is state-of-the-art, with guaranteed grammaticality of the system output due to the use of a constraint-based parser/generator . 




 Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network  We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation , (ii) broad use of lexical features , including jointly conditioning on multiple consecutive words , (iii) effective use of priors in conditional loglinear models , and (iv) fine-grained modeling of unknown word features . Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ , an error reduction of 4.4% on the best previous single automatically learned tagging result. 




 Getting More Mileage from Web Text Sources for Conversational Speech Language Modeling using Class-Dependent Mixtures  Sources of training data suitable for language modeling of conversational speech are limited. In this paper, we show how training data can be supplemented with text from the web filtered to match the style and/or topic of the target recognition task , but also that it is possible to get bigger performance gains from the data by using class-dependent interpolation of N-grams . 




 Adaptation Using Out-of-Domain Corpus within EBMT  In order to boost the translation quality of EBMT based on a small-sized bilingual corpus , we use an out-of-domain bilingual corpus and, in addition, the language model of an in-domain monolingual corpus . We conducted experiments with an EBMT system . The two evaluation measures of the BLEU score and the NIST score demonstrated the effect of using an out-of-domain bilingual corpus and the possibility of using the language model . 




 Unsupervised Learning of Morphology for English and Inuktitut  We describe a simple unsupervised technique for learning morphology by identifying hubs in an automaton . For our purposes, a hub is a node in a graph with in-degree greater than one and out-degree greater than one. We create a word-trie , transform it into a minimal DFA , then identify hubs . Those hubs mark the boundary between root and suffix , achieving similar performance to more complex mixtures of techniques. 




 Word Alignment with Cohesion Constraint  We present a syntax-based constraint for word alignment , known as the cohesion constraint . It requires disjoint English phrases to be mapped to non-overlapping intervals in the French sentence . We evaluate the utility of this constraint in two different algorithms. The results show that it can provide a significant improvement in alignment quality . 




 Bootstrapping for Named Entity Tagging Using Concept-based Seeds  A novel bootstrapping approach to Named Entity (NE) tagging using concept-based seeds and successive learners is presented. This approach only requires a few common noun or pronoun seeds that correspond to the concept for the targeted NE , e.g. he/she/man/woman for PERSON NE . The bootstrapping procedure is implemented as training two successive learners . First, decision list is used to learn the parsing-based NE rules . Then, a Hidden Markov Model is trained on a corpus automatically tagged by the first learner . The resulting NE system approaches supervised NE performance for some NE types . 




 A Phrase-Based Unigram Model for Statistical Machine Translation  In this paper, we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models . The units of translation are blocks - pairs of phrases . During decoding , we use a block unigram model and a word-based trigram language model . During training , the blocks are learned from source interval projections using an underlying word alignment . We show experimental results on block selection criteria based on unigram counts and phrase length. 




 Cooperative Model Based Language Understanding in Dialogue  In this paper, we propose a novel Cooperative Model for natural language understanding in a dialogue system . We build this based on both Finite State Model (FSM) and Statistical Learning Model (SLM) . FSM provides two strategies for language understanding and have a high accuracy but little robustness and flexibility. Statistical approach is much more robust but less accurate. Cooperative Model incorporates all the three strategies together and thus can suppress all the shortcomings of different strategies and has all the advantages of the three strategies. 




 JAVELIN: A Flexible, Planner-Based Architecture for Question Answering  The JAVELIN system integrates a flexible, planning-based architecture with a variety of language processing modules to provide an open-domain question answering capability on free text . The demonstration will focus on how JAVELIN processes questions and retrieves the most likely answer candidates from the given text corpus . The operation of the system will be explained in depth through browsing the repository of data objects created by the system during each question answering session . 




 Using Predicate-Argument Structures for Information Extraction  In this paper we present a novel, customizable : IE paradigm that takes advantage of predicate-argument structures . We also introduce a new way of automatically identifying predicate argument structures , which is central to our IE paradigm . It is based on: (1) an extended set of features ; and (2) inductive decision tree learning . The experimental results prove our claim that accurate predicate-argument structures enable high quality IE results. 




 Hierarchical Directed Acyclic Graph Kernel: Methods for Structured Natural Language Data  This paper proposes the Hierarchical Directed Acyclic Graph (HDAG) Kernel for structured natural language data . The HDAG Kernel directly accepts several levels of both chunks and their relations , and then efficiently computes the weighed sum of the number of common attribute sequences of the HDAGs . We applied the proposed method to question classification and sentence alignment tasks to evaluate its performance as a similarity measure and a kernel function . The results of the experiments demonstrate that the HDAG Kernel is superior to other kernel functions and baseline methods . 




 Clustering Polysemic Subcategorization Frame Distributions Semantically  Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data . We describe a new approach which involves clustering subcategorization frame (SCF) distributions using the Information Bottleneck and nearest neighbour methods. In contrast to previous work, we particularly focus on clustering polysemic verbs . A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters , offering us a good insight into the potential and limitations of semantically classifying undisambiguated SCF data . 




 A Machine Learning Approach to Pronoun Resolution in Spoken Dialogue  We apply a decision tree based approach to pronoun resolution in spoken dialogue . Our system deals with pronouns with NP- and non-NP-antecedents . We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features . We evaluate the system on twenty Switchboard dialogues and show that it compares well to Byron&apos;s (2002) manually tuned system . 




 Optimizing Story Link Detection is not Equivalent to Optimizing New Event Detection  Link detection has been regarded as a core technology for the Topic Detection and Tracking tasks of new event detection . In this paper we formulate story link detection and new event detection as information retrieval task and hypothesize on the impact of precision and recall on both systems. Motivated by these arguments, we introduce a number of new performance enhancing techniques including part of speech tagging , new similarity measures and expanded stop lists . Experimental results validate our hypothesis. 




 Corpus-based Discourse Understanding in Spoken Dialogue Systems  This paper concerns the discourse understanding process in spoken dialogue systems . This process enables the system to understand user utterances based on the context of a dialogue . Since multiple candidates for the understanding result can be obtained for a user utterance due to the ambiguity of speech understanding , it is not appropriate to decide on a single understanding result after each user utterance . By holding multiple candidates for understanding results and resolving the ambiguity as the dialogue progresses, the discourse understanding accuracy can be improved. This paper proposes a method for resolving this ambiguity based on statistical information obtained from dialogue corpora . Unlike conventional methods that use hand-crafted rules , the proposed method enables easy design of the discourse understanding process . Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple candidates for understanding results is effective. 




 Flexible Guidance Generation using User Model in Spoken Dialogue Systems  We address appropriate user modeling in order to generate cooperative responses to each user in spoken dialogue systems . Unlike previous studies that focus on user &apos;s knowledge or typical kinds of users , the user model we propose is more comprehensive. Specifically, we set up three dimensions of user models : skill level to the system, knowledge level on the target domain and the degree of hastiness . Moreover, the models are automatically derived by decision tree learning using real dialogue data collected by the system. We obtained reasonable classification accuracy for all dimensions. Dialogue strategies based on the user modeling are implemented in Kyoto city bus information system that has been developed at our laboratory. Experimental evaluation shows that the cooperative responses adaptive to individual users serve as good guidance for novice users without increasing the dialogue duration for skilled users . 




 Unsupervised Learning of Arabic Stemming using a Parallel Corpus  This paper presents an unsupervised learning approach to building a non-English (Arabic) stemmer . The stemming model is based on statistical machine translation and it uses an English stemmer and a small (10K sentences) parallel corpus as its sole training resources . No parallel text is needed after the training phase . Monolingual, unannotated text can be used to further improve the stemmer by allowing it to adapt to a desired domain or genre . Examples and results will be given for Arabic , but the approach is applicable to any language that needs affix removal . Our resource-frugal approach results in 87.5% agreement with a state of the art, proprietary Arabic stemmer built using rules , affix lists , and human annotated text , in addition to an unsupervised component . Task-based evaluation using Arabic information retrieval indicates an improvement of 22-38% in average precision over unstemmed text , and 96% of the performance of the proprietary stemmer above. 




 Language Model Based Arabic Word Segmentation  We approximate Arabic&apos;s rich morphology by a model that a word consists of a sequence of morphemes in the pattern prefix*-stem-suffix* (* denotes zero or more occurrences of a morpheme ). Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus . The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input . The language model is initially estimated from a small manually segmented corpus of about 110,000 words . To improve the segmentation accuracy , we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus , and re-estimate the model parameters with the expanded vocabulary and training corpus . The resulting Arabic word segmentation system achieves around 97% exact match accuracy on a test corpus containing 28,449 word tokens . We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest. 




 Exploiting Parallel Texts for Word Sense Disambiguation: An Empirical Study  
A central problem of word sense disambiguation (WSD) is the lack of manually sense-tagged data required for supervised learning . In this paper, we evaluate an approach to automatically acquire sense-tagged training data from English-Chinese parallel corpora , which are then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task . Our investigation reveals that this method of acquiring sense-tagged data is promising. On a subset of the most difficult SENSEVAL-2 nouns , the accuracy difference between the two approaches is only 14.0%, and the difference could narrow further to 6.5% if we disregard the advantage that manually sense-tagged data have in their sense coverage . Our analysis also highlights the importance of the issue of domain dependence in evaluating WSD programs . 




 Towards a Resource for Lexical Semantics: A Large German Corpus with Extensive Semantic Annotation  We describe the ongoing construction of a large, semantically annotated corpus resource as reliable basis for the large-scale acquisition of word-semantic information , e.g. the construction of domain-independent lexica . The backbone of the annotation are semantic roles in the frame semantics paradigm . We report experiences and evaluate the annotated data from the first project stage. On this basis, we discuss the problems of vagueness and ambiguity in semantic annotation . 




 Towards a Model of Face-to-Face Grounding  We investigate the verbal and nonverbal means for grounding , and propose a design for embodied conversational agents that relies on both kinds of signals to establish common ground in human-computer interaction . We analyzed eye gaze , head nods and attentional focus in the context of a direction-giving task . The distribution of nonverbal behaviors differed depending on the type of dialogue move being grounded, and the overall pattern reflected a monitoring of lack of negative feedback . Based on these results, we present an ECA that uses verbal and nonverbal grounding acts to update dialogue state . 




 Comparison between CFG filtering techniques for LTAG and HPSG  An empirical comparison of CFG filtering techniques for LTAG and HPSG is presented. We demonstrate that an approximation of HPSG produces a more effective CFG filter than that of LTAG . We also investigate the reason for that difference. 




 Lower and higher estimates of the number of &amp;quot;true analogies&amp;quot; between sentences contained in a large multilingual corpus  The reality of analogies between words is refuted by noone (e.g., I walked is to to walk as I laughed is to to laugh, noted I walked : to walk :: I laughed : to laugh). But computational linguists seem to be quite dubious about analogies between sentences : they would not be enough numerous to be of any use. We report experiments conducted on a multilingual corpus to estimate the number of analogies among the sentences that it contains. We give two estimates, a lower one and a higher one. As an analogy must be valid on the level of form as well as on the level of meaning , we relied on the idea that translation should preserve meaning to test for similar meanings . 




 Evaluating Multiple Aspects of Coherence in Student Essays  CriterionSM Online Essay Evaluation Service includes a capability that labels sentences in student writing with essay-based discourse elements (e.g., thesis statements ). We describe a new system that enhances Criterion &apos;s capability, by evaluating multiple aspects of coherence in essays . This system identifies features of sentences based on semantic similarity measures and discourse structure . A support vector machine uses these features to capture breakdowns in coherence due to relatedness to the essay question and relatedness between discourse elements . Intra-sentential quality is evaluated with rule-based heuristics . Results indicate that the system yields higher performance than a baseline on all three aspects.  




 Improving Multilingual Summarization: Using Redundancy in the Input to Correct MT errors  In this paper, we use the information redundancy in multilingual input to correct errors in machine translation and thus improve the quality of multilingual summaries . We consider the case of multi-document summarization , where the input documents are in Arabic , and the output summary is in English . Typically, information that makes it to a summary appears in many different lexical-syntactic forms in the input documents . Further, the use of multiple machine translation systems provides yet more redundancy , yielding different ways to realize that information in English . We demonstrate how errors in the machine translations of the input Arabic documents can be corrected by identifying and generating from such redundancy , focusing on noun phrases . 




 A Maximum Entropy Word Aligner for Arabic-English Machine Translation  This paper presents a maximum entropy word alignment algorithm for Arabic-English based on supervised training data . We demonstrate that it is feasible to create training material for problems in machine translation and that a mixture of supervised and unsupervised methods yields superior performance . The probabilistic model used in the alignment directly models the link decisions . Significant improvement over traditional word alignment techniques is shown as well as improvement on several machine translation tests . Performance of the algorithm is contrasted with human annotation performance . 




 Translating with non-contiguous phrases  This paper presents a phrase-based statistical machine translation method , based on non-contiguous phrases , i.e. phrases with gaps. A method for producing such phrases from a word-aligned corpora is proposed. A statistical translation model is also presented that deals such phrases , as well as a training method based on the maximization of translation accuracy , as measured with the NIST evaluation metric . Translations are produced by means of a beam-search decoder . Experimental results are presented, that demonstrate how the proposed method allows to better generalize from the training data . 




 Automatically Evaluating Answers to Definition Questions  Following recent developments in the automatic evaluation of machine translation and document summarization , we present a similar approach, implemented in a measure called POURPRE , for automatically evaluating answers to definition questions . Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system&apos;s response. The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address with this work. Experiments with the TREC 2003 and TREC 2004 QA tracks indicate that rankings produced by our metric correlate highly with official rankings , and that POURPRE outperforms direct application of existing metrics. 




 Pattern Visualization for Machine Translation Output  We describe a method for identifying systematic patterns in translation data using part-of-speech tag sequences . We incorporate this analysis into a diagnostic tool intended for developers of machine translation systems , and demonstrate how our application can be used by developers to explore patterns in machine translation output . 




 Evaluating the Word Sense Disambiguation Performance of Statistical Machine Translation  We present the first known empirical test of an increasingly common speculative claim, by evaluating a representative Chinese-to-English SMT model directly on word sense disambiguation performance , using standard WSD evaluation methodology and datasets from the Senseval-3 Chinese lexical sample task . Much effort has been put in designing and evaluating dedicated word sense disambiguation (WSD) models , in particular with the Senseval series of workshops. At the same time, the recent improvements in the BLEU scores of statistical machine translation (SMT) suggests that SMT models are good at predicting the right translation of the words in source language sentences . Surprisingly however, the WSD accuracy of SMT models has never been evaluated and compared with that of the dedicated WSD models . We present controlled experiments showing the WSD accuracy of current typical SMT models to be significantly lower than that of all the dedicated WSD models considered. This tends to support the view that despite recent speculative claims to the contrary, current SMT models do have limitations in comparison with dedicated WSD models , and that SMT should benefit from the better predictions made by the WSD models . 




 Statistical Machine Translation Part I: Hands-On Introduction  Statistical machine translation (SMT) is currently one of the hot spots in natural language processing . Over the last few years dramatic improvements have been made, and a number of comparative evaluations have shown, that SMT gives competitive results to rule-based translation systems , requiring significantly less development time. This is particularly important when building translation systems for new language pairs or new domains . This workshop is intended to give an introduction to statistical machine translation with a focus on practical considerations. Participants should be able, after attending this workshop, to set out building an SMT system themselves and achieving good baseline results in a short time. The tutorial will cover the basics of SMT : Theory will be put into practice. STTK , a statistical machine translation tool kit , will be introduced and used to build a working translation system . STTK has been developed by the presenter and co-workers over a number of years and is currently used as the basis of CMU&apos;s SMT system . It has also successfully been coupled with rule-based and example based machine translation modules to build a multi engine machine translation system . The source code of the tool kit will be made available. 




 Harvesting the Bitexts of the Laws of Hong Kong From the Web  In this paper we present our recent work on harvesting English-Chinese bitexts of the laws of Hong Kong from the Web and aligning them to the subparagraph level via utilizing the numbering system in the legal text hierarchy . Basic methodology and practical techniques are reported in detail. The resultant bilingual corpus , 10.4M English words and 18.3M Chinese characters , is an authoritative and comprehensive text collection covering the specific and special domain of HK laws. It is particularly valuable to empirical MT research . This piece of work has also laid a foundation for exploring and harvesting English-Chinese bitexts in a larger volume from the Web . 




 Using Machine Translation Evaluation Techniques to Determine Sentence-level Semantic Equivalence  The task of machine translation (MT) evaluation is closely related to the task of sentence-level semantic equivalence classification . This paper investigates the utility of applying standard MT evaluation methods (BLEU, NIST, WER and PER) to building classifiers to predict semantic equivalence and entailment . We also introduce a novel classification method based on PER which leverages part of speech information of the words contributing to the word matches and non-matches in the sentence . Our results show that MT evaluation techniques are able to produce useful features for paraphrase classification and to a lesser extent entailment . Our technique gives a substantial improvement in paraphrase classification accuracy over all of the other models used in the experiments. 




 Automatic generation of paraphrases to be used as translation references in objective evaluation measures of machine translation  We propose a method that automatically generates paraphrase sets from seed sentences to be used as reference sets in objective machine translation evaluation measures like BLEU and NIST . We measured the quality of the paraphrases produced in an experiment, i.e., (i) their grammaticality : at least 99% correct sentences ; (ii) their equivalence in meaning : at least 96% correct paraphrases either by meaning equivalence or entailment ; and, (iii) the amount of internal lexical and syntactical variation in a set of paraphrases : slightly superior to that of hand-produced sets . The paraphrase sets produced by this method thus seem adequate as reference sets to be used for MT evaluation . 




 Annotating Honorifics Denoting Social Ranking of Referents  This paper proposes an annotating scheme that encodes honorifics (respectful words). Honorifics are used extensively in Japanese , reflecting the social relationship (e.g. social ranks and age) of the referents . This referential information is vital for resolving zero pronouns and improving machine translation outputs . Annotating honorifics is a complex task that involves identifying a predicate with honorifics , assigning ranks to referents of the predicate , calibrating the ranks , and connecting referents with their predicates . 




 Discriminative Reranking for Natural Language Parsing  This article considers approaches which rerank the output of an existing probabilistic parser . The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses . A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features , without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account . We introduce a new method for the reranking task , based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank . The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model . The new model achieved 89.75% F-measure , a 13% relative decrease in F-measure error over the baseline model's score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data . Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach . We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on feature selection methods within log-linear (maximum-entropy) models . Although the experiments in this article are on natural language parsing (NLP) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example, speech recognition , machine translation , or natural language generation . 




 Improving Machine Translation Performance by Exploiting Non-Parallel Corpora   We present a novel method for discovering parallel sentences in comparable, non-parallel corpora . We train a maximum entropy classifier that, given a pair of sentences , can reliably determine whether or not they are translations of each other. Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora . We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system . We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words ) and exploiting a large non-parallel corpus . Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available. 




 Scaling Phrase-Based Statistical Machine Translation to Larger Corpora and Longer Phrases  In this paper we describe a novel data structure for phrase-based statistical machine translation which allows for the retrieval of arbitrarily long phrases while simultaneously using less memory than is required by current decoder implementations. We detail the computational complexity and average retrieval times for looking up phrase translations in our suffix array-based data structure . We show how sampling can be used to reduce the retrieval time by orders of magnitude with no loss in translation quality . 




 Dependency Treelet Translation: Syntactically Informed Phrasal SMT  We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation . This method requires a source-language dependency parser , target language word segmentation and an unsupervised word alignment component . We align a parallel corpus , project the source dependency parse onto the target sentence , extract dependency treelet translation pairs , and train a tree-based ordering model . We describe an efficient decoder and show that using these tree-based models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser . 




 Word Sense Disambiguation vs. Statistical Machine Translation  We directly investigate a subject of much recent debate: do word sense disambigation models help statistical machine translation quality ? We present empirical results casting doubt on this common, but unproved, assumption. Using a state-of-the-art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system , we find that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone. Error analysis suggests several key factors behind this surprising finding, including inherent limitations of current statistical MT architectures . 




 Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars  Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data . In this paper, we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar . Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees . We first introduce our approach to inducing such a grammar from parallel corpora . Second, we describe the graphical model for the machine translation task , which can also be viewed as a stochastic tree-to-tree transducer . We introduce a polynomial time decoding algorithm for the model . We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software . The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality . 




 A Localized Prediction Model for Statistical Machine Translation  In this paper, we present a novel training method for a localized phrase-based prediction model for statistical machine translation (SMT) . The model predicts blocks with orientation to handle local phrase re-ordering . We use a maximum likelihood criterion to train a log-linear block bigram model which uses real-valued features (e.g. a language model score ) as well as binary features based on the block identities themselves, e.g. block bigram features. Our training algorithm can easily handle millions of features . The best system obtains a 18.6% improvement over the baseline on a standard Arabic-English translation task . 




 Paraphrasing with Bilingual Parallel Corpora  Previous work has used monolingual parallel corpora to extract and generate paraphrases . We show that this task can be done using bilingual parallel corpora , a much more commonly available resource . Using alignment techniques from phrase-based statistical machine translation , we show how paraphrases in one language can be identified using a phrase in another language as a pivot. We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities , and show how it can be refined to take contextual information into account. We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments , and contrast the quality with paraphrases extracted from automatic alignments . 




 Dependency-Based Statistical Machine Translation  We present a Czech-English statistical machine translation system which performs tree-to-tree translation of dependency structures . The only bilingual resource required is a sentence-aligned parallel corpus . All other resources are monolingual . We also refer to an evaluation method and plan to compare our system&apos;s output with a benchmark system . 




 Word Sense Induction: Triplet-Based Clustering and Automatic Evaluation  In this paper a novel solution to automatic and unsupervised word sense induction (WSI) is introduced. It represents an instantiation of the one sense per collocation observation (Gale et al., 1992). Like most existing approaches it utilizes clustering of word co-occurrences . This approach differs from other approaches to WSI in that it enhances the effect of the one sense per collocation observation by using triplets of words instead of pairs. The combination with a two-step clustering process using sentence co-occurrences as features allows for accurate results. Additionally, a novel and likewise automatic and unsupervised evaluation method inspired by Schutze&apos;s (1992) idea of evaluation of word sense disambiguation algorithms is employed. Offering advantages like reproducability and independency of a given biased gold standard it also enables automatic parameter optimization of the WSI algorithm . 




 Addressee Identification in Face-to-Face Meetings  We present results on addressee identification in four-participants face-to-face meetings using Bayesian Network and Naive Bayes classifiers . First, we investigate how well the addressee of a dialogue act can be predicted based on gaze , utterance and conversational context features . Then, we explore whether information about meeting context can aid classifiers &apos; performances . Both classifiers perform the best when conversational context and utterance features are combined with speaker&apos;s gaze information . The classifiers show little gain from information about meeting context . 




 CDER: Efficient MT Evaluation Using Block Movements  Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks. In many cases though such movements still result in correct or almost correct sentences . In this paper, we will present a new evaluation measure which explicitly models block reordering as an edit operation . Our measure can be exactly calculated in quadratic time . Furthermore, we will show how some evaluation measures can be improved by the introduction of word-dependent substitution costs . The correlation of the new measure with human judgment has been investigated systematically on two different language pairs . The experimental results will show that it significantly outperforms state-of-the-art approaches in sentence-level correlation . Results from experiments with word dependent substitution costs will demonstrate an additional increase of correlation between automatic evaluation measures and human judgment .  




 Automatic Segmentation of Multiparty Dialogue  In this paper, we investigate the problem of automatically predicting segment boundaries in spoken multiparty dialogue . We extend prior work in two ways. We first apply approaches that have been proposed for predicting top-level topic shifts to the problem of identifying subtopic boundaries . We then explore the impact on performance of using ASR output as opposed to human transcription . Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks: (1) for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results, (2) for predicting top-level boundaries , the machine learning approach that combines lexical-cohesion and conversational features performs best, and (3) conversational cues , such as cue phrases and overlapping speech , are better indicators for the top-level prediction task. We also find that the transcription errors inevitable in ASR output have a negative impact on models that combine lexical-cohesion and conversational features , but do not change the general preference of approach for the two tasks.  




 Ensemble Methods for Unsupervised WSD  Combination methods are an effective way of improving system performance . This paper examines the benefits of system combination for unsupervised WSD . We investigate several voting- and arbiter-based combination strategies over a diverse pool of unsupervised WSD systems . Our combination methods rely on predominant senses which are derived automatically from raw text . Experiments using the SemCor and Senseval-3 data sets demonstrate that our ensembles yield significantly better results when compared with state-of-the-art.  




 An Improved Redundancy Elimination Algorithm for Underspecified Representations  We present an efficient algorithm for the redundancy elimination problem : Given an underspecified semantic representation (USR) of a scope ambiguity , compute an USR with fewer mutually equivalent readings . The algorithm operates on underspecified chart representations which are derived from dominance graphs ; it can be applied to the USRs computed by large-scale grammars . We evaluate the algorithm on a corpus , and show that it reduces the degree of ambiguity significantly while taking negligible runtime. 




 Using Machine Learning Techniques to Build a Comma Checker for Basque  In this paper, we describe the research using machine learning techniques to build a comma checker to be integrated in a grammar checker for Basque . After several experiments, and trained with a little corpus of 100,000 words , the system guesses correctly not placing commas with a precision of 96% and a recall of 98%. It also gets a precision of 70% and a recall of 49% in the task of placing commas . Finally, we have shown that these results can be improved using a bigger and a more homogeneous corpus to train, that is, a bigger corpus written by one unique author . 




 Unsupervised Relation Disambiguation Using Spectral Clustering  This paper presents an unsupervised learning approach to disambiguate various relations between named entities by use of various lexical and syntactic features from the contexts . It works by calculating eigenvectors of an adjacency graph &apos;s Laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors . Experiment results on ACE corpora show that this spectral clustering based approach outperforms the other clustering methods . 




 Automatic Construction of Polarity-tagged Corpus from HTML Documents  This paper proposes a novel method of building polarity-tagged corpus from HTML documents . The characteristics of this method is that it is fully automatic and can be applied to arbitrary HTML documents . The idea behind our method is to utilize certain layout structures and linguistic pattern . By using them, we can automatically extract such sentences that express opinion. In our experiment, the method could construct a corpus consisting of 126,610 sentences . 




 Intelligent Access to Text: Integrating Information Extraction Technology into Text Browsers  
 In this paper we show how two standard outputs from information extraction (IE) systems - named entity annotations and scenario templates - can be used to enhance access to text collections via a standard text browser . We describe how this information is used in a prototype system designed to support information workers &apos; access to a pharmaceutical news archive as part of their industry watch function. We also report results of a preliminary, qualitative user evaluation of the system, which while broadly positive indicates further work needs to be done on the interface to make users aware of the increased potential of IE-enhanced text browsers . 




 Natural Language Generation in Dialog Systems  
 Recent advances in Automatic Speech Recognition technology have put the goal of naturally sounding dialog systems within reach. However, the improved speech recognition has brought to light a new problem: as dialog systems understand more of what the user tells them, they need to be more sophisticated at responding to the user . The issue of system response to users has been extensively studied by the natural language generation community , though rarely in the context of dialog systems . We show how research in generation can be adapted to dialog systems , and how the high cost of hand-crafting knowledge-based generation systems can be overcome by employing machine learning techniques . 




 A Three-Tiered Evaluation Approach for Interactive Spoken Dialogue Systems  
 We describe a three-tiered approach for evaluation of spoken dialogue systems . The three tiers measure user satisfaction , system support of mission success and component performance . We describe our use of this approach in numerous fielded user studies conducted with the U.S. military. 




 TAP-XL: An Automated Analyst&apos;s Assistant  
 The TAP-XL Automated Analyst&apos;s Assistant is an application designed to help an English -speaking analyst write a topical report , culling information from a large inflow of multilingual, multimedia data . It gives users the ability to spend their time finding more data relevant to their task, and gives them translingual reach into other languages by leveraging human language technology . 




 Some Computational Complexity Results for Synchronous Context-Free Grammars  
 This paper investigates some computational problems associated with probabilistic translation models that have recently been adopted in the literature on machine translation . These models can be viewed as pairs of probabilistic context-free grammars working in a &apos;synchronous&apos; way. Two hardness results for the class NP are reported, along with an exponential time lower-bound for certain classes of algorithms that are currently used in the literature. 




 BLEU in characters: towards automatic MT evaluation in languages without word delimiters  
 Automatic evaluation metrics for Machine Translation (MT) systems , such as BLEU or NIST , are now well established. Yet, they are scarcely used for the assessment of language pairs like English-Chinese or English-Japanese , because of the word segmentation problem . This study establishes the equivalence between the standard use of BLEU in word n-grams and its application at the character level. The use of BLEU at the character level eliminates the word segmentation problem : it makes it possible to directly compare commercial systems outputting unsegmented texts with, for instance, statistical MT systems which usually segment their outputs . 




 Interactively Exploring a Machine Translation Model  
 This paper describes a method of interactively visualizing and directing the process of translating a sentence . The method allows a user to explore a model of syntax-based statistical machine translation (MT) , to understand the model &apos;s strengths and weaknesses, and to compare it to other MT systems . Using this visualization method , we can find and address conceptual and practical problems in an MT system . In our demonstration at ACL , new users of our tool will drive a syntax-based decoder for themselves. 




 Computational Complexity of Statistical Machine Translation  
 In this paper we study a set of problems that are of considerable importance to Statistical Machine Translation (SMT) but which have not been addressed satisfactorily by the SMT research community . Over the last decade, a variety of SMT algorithms have been built and empirically tested whereas little is known about the computational complexity of some of the fundamental problems of SMT . Our work aims at providing useful insights into the the computational complexity of those problems. We prove that while IBM Models 1-2 are conceptually and computationally simple, computations involving the higher (and more useful) models are hard . Since it is unlikely that there exists a polynomial time solution for any of these hard problems (unless P = NP and P#P = P ), our results highlight and justify the need for developing polynomial time approximations for these computations. We also discuss some practical ways of dealing with complexity . 




 Structuring Knowledge for Reference Generation: A Clustering Algorithm  
 This paper discusses two problems that arise in the Generation of Referring Expressions : (a) numeric-valued attributes , such as size or location; (b) perspective-taking in reference . Both problems, it is argued, can be resolved if some structure is imposed on the available knowledge prior to content determination . We describe a clustering algorithm which is sufficiently general to be applied to these diverse problems, discuss its application, and evaluate its performance. 




 Answering the Question You Wish They Had Asked: The Impact of Paraphrasing for Question Answering  
 State-of-the-art Question Answering (QA) systems are very sensitive to variations in the phrasing of an information need . Finding the preferred language for such a need is a valuable task. We investigate that claim by adopting a simple MT-based paraphrasing technique and evaluating QA system performance on paraphrased questions . We found a potential increase of 35% in MRR with respect to the original question . 




 A Comparison of Tagging Strategies for Statistical Information Extraction  
 There are several approaches that model information extraction as a token classification task , using various tagging strategies to combine multiple tokens . We describe the tagging strategies that can be found in the literature and evaluate their relative performances. We also introduce a new strategy, called Begin/After tagging or BIA , and show that it is competitive to the best other strategies.  




 InfoMagnets: Making Sense of Corpus Data  
 We introduce a new interactive corpus exploration tool called InfoMagnets . InfoMagnets aims at making exploratory corpus analysis accessible to researchers who are not experts in text mining . As evidence of its usefulness and usability, it has been used successfully in a research context to uncover relationships between language and behavioral patterns in two distinct domains: tutorial dialogue (Kumar et al., submitted) and on-line communities (Arguello et al., 2006). As an educational tool , it has been used as part of a unit on protocol analysis in an Educational Research Methods course . 




 Polarized Unification Grammars  
 This paper proposes a generic mathematical formalism for the combination of various structures : strings , trees , dags , graphs , and products of them. The polarization of the objects of the elementary structures controls the saturation of the final structure . This formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as rewriting systems , dependency grammars , TAG , HPSG and LFG .  




 Word Vectors and Two Kinds of Similarity  
 This paper examines what kind of similarity between words can be represented by what kind of word vectors in the vector space model . Through two experiments, three methods for constructing word vectors , i.e., LSA-based, cooccurrence-based and dictionary-based methods , were compared in terms of the ability to represent two kinds of similarity , i.e., taxonomic similarity and associative similarity . The result of the comparison was that the dictionary-based word vectors better reflect taxonomic similarity , while the LSA-based and the cooccurrence-based word vectors better reflect associative similarity .  




 Investigations on Event-Based Summarization  
 We investigate independent and relevant event-based extractive mutli-document summarization approaches . In this paper, events are defined as event terms and associated event elements . With independent approach, we identify important contents by frequency of events . With relevant approach, we identify important contents by PageRank algorithm on the event map constructed from documents . Experimental results are encouraging. 




 FERRET: Interactive Question-Answering for Real-World Environments  
 This paper describes FERRET , an interactive question-answering (Q/A) system designed to address the challenges of integrating automatic Q/A applications into real-world environments. FERRET utilizes a novel approach to Q/A known as predictive questioning which attempts to identify the questions (and answers ) that users need by analyzing how a user interacts with a system while gathering information related to a particular scenario. 




 Computational Analysis of Move Structures in Academic Abstracts  
 This paper introduces a method for computational analysis of move structures in abstracts of research articles . In our approach, sentences in a given abstract are analyzed and labeled with a specific move in light of various rhetorical functions . The method involves automatically gathering a large number of abstracts from the Web and building a language model of abstract moves . We also present a prototype concordancer , CARE , which exploits the move-tagged abstracts for digital learning . This system provides a promising approach to Web-based computer-assisted academic writing . 




 Re-Usable Tools for Precision Machine Translation  
 The LOGON MT demonstrator assembles independently valuable general-purpose NLP components into a machine translation pipeline that capitalizes on output quality . The demonstrator embodies an interesting combination of hand-built, symbolic resources and stochastic processes .  




 Testing The Psychological Reality of a Representational Model  
 A research program is described in which a particular representational format for meaning is tested as broadly as possible. In this format, developed by the LNR research group at The University of California at San Diego, verbs are represented as interconnected sets of subpredicates . These subpredicates may be thought of as the almost inevitable inferences that a listener makes when a verb is used in a sentence . They confer a meaning structure on the sentence in which the verb is used.  




 Fragments of a Theory of Human Plausible Reasoning  
 The paper outlines a computational theory of human plausible reasoning constructed from analysis of people&apos;s answers to everyday questions. Like logic , the theory is expressed in a content-independent formalism . Unlike logic , the theory specifies how different information in memory affects the certainty of the conclusions drawn. The theory consists of a dimensionalized space of different inference types and their certainty conditions , including a variety of meta-inference types where the inference depends on the person&apos;s knowledge about his own knowledge. The protocols from people&apos;s answers to questions are analyzed in terms of the different inference types . The paper also discusses how memory is structured in multiple ways to support the different inference types , and how the information found in memory determines which inference types are triggered.  




 PATH-BASED AND NODE-BASED INFERENCE IN SEMANTIC NETWORKS  
 Two styles of performing inference in semantic networks are presented and compared. Path-based inference allows an arc or a path of arcs between two given nodes to be inferred from the existence of another specified path between the same two nodes . Path-based inference rules may be written using a binary relational calculus notation . Node-based inference allows a structure of nodes to be inferred from the existence of an instance of a pattern of node structures . Node-based inference rules can be constructed in a semantic network using a variant of a predicate calculus notation . Path-based inference is more efficient, while node-based inference is more general. A method is described of combining the two styles in a single system in order to take advantage of the strengths of each. Applications of path-based inference rules to the representation of the extensional equivalence of intensional concepts , and to the explication of inheritance in hierarchies are sketched.  




 ON FROFF: A TEXT PROCESSING SYSTEM FOR ENGLISH TEXTS AND FIGURES  
 In order to meet the needs of a publication of papers in English, many systems to run off texts have been developed. In this paper, we report a system FROFF which can make a fair copy of not only texts but also graphs and tables indispensable to our papers. Its selection of fonts , specification of character size are dynamically changeable, and the typing location can be also changed in lateral or longitudinal directions. Each character has its own width and a line length is counted by the sum of each character . By using commands or rules which are defined to facilitate the construction of format expected or some mathematical expressions , elaborate and pretty documents can be successfully obtained.  




 ATNS USED AS A PROCEDURAL DIALOG MODEL  
 An attempt has been made to use an Augmented Transition Network as a procedural dialog model . The development of such a model appears to be important in several respects: as a device to represent and to use different dialog schemata proposed in empirical conversation analysis ; as a device to represent and to use models of verbal interaction ; as a device combining knowledge about dialog schemata and about verbal interaction with knowledge about task-oriented and goal-directed dialogs . A standard ATN should be further developed in order to account for the verbal interactions of task-oriented dialogs .  




 Metaphor - A Key to Extensible Semantic Analysis  
 Interpreting metaphors is an integral and inescapable process in human understanding of natural language . This paper discusses a method of analyzing metaphors based on the existence of a small number of generalized metaphor mappings . Each generalized metaphor contains a recognition network , a basic mapping , additional transfer mappings , and an implicit intention component . It is argued that the method reduces metaphor interpretation from a reconstruction to a recognition task . Implications towards automating certain aspects of language learning are also discussed. 




 Expanding the Horizons of Natural Language Interfaces  
 Current natural language interfaces have concentrated largely on determining the literal meaning of input from their users . While such decoding is an essential underpinning, much recent work suggests that natural language interfaces will never appear cooperative or graceful unless they also incorporate numerous non-literal aspects of communication , such as robust communication procedures . This paper defends that view, but claims that direct imitation of human performance is not the best way to implement many of these non-literal aspects of communication ; that the new technology of powerful personal computers with integral graphics displays offers techniques superior to those of humans for these aspects, while still satisfying human communication needs . The paper proposes interfaces based on a judicious mixture of these techniques and the still valuable methods of more traditional natural language interfaces.  




 Flexiable Parsing  
When people use natural language in natural settings, they often use it ungrammatically, missing out or repeating words, breaking-off and restarting, speaking in fragments, etc.. Their human listeners are usually able to cope with these deviations with little difficulty. If a computer system wishes to accept natural language input from its users on a routine basis, it must display a similar indifference. In this paper, we outline a set of parsing flexibilities that such a system should provide. We go, on to describe FlexP , a bottom-up pattern-matching parser that we have designed and implemented to provide these flexibilities for restricted natural language input to a limited-domain computer system.  




 AN IMPROVED LEFT-CORNER PARSING ALGORITHM  
 This paper proposes a series of modifications to the left corner parsing algorithm for context-free grammars . It is argued that the resulting algorithm is both efficient and flexible and is, therefore, a good choice for the parser used in a natural language interface . 




 An Efficient Easily Adaptable System for Interpreting Natural Language Queries  
 This paper gives an overall account of a prototype natural language question answering system , called Chat-80 . Chat-80 has been designed to be both efficient and easily adaptable to a variety of applications. The system is implemented entirely in Prolog , a programming language based on logic . With the aid of a logic-based grammar formalism called extraposition grammars , Chat-80 translates English questions into the Prolog subset of logic . The resulting logical expression is then transformed by a planning algorithm into efficient Prolog , cf. query optimisation in a relational database . Finally, the Prolog form is executed to yield the answer. 




 Scruffy Text Understanding: Design and Implementation of &apos;Tolerant&apos; Understanders  
 Most large text-understanding systems have been designed under the assumption that the input text will be in reasonably neat form, e.g., newspaper stories and other edited texts . However, a great deal of natural language texts e.g., memos , rough drafts , conversation transcripts etc., have features that differ significantly from neat texts , posing special problems for readers, such as misspelled words , missing words , poor syntactic construction , missing periods , etc. Our solution to these problems is to make use of expectations , based both on knowledge of surface English and on world knowledge of the situation being described. These syntactic and semantic expectations can be used to figure out unknown words from context , constrain the possible word-senses of words with multiple meanings ( ambiguity ), fill in missing words ( ellipsis ), and resolve referents ( anaphora ). This method of using expectations to aid the understanding of scruffy texts has been incorporated into a working computer program called NOMAD , which understands scruffy texts in the domain of Navy messages. 




 LIMITED DOMAIN SYSTEMS FOR LANGUAGE TEACHING  
 This abstract describes a natural language system which deals usefully with ungrammatical input and describes some actual and potential applications of it in computer aided second language learning . However, this is not the only area in which the principles of the system might be used, and the aim in building it was simply to demonstrate the workability of the general mechanism, and provide a framework for assessing developments of it.  




 A PROPER TREATMEMT OF SYNTAX AND SEMANTICS IN MACHINE TRANSLATION  
 A proper treatment of syntax and semantics in machine translation is introduced and discussed from the empirical viewpoint. For English-Japanese machine translation , the syntax directed approach is effective where the Heuristic Parsing Model (HPM) and the Syntactic Role System play important roles. For Japanese-English translation , the semantics directed approach is powerful where the Conceptual Dependency Diagram (CDD) and the Augmented Case Marker System (which is a kind of Semantic Role System ) play essential roles. Some examples of the difference between Japanese sentence structure and English sentence structure , which is vital to machine translation are also discussed together with various interesting ambiguities .  




 Entity-Oriented Parsing  
 An entity-oriented approach to restricted-domain parsing is proposed. In this approach, the definitions of the structure and surface representation of domain entities are grouped together. Like semantic grammar , this allows easy exploitation of limited domain semantics . In addition, it facilitates fragmentary recognition and the use of multiple parsing strategies , and so is particularly useful for robust recognition of extra-grammatical input . Several advantages from the point of view of language definition are also noted. Representative samples from an entity-oriented language definition are presented, along with a control structure for an entity-oriented parser , some parsing strategies that use the control structure , and worked examples of parses . A parser incorporating the control structure and the parsing strategies is currently under implementation . 




 A COMPUTATIONAL THEORY OF DISPOSITIONS  
 Informally, a disposition is a proposition which is preponderantly, but not necessarily always, true. For example, birds can fly is a disposition , as are the propositions Swedes are blond and Spaniards are dark. An idea which underlies the theory described in this paper is that a disposition may be viewed as a proposition with implicit fuzzy quantifiers which are approximations to all and always, e.g., almost all, almost always, most, frequently, etc. For example, birds can fly may be interpreted as the result of suppressing the fuzzy quantifier most in the proposition most birds can fly. Similarly, young men like young women may be read as most young men like mostly young women. The process of transforming a disposition into a proposition is referred to as explicitation or restoration . Explicitation sets the stage for representing the meaning of a proposition through the use of test-score semantics (Zadeh, 1978, 1982). In this approach to semantics , the meaning of a proposition , p, is represented as a procedure which tests, scores and aggregates the elastic constraints which are induced by p. The paper closes with a description of an approach to reasoning with dispositions which is based on the concept of a fuzzy syllogism . Syllogistic reasoning with dispositions has an important bearing on commonsense reasoning as well as on the management of uncertainty in expert systems . As a simple application of the techniques described in this paper, we formulate a definition of typicality -- a concept which plays an important role in human cognition and is of relevance to default reasoning . 




 Controlling Lexical Substitution in Computer Text Generation  
 This report describes Paul , a computer text generation system designed to create cohesive text through the use of lexical substitutions . Specifically, this system is designed to deterministically choose between pronominalization , superordinate substitution , and definite noun phrase reiteration . The system identifies a strength of antecedence recovery for each of the lexical substitutions , and matches them against the strength of potential antecedence of each element in the text to select the proper substitutions for these elements. 




 A LOGICAL FORMALISM FOR THE REPRESENTATION OF DETERMINERS  
 Determiners play an important role in conveying the meaning of an utterance , but they have often been disregarded, perhaps because it seemed more important to devise methods to grasp the global meaning  of a sentence , even if not in a precise way. Another problem with determiners is their inherent ambiguity . In this paper we propose a logical formalism , which, among other things, is suitable for representing determiners without forcing a particular interpretation when their meaning is still not clear. 




 SYNTHESIZING WEATHER FORECASTS FROM FORMATFED DATA  
 This paper describes a system ( RAREAS ) which synthesizes marine weather forecasts directly from formatted weather data . Such synthesis appears feasible in certain natural sublanguages with stereotyped text structure . RAREAS draws on several kinds of linguistic and non-linguistic knowledge and mirrors a forecaster&apos;s apparent tendency to ascribe less precise temporal adverbs to more remote meteorological events. The approach can easily be adapted to synthesize bilingual or multi-lingual texts . 




 THE CORRECTION OF ILL-FORMED INPUT USING HISTORY-BASED EXPECTATION WITH APPLICATIONS TO SPEECH UNDERSTANDING  
 A method for error correction of ill-formed input is described that acquires dialogue patterns in typical usage and uses these patterns to predict new inputs. Error correction is done by strongly biasing parsing toward expected meanings unless clear evidence from the input shows the current sentence is not expected. A dialogue acquisition and tracking algorithm is presented along with a description of its implementation in a voice interactive system . A series of tests are described that show the power of the error correction methodology when stereotypic dialogue occurs.  




 Attention, Intentions, And The Structure Of Discourse  
 In this paper we explore a new theory of discourse structure that stresses the role of purpose and processing in discourse . In this theory, discourse structure is composed of three separate but interrelated components: the structure of the sequence of utterances (called the linguistic structure ), a structure of purposes (called the intentional structure ), and the state of focus of attention (called the attentional state ). The linguistic structure consists of segments of the discourse into which the utterances naturally aggregate. The intentional structure captures the discourse-relevant purposes , expressed in each of the linguistic segments as well as relationships among them. The attentional state is an abstraction of the focus of attention of the participants as the discourse unfolds. The attentional state , being dynamic, records the objects, properties, and relations that are salient at each point of the discourse . The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases , referring expressions , and interruptions . The theory of attention, intention, and aggregation of utterances is illustrated in the paper with a number of example discourses . Various properties of discourse are described, and explanations for the behaviour of cue phrases , referring expressions , and interruptions are explored. This theory provides a framework for describing the processing of utterances in a discourse . Discourse processing requires recognizing how the utterances of the discourse aggregate into segments , recognizing the intentions expressed in the discourse and the relationships among intentions , and tracking the discourse through the operation of the mechanisms associated with attentional state . This processing description specifies in these recognition tasks the role of information from the discourse and from the participants &apos; knowledge of the domain. 




 REFERENCE IDENTIFICATION AND REFERENCE IDENTIFICATION FAILURES  
 The goal of this work is the enrichment of human-machine interactions in a natural language environment . Because a speaker and listener cannot be assured to have the same beliefs , contexts , perceptions , backgrounds , or goals , at each point in a conversation , difficulties and mistakes arise when a listener interprets a speaker&apos;s utterance . These mistakes can lead to various kinds of misunderstandings between speaker and listener , including reference failures or failure to understand the speaker&apos;s intention . We call these misunderstandings miscommunication . Such mistakes can slow, and possibly break down, communication . Our goal is to recognize and isolate such miscommunications and circumvent them. This paper highlights a particular class of miscommunication --- reference problems --- by describing a case study and techniques for avoiding failures of reference . We want to illustrate a framework less restrictive than earlier ones by allowing a speaker leeway in forming an utterance about a task and in determining the conversational vehicle to deliver it. The paper also promotes a new view for extensional reference . 




 The Relationship Between Tree Adjoining Grammars And Head Grammars  
 We examine the relationship between the two grammatical formalisms : Tree Adjoining Grammars and Head Grammars . We briefly investigate the weak equivalence of the two formalisms . We then turn to a discussion comparing the linguistic expressiveness of the two formalisms . 




 A LOGICAL SEMANTICS FOR FEATURE STRUCTURES  
 Unification-based grammar formalisms use structures containing sets of features to describe linguistic objects . Although computational algorithms for unification of feature structures have been worked out in experimental research, these algorithms become quite complicated, and a more precise description of feature structures is desirable. We have developed a model in which descriptions of feature structures can be regarded as logical formulas , and interpreted by sets of directed graphs which satisfy them. These graphs are, in fact, transition graphs for a special type of deterministic finite automaton . This semantics for feature structures extends the ideas of Pereira and Shieber [11], by providing an interpretation for values which are specified by disjunctions and path values embedded within disjunctions . Our interpretation differs from that of Pereira and Shieber by using a logical model in place of a denotational semantics . This logical model yields a calculus of equivalences , which can be used to simplify formulas . Unification is attractive, because of its generality, but it is often computationally inefficient. Our model allows a careful examination of the computational complexity of unification . We have shown that the consistency problem for formulas with disjunctive values is NP-complete . To deal with this complexity , we describe how disjunctive values can be specified in a way which delays expansion to disjunctive normal form . 




 The Multimedia Articulation of Answers in a Natural Language Database Query System  
 This paper describes a domain independent strategy for the multimedia articulation of answers elicited by a natural language interface to database query applications . Multimedia answers include videodisc images and heuristically-produced complete sentences in text or text-to-speech form . Deictic reference and feedback about the discourse are enabled. The interface thus presents the application as cooperative and conversational. 




 An Architecture for Anaphora Resolution  
 In this paper, we describe the pronominal anaphora resolution module of Lucy , a portable English understanding system . The design of this module was motivated by the observation that, although there exist many theories of anaphora resolution , no one of these theories is complete. Thus we have implemented a blackboard-like architecture in which individual partial theories can be encoded as separate modules that can interact to propose candidate antecedents and to evaluate each other&apos;s proposals. 




 Machine Translation Using Isomorphic UCGs  
 This paper discusses the application of Unification Categorial Grammar (UCG) to the framework of Isomorphic Grammars for Machine Translation pioneered by Landsbergen. The Isomorphic Grammars approach to MT involves developing the grammars of the Source and Target languages in parallel, in order to ensure that SL and TL expressions which stand in the translation relation have isomorphic derivations . The principle advantage of this approach is that knowledge concerning translation equivalence of expressions may be directly exploited, obviating the need for answers to semantic questions that we do not yet have. Semantic and other information may still be incorporated, but as constraints on the translation relation , not as levels of textual representation . After introducing this approach to MT system design, and the basics of monolingual UCG , we will show how the two can be integrated, and present an example from an implemented bi-directional English-Spanish fragment . Finally we will present some outstanding problems with the approach. 




 On the Generation and Interpretation of Demonstrative Expressions  
 This paper presents necessary and sufficient conditions for the use of demonstrative expressions in English and discusses implications for current discourse processing algorithms . We examine a broad range of texts to show how the distribution of demonstrative forms and functions is genre dependent . This research is part of a larger study of anaphoric expressions , the results of which will be incorporated into a natural language generation system . 




 Parsing with Category Coocurrence Restrictions  
 This paper summarizes the formalism of Category Cooccurrence Restrictions (CCRs) and describes two parsing algorithms that interpret it. CCRs are Boolean conditions on the cooccurrence of categories in local trees which allow the statement of generalizations which cannot be captured in other current syntax formalisms . The use of CCRs leads to syntactic descriptions formulated entirely with restrictive statements . The paper shows how conventional algorithms for the analysis of context free languages can be adapted to the CCR formalism . Special attention is given to the part of the parser that checks the fulfillment of logical well-formedness conditions on trees . 




 Solving Some Persistent Presupposition Problems  
 Soames 1979 provides some counterexamples to the theory of natural language presuppositions that is presented in Gazdar 1979. Soames 1982 provides a theory which explains these counterexamples. Mercer 1987 rejects the solution found in Soames 1982 leaving these counterexamples unexplained. By reappraising these insightful counterexamples, the inferential theory for natural language presuppositions described in Mercer 1987, 1988 gives a simple and straightforward explanation for the presuppositional nature of these sentences . 




 Directing the Generation of Living Space Descriptions  
 We have developed a computational model of the process of describing the layout of an apartment or house, a much-studied discourse task first characterized linguistically by Linde (1974). The model is embodied in a program, APT , that can reproduce segments of actual tape-recorded descriptions, using organizational and discourse strategies derived through analysis of our corpus . 




 Island Parsing and Bidirectional Charts  
 Chart parsing is directional in the sense that it works from the starting point (usually the beginning of the sentence) extending its activity usually in a rightward manner. We shall introduce the concept of a chart that works outward from islands and makes sense of as much of the sentence as it is actually possible, and after that will lead to predictions of missing fragments . So, for any place where the easily identifiable fragments occur in the sentence , the process will extend to both the left and the right of the islands , until possibly completely missing fragments are reached. At that point, by virtue of the fact that both a left and a right context were found, heuristics can be introduced that predict the nature of the missing fragments . 




 Interactive Translation : a new approach  
 A new approach for Interactive Machine Translation where the author interacts during the creation or the modification of the document is proposed. The explanation of an ambiguity or an error for the purposes of correction does not use any concepts of the underlying linguistic theory : it is a reformulation of the erroneous or ambiguous sentence . The interaction is limited to the analysis step of the translation process . This paper presents a new interactive disambiguation scheme based on the paraphrasing of a parser &apos;s multiple output. Some examples of paraphrasing ambiguous sentences are presented. 




 NETL: A System for Representing and Using Real-World Knowledge  
 Computer programs so far have not fared well in modeling language acquisition . For one thing, learning methodology applicable in general domains does not readily lend itself in the linguistic domain . For another, linguistic representation used by language processing systems is not geared to learning . We introduced a new linguistic representation , the Dynamic Hierarchical Phrasal Lexicon (DHPL) [Zernik88], to facilitate language acquisition . From this, a language learning model was implemented in the program RINA , which enhances its own lexical hierarchy by processing examples in context. We identified two tasks: First, how linguistic concepts are acquired from training examples and organized in a hierarchy ; this task was discussed in previous papers [Zernik87]. Second, we show in this paper how a lexical hierarchy is used in predicting new linguistic concepts . Thus, a program does not stall even in the presence of a lexical unknown , and a hypothesis can be produced for covering that lexical gap . 




 COMPLEX: A Computational Lexicon for Natural Language Systems  
 Although every natural language system needs a computational lexicon , each system puts different amounts and types of information into its lexicon according to its individual needs. However, some of the information needed across systems is shared or identical information. This paper presents our experience in planning and building COMPLEX , a computational lexicon designed to be a repository of shared lexical information for use by Natural Language Processing (NLP) systems . We have drawn primarily on explicit and implicit information from machine-readable dictionaries (MRD&apos;s) to create a broad coverage lexicon . 




 MODELING THE USER IN NATURAL LANGUAGE SYSTEMS  
 For intelligent interactive systems to communicate with humans in a natural manner, they must have knowledge about the system users . This paper explores the role of user modeling in such systems . It begins with a characterization of what a user model is and how it can be used. The types of information that a user model may be required to keep about a user are then identified and discussed. User models themselves can vary greatly depending on the requirements of the situation and the implementation, so several dimensions along which they can be classified are presented. Since acquiring the knowledge for a user model is a fundamental problem in user modeling , a section is devoted to this topic. Next, the benefits and costs of implementing a user modeling component for a system are weighed in light of several aspects of the interaction requirements that may be imposed by the system. Finally, the current state of research in user modeling is summarized, and future research topics that must be addressed in order to achieve powerful, general user modeling systems are assessed. 




 Generation for Dialogue Translation Using Typed Feature Structure Unification  
 This article introduces a bidirectional grammar generation system called feature structure-directed generation , developed for a dialogue translation system . The system utilizes typed feature structures to control the top-down derivation in a declarative way. This generation system also uses disjunctive feature structures to reduce the number of copies of the derivation tree . The grammar for this generator is designed to properly generate the speaker&apos;s intention in a telephone dialogue .  




 Sentence disambiguation by document preference sets oriented  
 This paper proposes document oriented preference sets(DoPS) for the disambiguation of the dependency structure of sentences . The DoPS system extracts preference knowledge from a target document or other documents automatically. Sentence ambiguities can be resolved by using domain targeted preference knowledge without using complicated large knowledgebases . Implementation and empirical results are described for the the analysis of dependency structures of Japanese patent claim sentences .  




 A phonological knowledge base system using unification-based formalism: a case study of Korean phonology  
 This paper describes the framework of a Korean phonological knowledge base system using the unification-based grammar formalism : Korean Phonology Structure Grammar (KPSG) . The approach of KPSG provides an explicit development model for constructing a computational phonological system : speech recognition and synthesis system . We show that the proposed approach is more describable than other approaches such as those employing a traditional generative phonological approach .  




 Synchronous Tree-Adjoining Grammars  
 The unique properties of tree-adjoining grammars (TAG) present a challenge for the application of TAGs beyond the limited confines of syntax , for instance, to the task of semantic interpretation or automatic translation of natural language . We present a variant of TAGs , called synchronous TAGs , which characterize correspondences between languages . The formalism&apos;s intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language , or to their translates in another natural language ; in summary, we intend it to allow TAGs to be used beyond their role in syntax proper . We discuss the application of synchronous TAGs to concrete examples, mentioning primarily in passing some computational issues that arise in its interpretation.  




 Japanese Sentence Analysis as Argumentation  
 This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige, which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .  




 Spelling-checking for Highly Inflective Languages  
 Spelling-checkers have become an integral part of most text processing software . From different reasons among which the speed of processing prevails they are usually based on dictionaries of word forms instead of words . This approach is sufficient for languages with little inflection such as English , but fails for highly inflective languages such as Czech , Russian , Slovak or other Slavonic languages . We have developed a special method for describing inflection for the purpose of building spelling-checkers for such languages. The speed of the resulting program lies somewhere in the middle of the scale of existing spelling-checkers for English and the main dictionary fits into the standard 360K floppy , whereas the number of recognized word forms exceeds 6 million (for Czech ). Further, a special method has been developed for easy word classification .  




 Toward a Real-Time Spoken Language System Using Commercial Hardware  
 We describe the methods and hardware that we are using to produce a real-time demonstration of an integrated Spoken Language System . We describe algorithms that greatly reduce the computation needed to compute the N-Best sentence hypotheses . To avoid grammar coverage problems we use a fully-connected first-order statistical class grammar . The speech-search algorithm is implemented on a board with a single Intel i860 chip , which provides a factor of 5 speed-up over a SUN 4 for straight C code . The board plugs directly into the VME bus of the SUN4 , which controls the system and contains the natural language system and application back end .  




 A New Paradigm for Speaker-Independent Training and Speaker Adaptation  
 This paper reports on two contributions to large vocabulary continuous speech recognition . First, we present a new paradigm for speaker-independent (SI) training of hidden Markov models (HMM) , which uses a large amount of speech from a few speakers instead of the traditional practice of using a little speech from many speakers . In addition, combination of the training speakers is done by averaging the statistics of independently trained models rather than the usual pooling of all the speech data from many speakers prior to training . With only 12 training speakers for SI recognition , we achieved a 7.5% word error rate on a standard grammar and test set from the DARPA Resource Management corpus . This performance is comparable to our best condition for this test suite, using 109 training speakers . Second, we show a significant improvement for speaker adaptation (SA) using the new SI corpus and a small amount of speech from the new (target) speaker . A probabilistic spectral mapping is estimated independently for each training (reference) speaker and the target speaker . Each reference model is transformed to the space of the target speaker and combined by averaging . Using only 40 utterances from the target speaker for adaptation , the error rate dropped to 4.1% --- a 45% reduction in error compared to the SI result.  




 AN EDITOR FOR THE EXPLANATORY AND COMBINATORY DICTIONARY OF CONTEMPORARY FRENCH (DECFC)  
 This paper presents a specialized editor for a highly structured dictionary . The basic goal in building that editor was to provide an adequate tool to help lexicologists produce a valid and coherent dictionary on the basis of a linguistic theory . If we want valuable lexicons and grammars to achieve complex natural language processing , we must provide very powerful tools to help create and ensure the validity of such complex linguistic databases . Our most important task in building the editor was to define a set of coherence rules that could be computationally applied to ensure the validity of lexical entries . A customized interface for browsing and editing was also designed and implemented. 




 Free Indexation: Combinatorial Analysis and A Compositional Algorithm  
 The principle known as free indexation plays an important role in the determination of the referential properties of noun phrases in the principle-and-parameters language framework . First, by investigating the combinatorics of free indexation , we show that the problem of enumerating all possible indexings requires exponential time . Secondly, we exhibit a provably optimal free indexation algorithm .  




 Robust Processing of Real-World Natural-Language Texts  
 It is often assumed that when natural language processing meets the real world, the ideal of aiming for complete and correct interpretations has to be abandoned. However, our experience with TACITUS ; especially in the MUC-3 evaluation , has shown that principled techniques for syntactic and pragmatic analysis can be bolstered with methods for achieving robustness. We describe three techniques for making syntactic analysis more robust---an agenda-based scheduling parser , a recovery technique for failed parses , and a new technique called terminal substring parsing . For pragmatics processing , we describe how the method of abductive inference is inherently robust, in that an interpretation is always possible, so that in the absence of the required world knowledge , performance degrades gracefully. Each of these techniques have been evaluated and the results of the evaluations are presented.  




 An Efficient Chart-based Algorithm for Partial-Parsing of Unrestricted Texts  
 We present an efficient algorithm for chart-based phrase structure parsing of natural language that is tailored to the problem of extracting specific information from unrestricted texts where many of the words are unknown and much of the text is irrelevant to the task. The parser gains algorithmic efficiency through a reduction of its search space . As each new edge is added to the chart , the algorithm checks only the topmost of the edges adjacent to it, rather than all such edges as in conventional treatments. The resulting spanning edges are insured to be the correct ones by carefully controlling the order in which edges are introduced so that every final constituent covers the longest possible span . This is facilitated through the use of phrase boundary heuristics based on the placement of function words , and by heuristic rules that permit certain kinds of phrases to be deduced despite the presence of unknown words . A further reduction in the search space is achieved by using semantic rather than syntactic categories on the terminal and non-terminal edges , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced.  




 Temporal Structure of Discourse  
 In this paper discourse segments are defined and a method for discourse segmentation primarily based on abduction of temporal relations between segments is proposed. This method is precise and computationally feasible and is supported by previous work in the area of temporal anaphora resolution .  




 Syntactic Ambiguity Resolution Using A Discrimination and Robustness Oriented Adaptive Learning Algorithm  
 In this paper, a discrimination and robustness oriented adaptive learning procedure is proposed to deal with the task of syntactic ambiguity resolution . Owing to the problem of insufficient training data and approximation error introduced by the language model , traditional statistical approaches , which resolve ambiguities by indirectly and implicitly using maximum likelihood method , fail to achieve high performance in real applications. The proposed method remedies these problems by adjusting the parameters to maximize the accuracy rate directly. To make the proposed algorithm robust, the possible variations between the training corpus and the real tasks are also taken into consideration by enlarging the separation margin between the correct candidate and its competing members. Significant improvement has been observed in the test. The accuracy rate of syntactic disambiguation is raised from 46.0% to 60.62% by using this novel approach.  




 Quasi-Destructive Graph Unification with Structure-Sharing  
 Graph unification remains the most expensive part of unification-based grammar parsing . We focus on one speed-up element in the design of unification algorithms : avoidance of copying of unmodified subgraphs . We propose a method of attaining such a design through a method of structure-sharing which avoids log(d) overheads often associated with structure-sharing of graphs without any use of costly dependency pointers . The proposed scheme eliminates redundant copying while maintaining the quasi-destructive scheme&apos;s ability to avoid over copying and early copying combined with its ability to handle cyclic structures without algorithmic additions.  




 A Similarity-Driven Transfer System  
 The transfer phase in machine translation (MT) systems has been considered to be more complicated than analysis and generation , since it is inherently a conglomeration of individual lexical rules . Currently some attempts are being made to use case-based reasoning in machine translation , that is, to make decisions on the basis of translation examples at appropriate pints in MT . This paper proposes a new type of transfer system , called a Similarity-driven Transfer System (SimTran) , for use in such case-based MT (CBMT) . 




 Interactive Speech Understanding  
 This paper introduces a robust interactive method for speech understanding . The generalized LR parsing is enhanced in this approach. Parsing proceeds from left to right correcting minor errors. When a very noisy portion is detected, the parser skips that portion using a fake non-terminal symbol . The unidentified portion is resolved by re-utterance of that portion which is parsed very efficiently by using the parse record of the first utterance . The user does not have to speak the whole sentence again. This method is also capable of handling unknown words , which is important in practical systems. Detected unknown words can be incrementally incorporated into the dictionary after the interaction with the user . A pilot system has shown great effectiveness of this approach. 




 Recognizing Unregistered Names for Mandarin Word Identification  
 Word Identification has been an important and active issue in Chinese Natural Language Processing . In this paper, a new mechanism, based on the concept of sublanguage , is proposed for identifying unknown words , especially personal names , in Chinese newspapers . The proposed mechanism includes title-driven name recognition , adaptive dynamic word formation , identification of 2-character and 3-character Chinese names without title . We will show the experimental results for two corpora and compare them with the results by the NTHU&apos;s statistic-based system , the only system that we know has attacked the same problem. The experimental results have shown significant improvements over the WI systems without the name identification capability. 




 Reconstructing Spatial Image from Natural Language Texts  
 This paper describes the understanding process of the spatial descriptions in Japanese . In order to understand the described world , the authors try to reconstruct the geometric model of the global scene from the scenic descriptions drawing a space. It is done by an experimental computer program SPRINT , which takes natural language texts and produces a model of the described world . To reconstruct the model , the authors extract the qualitative spatial constraints from the text , and represent them as the numerical constraints on the spatial attributes of the entities . This makes it possible to express the vagueness of the spatial concepts and to derive the maximally plausible interpretation from a chunk of information accumulated as the constraints. The interpretation reflects the temporary belief about the world .  




 Multi-Site Data Collection for a Spoken Language Corpus: MADCOW  
 This paper describes a recently collected spoken language corpus for the ATIS (Air Travel Information System) domain . This data collection effort has been co-ordinated by MADCOW (Multi-site ATIS Data COllection Working group) . We summarize the motivation for this effort, the goals, the implementation of a multi-site data collection paradigm , and the accomplishments of MADCOW in monitoring the collection and distribution of 12,000 utterances of spontaneous speech from five sites for use in a multi-site common evaluation of speech, natural language and spoken language .




 Spoken Language Processing in the Framework of Human-Machine Communication at LIMSI  
 The paper provides an overview of the research conducted at LIMSI in the field of speech processing , but also in the related areas of Human-Machine Communication , including Natural Language Processing , Non Verbal and Multimodal Communication . Also presented are the commercial applications of some of the research projects. When applicable, the discussion is placed in the framework of international collaborations. 




 The MIT ATIS System: February 1992 Progress Report  
 This paper describes the status of the MIT ATIS system as of February 1992, focusing especially on the changes made to the SUMMIT recognizer . These include context-dependent phonetic modelling , the use of a bigram language model in conjunction with a probabilistic LR parser , and refinements made to the lexicon . Together with the use of a larger training set , these modifications combined to reduce the speech recognition word and sentence error rates by a factor of 2.5 and 1.6, respectively, on the October &apos;91 test set . The weighted error for the entire spoken language system on the same test set is 49.3%. Similar results were also obtained on the February &apos;92 benchmark evaluation . 




 Recent Improvements and Benchmark Results for Paramax ATIS System  
 This paper describes three relatively domain-independent capabilities recently added to the Paramax spoken language understanding system : non-monotonic reasoning , implicit reference resolution , and database query paraphrase . In addition, we discuss the results of the February 1992 ATIS benchmark tests . We describe a variation on the standard evaluation metric which provides a more tightly controlled measure of progress. Finally, we briefly describe an experiment which we have done in extending the n-best speech/language integration architecture to improving OCR accuracy .  




 Towards History-based Grammars: Using Richer Models for Probabilistic Parsing  
 We describe a generative probabilistic model of natural language , which we call HBG , that takes advantage of detailed linguistic information to resolve ambiguity .  HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences , called a Treebank , in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence . This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse . In head-to-head tests against one of the best existing robust probabilistic parsing models , which we call P-CFG, the HBG model significantly outperforms P-CFG , increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.  




 MAP Estimation of Continuous Density HMM: Theory and Applications  
 We discuss maximum a posteriori estimation of continuous density hidden Markov models (CDHMM) . The classical MLE reestimation algorithms , namely the forward-backward algorithm and the segmental k-means algorithm , are expanded and reestimation formulas are given for HMM with Gaussian mixture observation densities . Because of its adaptive nature, Bayesian learning serves as a unified approach for the following four speech recognition applications, namely parameter smoothing , speaker adaptation , speaker group modeling and corrective training . New experimental results on all four applications are provided to show the effectiveness of the MAP estimation approach . 




 One Sense Per Discourse  
 It is well-known that there are polysemous words like sentence whose meaning or sense depends on the context of use. We have recently reported on two new word-sense disambiguation systems , one trained on bilingual material (the Canadian Hansards ) and the other trained on monolingual material ( Roget&apos;s Thesaurus and Grolier&apos;s Encyclopedia ). As this work was nearing completion, we observed a very strong discourse effect. That is, if a polysemous word such as sentence appears two or more times in a well-written discourse , it is extremely likely that they will all share the same sense . This paper describes an experiment which confirmed this hypothesis and found that the tendency to share sense in the same discourse is extremely strong (98%). This result can be used as an additional source of constraint for improving the performance of the word-sense disambiguation algorithm . In addition, it could also be used to help evaluate disambiguation algorithms that did not make use of the discourse constraint .  





Exploring Correlation Of Dependency Relation Paths For Answer Extraction 

In this paper, we explore correlation of dependency relation paths to rank candidate answers in answer extraction. Using the correlation measure, we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question. Different from previous studies, we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure. The correlations are further incorporated into a Maximum Entropy-based ranking model which estimates path weights from training. Experimental results show that our method significantly outperforms state-of-the-art syntactic relation-based methods by up to 20% in MRR.







 Automatic Processing Of Large Corpora For The Resolution Of Anaphora References 

Manual acquisition of semantic constraints in broad domains is very expensive. This paper presents an automatic scheme for collecting statistics on cooccurrence patterns in a large corpus. To a large extent, these statistics reflect semantic constraints and thus are used to disambiguate anaphora references and syntactic ambiguities. The scheme was implemented by gathering statistics on the output of other linguistic tools. An experiment was performed to resolve references of the pronoun "it" in sentences that were randomly selected from the corpus. The results of the experiment show that in most of the cases the cooccurrence statistics indeed reflect the semantic constraints and thus provide a basis for a useful disambiguation tool.







 Kullback-Leibler Distance Between Probabilistic Context-Free Grammars And Probabilistic Finite Automata 

We consider the problem of computing the Kullback-Leibler distance, also called the relative entropy, between a probabilistic context-free grammar and a probabilistic finite automaton. We show that there is a closed-form (analytical) solution for one part of the Kullback-Leibler distance, viz. the cross-entropy. We discuss several applications of the result to the problem of distributional approximation of probabilistic context-free grammars by means of probabilistic finite automata.







 Spelling Correction In Agglutinative Languages 

Methods developed for spelling correction for languages like English (see the review by Kukich (Kukich, 1992)) are not readily applicable to agglutinative languages. This poster presents an approach to spelling correction in agglutinative languages that is based on two-level morphology and a dynamic-programming based search algorithm. After an overview of our approach, we present results from experiments with spelling correction in Turkish.







Robust Continuous Speech Recognition Technology Program Summary

The major objective of this program is to develop and demonstrate robust, high performance continuous speech recognition (CSR) techniques focussed on application in Spoken Language Systems (SLS) which will enhance the effectiveness of military and civilian computer-based systems. A key complementary objective is to define and develop applications of robust speech recognition and understanding systems, and to help catalyze the transition of spoken language technology into military and civilian systems, with particular focus on application of robust CSR to mobile military command and control. The research effort focusses on developing advanced acoustic modelling, rapid search, and recognition-time adaptation techniques for robust large-vocabulary CSR, and on applying these techniques to the new ARPA large-vocabulary CSR corpora and to military application tasks.







A Framework for Customizable Generation of Hypertext Presentations 

In this paper, we present a framework, Presentor, for the development and customization of hypertext presentation generators. Presentor offers intuitive and powerful declarative languages specifying the presentation at different levels: macro-planning, micro-planning, realization, and formatting. Presentor is implemented and is portable cross-platform and cross-domain. It has been used with success in several application domains including weather forecasting, object modeling, system description and requirements summarization.







A Practical Methodology For The Evaluation Of Spoken Language Systems 

A meaningful evaluation methodology can advance the state-of-the-art by encouraging mature, practical applications rather than "toy" implementations. Evaluation is also crucial to assessing competing claims and identifying promising technical approaches. While work in speech recognition (SR) has a history of evaluation methodologies that permit comparison among various systems, until recently no methodology existed for either developers of natural language (NL) interfaces or researchers in speech understanding (SU) to evaluate and compare the systems they developed. Recently considerable progress has been made by a number of groups involved in the DARPA Spoken Language Systems (SLS) program to agree on a methodology for comparative evaluation of SLS systems, and that methodology has been put into practice several times in comparative tests of several SLS systems. These evaluations are probably the only NL evaluations other than the series of Message Understanding Conferences (Sundheim, 1989; Sundheim, 1991) to have been developed and used by a group of researchers at different sites, although several excellent workshops have been held to study some of these problems (Palmer et al., 1989; Neal et al., 1991). This paper describes a practical "black-box" methodology for automatic evaluation of question-answering NL systems. While each new application domain will require some development of special resources, the heart of the methodology is domain-independent, and it can be used with either speech or text input. The particular characteristics of the approach are described in the following section: subsequent sections present its implementation in the DARPA SLS community, and some problems and directions for future development.







Integrating Syntactic Priming Into An Incremental Probabilistic Parser, With An Application To Psycholinguistic Modeling 

The psycholinguistic literature provides evidence for syntactic priming, i.e., the tendency to repeat structures. This paper describes a method for incorporating priming into an incremental probabilistic parser. Three models are compared, which involve priming of rules between sentences, within sentences, and within coordinate structures. These models simulate the reading time advantage for parallel structures found in human data, and also yield a small increase in overall parsing accuracy.








Estimating Class Priors In Domain Adaptation For Word Sense Disambiguation 

Instances of a word drawn from different domains may have different sense priors (the proportions of the different senses of a word). This in turn affects the accuracy of word sense disambiguation (WSD) systems trained and applied on different domains. This paper presents a method to estimate the sense priors of words drawn from a new domain, and highlights the importance of using well calibrated probabilities when performing these estimations. By using well calibrated probabilities, we are able to estimate the sense priors effectively to achieve significant improvements in WSD accuracy.







An Attempt To Automatic Thesaurus Construction From An Ordinary Japanese Language Dictionary 

How to obtain hierarchical relations(e.g. superordinate -hyponym relation, synonym relation) is one of the most important problems for thesaurus construction. A pilot system for extracting these relations automatically from an ordinary Japanese language dictionary (Shinmeikai Kokugojiten, published by Sansei-do, in machine readable form) is given. The features of the definition sentences in the dictionary, the mechanical extraction of the hierarchical relations and the estimation of the results are discussed.







The Transfer Phase Of Mu Machine Translation System

The interlingual approach to MT has been repeatedly advocated by researchers originally interested in natural language understanding who take machine translation to be one possible application. However, not only the ambiguity but also the vagueness which every natural language inevitably has leads this approach into essential difficulties. In contrast, our project, the Mu-project, adopts the transfer approach as the basic framework of MT. This paper describes the detailed construction of the transfer phase of our system from Japanese to English, and gives some examples of problems which seem difficult to treat in the interlingual approach. The basic design principles of the transfer phase of our system have already been mentioned in (1) (2). Some of the principles which are relevant to the topic of this paper are: (a) Multiple Layer of Grammars (b) Multiple Layer Presentation (c) Lexicon Driven Processing (d) Form-Oriented Dictionary Description. This paper also shows how these principles are realized in the current system.








Why Nitpicking Works: Evidence For Occam's Razor In Error Correctors 


Empirical experience and observations have shown us when powerful and highly tunable classifiers such as maximum entropy classifiers, boosting and SVMs are applied to language processing tasks, it is possible to achieve high accuracies, but eventually their performances all tend to plateau out at around the same point. To further improve performance, various error correction mechanisms have been developed, but in practice, most of them cannot be relied on to predictably improve performance on unseen data; indeed, depending upon the test set, they are as likely to degrade accuracy as to improve it. This problem is especially severe if the base classifier has already been finely tuned. In recent work, we introduced N-fold Templated Piped Correction, or NTPC ("nitpick"), an intriguing error corrector that is designed to work in these extreme operating conditions. Despite its simplicity, it consistently and robustly improves the accuracy of existing highly accurate base models. This paper investigates some of the more surprising claims made by NTPC, and presents experiments supporting an Occam's Razor argument that more complex models are damaging or unnecessary in practice.







 Acquisition Of Verb Entailment From Text 

The study addresses the problem of automatic acquisition of entailment relations between verbs. While this task has much in common with paraphrases acquisition which aims to discover semantic equivalence between verbs, the main challenge of entailment acquisition is to capture asymmetric, or directional, relations. Motivated by the intuition that it often underlies the local structure of coherent text, we develop a method that discovers verb entailment using evidence about discourse relations between clauses available in a parsed corpus. In comparison with earlier work, the proposed method covers a much wider range of verb entailment types and learns the mapping between verbs with highly varied argument structures.







 Forest-Based Statistical Sentence Generation 

This paper presents a new approach to statistical sentence generation in which alternative phrases are represented as packed sets of trees, or forests, and then ranked statistically to choose the best one. This representation offers advantages in compactness and in the ability to represent syntactic information. It also facilitates more efficient statistical ranking than a previous approach to statistical generation. An efficient ranking algorithm is described, together with experimental results showing significant improvements over simple enumeration or a lattice-based approach.







An NTU-Approach To Automatic Sentence Extraction For Summary Generation 


Automatic summarization and information extraction are two important Internet services. MUC and SUMMAC play their appropriate roles in the next generation Internet. This paper focuses on the automatic summarization and proposes two different models to extract sentences for summary generation under two tasks initiated by SUMMAC-1. For categorization task, positive feature vectors and negative feature vectors are used cooperatively to construct generic, indicative summaries. For adhoc task, a text model based on relationship between nouns and verbs is used to filter out irrelevant discourse segment, to rank relevant sentences, and to generate the user-directed summaries. The result shows that the NormF of the best summary and that of the fixed summary for adhoc tasks are 0.456 and 0. 447. The NormF of the best summary and that of the fixed summary for categorization task are 0.4090 and 0.4023. Our system outperforms the average system in categorization task but does a common job in adhoc task.








A Method for Relating Multiple Newspaper Articles by Using Graphs, and Its Application to Webcasting 


This paper describes methods for relating (threading) multiple newspaper articles, and for visualizing various characteristics of them by using a directed graph. A set of articles is represented by a set of word vectors, and the similarity between the vectors is then calculated. The graph is constructed from the similarity matrix. By applying some constraints on the chronological ordering of articles, an efficient threading algorithm that runs in 0(n) time (where n is the number of articles) is obtained. The constructed graph is visualized with words that represent the topics of the threads, and words that represent new information in each article. The threading technique is suitable for Webcasting (push) applications. A threading server determines relationships among articles from various news sources, and creates files containing their threading information. This information is represented in eXtended Markup Language (XML), and can be visualized on most Web browsers. The XML-based representation and a current prototype are described in this paper.








A Flexible Example-Based Parser Based on the SSTC



In this paper we sketch an approach for Natural Language parsing. Our approach is an example-based approach, which relies mainly on examples that already parsed to their representation structure, and on the knowledge that we can get from these examples the required information to parse a new input sentence. In our approach, examples are annotated with the Structured String Tree Correspondence (SSTC) annotation schema where each SSTC describes a sentence, a representation tree as well as the correspondence between substrings in the sentence and subtrees in the representation tree. In the process of parsing, we first try to build subtrees for phrases in the input sentence which have been successfully found in the example-base - a bottom up approach. These subtrees will then be combined together to form a single rooted representation tree based on an example with similar representation structure - a top down approach.Keywords:








Named Entity Recognition Using An HMM-Based Chunk Tagger


This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities. Through the HMM, our system is able to apply and integrate four types of internal and external evidences : 1) simple deterministic internal feature of the words, such as capitalization and digitalization ; 2) internal semantic feature of important triggers ; 3) internal gazetteer feature; 4) external macro context feature. In this way, the NER problem can be resolved effectively. Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively. It shows that the performance is significantly better than reported by any other machine-learning system. Moreover, the performance is even consistently better than those based on handcrafted rules.







Using A Hybrid System Of Corpus- And Knowledge-Based Techniques To Automate The Induction Of A Lexical Sublanguage Grammar 

Porting a Natural Language Processing (NLP) system to a new domain remains one of the bottlenecks in syntactic parsing, because of the amount of effort required to fix gaps in the lexicon, and to attune the existing grammar to the idiosyncracies of the new sublanguage. This paper shows how the process of fitting a lexicalized grammar to a domain can be automated to a great extent by using a hybrid system that combines traditional knowledge-based techniques with a corpus-based approach.







Detecting Transliterated Orthographic Variants Via Two Similarity Metrics 

We propose a detection method for orthographic variants caused by transliteration in a large corpus. The method employs two similarities. One is string similarity based on edit distance. The other is contextual similarity by a vector space model. Experimental results show that the method performed a 0.889 F-measure in an open test.







Understanding Temporal Expressions In Emails 

Recent years have seen increasing research on extracting and using temporal information in natural language applications. However most of the works found in the literature have focused on identifying and understanding temporal expressions in newswire texts. In this paper we report our work on anchoring temporal expressions in a novel genre, emails. The highly under-specified nature of these expressions fits well with our constraint-based representation of time, Time Calculus for Natural Language (TCNL). We have developed and evaluated a Temporal Expression Anchoror (TEA), and the result shows that it performs significantly better than the baseline, and compares favorably with some of the closely related work.






Research And Development For Spoken Language Systems 

The goal of this research is to develop a spoken language system that will demonstrate the usefulness of voice input for interactive problem solving. The system will accept continuous speech, and will handle multiple speakers without explicit speaker enrollment. Combining speech recognition and natural language processing to achieve speech understanding, the system will be demonstrated in an application domain relevant to the DoD. The objective of this project is to develop a robust and high-performance speech recognition system using a segment-based approach to phonetic recognition. The recognition system will eventually be integrated with natural language processing to achieve spoken language understanding.







 Computer Generation Of Multiparagraph English Text 

This paper reports recent research into methods for creating natural language text. A new processing paradigm called Fragment-and-Compose has been created and an experimental system implemented in it. The knowledge to be expressed in text is first divided into small propositional units, which are then composed into appropriate combinations and converted into text.KDS (Knowledge Delivery System), which embodies this paradigm, has distinct parts devoted to creation of the propositional units, to organization of the text, to prevention of excess redundancy, to creation of combinations of units, to evaluation of these combinations as potential sentences, to selection of the best among competing combinations, and to creation of the final text. The Fragment-and-Compose paradigm and the computational methods of KDS are described.







 Design Of A Hybrid Deterministic Parser 

A deterministic parser is under development which represents a departure from traditional deterministic parsers in that it combines both symbolic and connectionist components. The connectionist component is trained either from patterns derived from the rules of a deterministic grammar. The development and evolution of such a hybrid architecture has lead to a parser which is superior to any known deterministic parser. Experiments are described and powerful training techniques are demonstrated that permit decision-making by the connectionist component in the parsing process. This approach has permitted some simplifications to the rules of other deterministic parsers, including the elimination of rule packets and priorities. Furthermore, parsing is performed more robustly and with more tolerance for error. Data are presented which show how a connectionist (neural) network trained with linguistic rules can parse both expected (grammatical) sentences as well as some novel (ungrammatical or lexically ambiguous) sentences.







 Supervised Ranking In Open-Domain Text Summarization 

The paper proposes and empirically motivates an integration of supervised learning with unsupervised learning to deal with human biases in summarization. In particular, we explore the use of probabilistic decision tree within the clustering framework to account for the variation as well as regularity in human created summaries. The corpus of human created extracts is created from a newspaper corpus and used as a test set. We build probabilistic decision trees of different flavors and integrate each of them with the clustering framework. Experiments with the corpus demonstrate that the mixture of the two paradigms generally gives a significant boost in performance compared to cases where either ofthe two is considered alone.







 Sequential Conditional Generalized Iterative Scaling 

We describe a speedup for training conditional maximum entropy models. The algorithm is a simple variation on Generalized Iterative Scaling, but converges roughly an order of magnitude faster, depending on the number of constraints, and the way speed is measured. Rather than attempting to train all model parameters simultaneously, the algorithm trains them sequentially. The algorithm is easy to implement, typically uses only slightly more memory, and will lead to improvements for most maximum entropy problems.







 Word Re-Ordering And DP-Based Search In Statistical Machine Translation 

In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP). Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an efficient search algorithm. A search restriction especially useful for the translation direction from German to English is presented. The experimental tests are carried out on the Verbmobil task (German-English, 8000-word vocabulary), which is a limited-domain spoken-language task.







Role Of Word Sense Disambiguation In Lexical Acquisition : Predicting Semantics From Syntactic Cues

This paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the construction of large-scale knowledge sources. We describe two experiments: one which ignored word-sense distinctions, resulting in 6.3% accuracy for semantic classification of verbs based on (Levin, 1993); and one which exploited word-sense distinctions, resulting in 97.9% accuracy. These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic behavior are predictably related; (2) to demonstrate that a 15-fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses. Finally, we show that we can provide effective acquisition techniques for novel word senses using a combination of online sources.







PRC Inc: Description Of The PAKTUS System Used For MUC-3 

The PRC Adaptive Knowledge-based Text Understanding System (PAKTUS) has been under development as an Independent Research and Development project at PRC since 1984. The objective is a generic system of tools, including a core English lexicon, grammar, and concept representations, for building natural language processing (NLP) systems for text understanding. Systems built with PAKTUS are intended to generate input to knowledge based systems ordata base systems. Input to the NLP system is typically derived from an existing electronic message stream, such as a news wire. PAKTUS supports the adaptation of the generic core to a variety of domains: JINTACCS messages, RAINFORM messages, news reports about a specific type of event, such as financial transfers or terrorist acts, etc., by acquiring sublanguage and domain-specific grammar, words, conceptual mappings, and discourse patterns. The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success.







Memorisation for Glue Language Deduction and Categorial Parsing 

The multiplicative fragment of linear logic has found a number of applications in computational linguistics: in the "glue language" approach to LFG semantics, and in the formulation and parsing of various categorial grammars. These applications call for efficient deduction methods. Although a number of deduction methods for multiplicative linear logic are known, none of them are tabular methods, which bring a substantial efficiency gain by avoiding redundant computation (cf. chart methods in CFG parsing): this paper presents such a method, and discusses its use in relation to the above applications.







Data-Driven Generation Of Emphatic Facial Displays

We describe an implementation of data-driven selection of emphatic facial displays for an embodied conversational agent in a dialogue system. A corpus of sentences in the domain of the target dialogue system was recorded, and the facial displays used by the speaker were annotated. The data from those recordings was used in a range of models for generating facial displays, each model making use of a different amount of context or choosing displays differently within a context. The models were evaluated in two ways: by cross-validation against the corpus, and by asking users to rate the output. The predictions of the cross-validation study differed from the actual user ratings. While the cross-validation gave the highest scores to models making a majority choice within a context, the user study showed a significant preference for models that produced more variation. This preference was especially strong among the female subjects.







 Head-Driven Parsing For Word Lattices 

We present the first application of the head-driven statistical parsing model of Collins (1999) as a simultaneous language model and parser for large-vocabulary speech recognition. The model is adapted to an online left to right chart-parser for word lattices, integrating acoustic, n-gram, and parser probabilities. The parser uses structural and lexical dependencies not considered by n-gram models, conditioning recognition on more linguistically-grounded relationships. Experiments on the Wall Street Journal treebank and lattice corpora show word error rates competitive with the standard n-gram language model while extracting additional structural information useful for speech understanding.







Improving Language Model Size Reduction Using Better Pruning Criteria 


Reducing language model (LM) size is a critical issue when applying a LM to realistic applications which have memory constraints. In this paper, three measures are studied for the purpose of LM pruning. They are probability, rank, and entropy. We evaluated the performance of the three pruning criteria in a real application of Chinese text input in terms of character error rate (CER). We first present an empirical comparison, showing that rank performs the best in most cases. We also show that the high-performance of rank lies in its strong correlation with error rate. We then present a novel method of combining two criteria in model pruning. Experimental results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately, at the same CER.







 Finite-State Multimodal Parsing And Understanding 


 Multimodal interfaces require effective parsing and understanding of utterances whose content is distributed across multiple input modes. Johnston 1998 presents an approach in which strategies for multimodal integration are stated declaratively using a unification-based grammar that is used by a multidimensional chart parser to compose inputs. This approach is highly expressive and supports a broad class of interfaces, but offers only limited potential for mutual compensation among the input modes, is subject to significant concerns in terms of computational complexity, and complicates selection among alternative multimodal interpretations of the input. In this paper, we present an alternative approach in which multimodal parsing and understanding are achieved using a weighted finite-state device which takes speech and gesture streams as inputs and outputs their joint interpretation. This approach is significantly more efficient, enables tight-coupling of multimodal understanding with speech recognition, and provides a general probabilistic framework for multimodal ambiguity resolution.







Exploring Syntactic Features For Relation Extraction Using A Convolution Tree Kernel 


This paper proposes to use a convolution kernel over parse trees to model syntactic structure information for relation extraction. Our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these features can be well captured by the convolution tree kernel. Evaluation on the ACE 2003 corpus shows that the convolution kernel over parse trees can achieve comparable performance with the previous best-reported feature-based methods on the 24 ACE relation subtypes. It also shows that our method significantly outperforms the previous two dependency tree kernels on the 5 ACE relation major types.







Performing Integrated Syntactic And Semantic Parsing Using Classification 


This paper describes a particular approach to parsing that utilizes recent advances in unification-based parsing and in classification-based knowledge representation. As unification-based grammatical frameworks are extended to handle richer descriptions of linguistic information, they begin to share many of the properties that have been developed in KL-ONE-like knowledge representation systems. This commonality suggests that some of the classification-based representation techniques can be applied to unification-based linguistic descriptions. This merging supports the integration of semantic and syntactic information into the same system, simultaneously subject to the same types of processes, in an efficient manner. The result is expected to be more efficient parsing due to the increased organization of knowledge. The use of a KL-ONE style representation for parsing and semantic interpretation was first explored in the PSI-KLONE system [2], in which parsing is characterized as an inference process called incremental description refinement.







Developing And Empirically Evaluating Robust Explanation Generators : The KNIGHT Experiments


"To explain complex phenomena, an explanation system must be able to select information from a formal representation of domain knowledge, organize the selected information into multisentential discourse plans, and realize the discourse plans in text. Although recent years have witnessed significant progress in the development of sophisticated computational mechanisms for explanation, empirical results have been limited. This paper reports on a seven-year effort to empirically study explanation generation from semantically rich, large-scale knowledge bases. In particular, it describes a robust explanation system that constructs multisentential and multi-paragraph explanations from the a large-scale knowledge base in the domain of botanical anatomy, physiology, and development. We introduce the evaluation methodology and describe how performance was assessed with this methodology in the most extensive empirical evaluation conducted on an explanation system. In this evaluation, scored within ""half a grade"" of domain experts, and its performance exceeded that of one of the domain experts."







 Automatic Acquisition Of English Topic Signatures Based On A Second Language 
We present a novel approach for automatically acquiring English topic signatures. Given a particular concept, or word sense, a topic signature is a set of words that tend to co-occur with it. Topic signatures can be useful in a number of Natural Language Processing (NLP) applications, such as Word Sense Disambiguation (WSD) and Text Summarisation. Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese, and also exploits the large amount of Chinese text available in corpora and on the Web. We evaluated the topic signatures on a WSD task, where we trained a second-order vector cooccurrence algorithm on standard WSD datasets, with promising results.








A Machine Learning Approach To German Pronoun Resolution 
This paper presents a novel ensemble learning approach to resolving German pronouns. Boosting, the method in question, combines the moderately accurate hypotheses of several classifiers to form a highly accurate one. Experiments show that this approach is superior to a single decision-tree classifier. Furthermore, we present a standalone system that resolves pronouns in unannotated text by using a fully automatic sequence of preprocessing modules that mimics the manual annotation process. Although the system performs well within a limited textual domain, further research is needed to make it effective for open-domain question answering and text summarisation.








Classifying Ellipsis In Dialogue : A Machine Learning Approach 
This paper presents a machine learning approach to bare slice disambiguation in dialogue. We extract a set of heuristic principles from a corpus-based sample and formulate them as probabilistic Horn clauses. We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset, and run two different machine learning algorithms : SLIPPER, a rule-based learning algorithm, and TiMBL, a memory-based system. Both learners perform well, yielding similar success rates of approx 90%. The results show that the features in terms of which we formulate our heuristic principles have significant predictive power, and that rules that closely resemble our Horn clauses can be learnt automatically from these features.







 Feature Vector Quality And Distributional Similarity 
We suggest a new goal and evaluation criterion for word similarity measures. The new criterion  meaning-entailing substitutability  fits the needs of semantic-oriented NLP applications and can be evaluated directly (independent of an application) at a good level of human agreement. Motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results, proposing an objective measure for evaluating feature vector quality. Finally, a novel feature weighting and selection function is presented, which yields superior feature vectors and better word similarity performance.







Filtering Speaker-Specific Words From Electronic Discussions
The work presented in this paper is the first step in a project which aims to cluster and summarise electronic discussions in the context of help-desk applications. The eventual objective of this project is to use these summaries to assist help-desk users and operators. In this paper, we identify features of electronic discussions that influence the clustering process, and offer a filtering mechanism that removes undesirable influences. We tested the clustering and filtering processes on electronic newsgroup discussions, and evaluated their performance by means of two experiments : coarse-level clustering simple information retrieval.







 Part- Of- Speech Tagging In Context 
We present a new HMM tagger that exploits context on both sides of a word to be tagged, and evaluate it in both the unsupervised and supervised case. Along the way, we present the first comprehensive comparison of unsupervised methods for part-of-speech tagging, noting that published results to date have not been comparable across corpora or lexicons. Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms, we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable. Finally, we show how this new tagger achieves state-of-the-art results in a supervised, non-training intensive framework.







 Generation Of Relative Referring Expressions Based On Perceptual Grouping
Past work of generating referring expressions mainly utilized attributes of objects and binary relations between objects. However, such an approach does not work well when there is no distinctive attribute among objects. To overcome this limitation, this paper proposes a method utilizing the perceptual groups of objects and n-ary relations among them. The key is to identify groups of objects that are naturally recognized by humans. We conducted psychological experiments with 42 subjects to collect referring expressions in such situations, and built a generation algorithm based on the results. The evaluation using another 23 subjects showed that the proposed method could effectively generate proper referring expressions.








Direct Orthographical Mapping For Machine Transliteration 
Machine transliteration/back-transliteration plays an important role in many multilingual speech and language applications. In this paper, a novel framework for machine transliteration/backtransliteration that allows us to carry out direct orthographical mapping (DOM) between two different languages is presented. Under this framework, a joint source-channel transliteration model, also called n-gram transliteration model (n-gram TM), is further proposed to model the transliteration process. We evaluate the proposed methods through several transliteration/backtransliteration experiments for English/Chinese and English/Japanese language pairs. Our study reveals that the proposed method not only reduces an extensive system development effort but also improves the transliteration accuracy significantly.








A Lemma- Based Approach To A Maximum Entropy Word Sense Disambiguation System For Dutch
In this paper, we present a corpus-based supervised word sense disambiguation (WSD) system for Dutch which combines statistical classification (maximum entropy) with linguistic information. Instead of building individual classifiers per ambiguous wordform, we introduce a lemma-based approach. The advantage of this novel method is that it clusters all inflected forms of an ambiguous word in one classifier, therefore augmenting the training material available to the algorithm. Testing the lemma-based model on the Dutch Senseval-2 test data, we achieve a significant increase in accuracy over the wordform model. Also, the WSD system based on lemmas is smaller and more robust.







 Term Aggregation : Mining Synonymous Expressions Using Personal Stylistic Variations
We present a text mining method for finding synonymous expressions based on the distributional hypothesis in a set of coherent corpora. This paper proposes a new methodology to improve the accuracy of a term aggregation system using each author's text as a coherent corpus. Our approach is based on the idea that one person tends to use one expression for one meaning. According to our assumption, most of the words with similar context features in each author's corpus tend not to be synonymous expressions. Our proposed method improves the accuracy of our term aggregation system, showing that our approach is successful.







 Detection Of Question- Answer Pairs In Email Conversations
While sentence extraction as an approach to summarization has been shown to work in documents of certain genres, because of the conversational nature of email communication where utterances are made in relation to one made previously, sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent. In this paper, we present our work on the detection of question-answer pairs in an email conversation for the task of email summarization. We show that various features based on the structure of email-threads can be used to improve upon lexical similarity of discourse segments for question-answer pairing.








Fast Computation Of Lexical Affinity Models 
We present a framework for the fast computation of lexical affinity models. The framework is composed of a novel algorithm to efficiently compute the co-occurrence distribution between pairs of terms, an independence model, and a parametric affinity model. In comparison with previous models, which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models, in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus. The framework is flexible, allowing fast adaptation to applications and it is scalable. We apply it in combination with a terabyte corpus to answer natural language tests, achieving encouraging results.








Fine-Grained Word Sense Disambiguation Based On Parallel Corpora, Word Alignment, Word Clustering And Aligned Wordnets
The paper presents a method for word sense disambiguation based on parallel corpora. The method exploits recent advances in word alignment and word clustering based on automatic extraction of translation equivalents and being supported by available aligned wordnets for the languages in the corpus. The wordnets are aligned to the Princeton Wordnet, according to the principles established by EuroWordNet. The evaluation of the WSD system, implementing the method described herein showed very encouraging results. The same system used in a validation mode, can be used to check and spot alignment errors in multilingually aligned wordnets as BalkaNet and EuroWordNet.







 Minimum Bayes-Risk Decoding For Statistical Machine Translation 
We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation. This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance. We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences. We report the performance of the MBR decoders on a Chinese-to-English translation task. Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions.







 Confidence Estimation For Information Extraction 
Information extraction techniques automatically create structured databases from unstructured data sources, such as the Web or newswire documents. Despite the successes of these systems, accuracy will always be imperfect. For many reasons, it is highly desirable to accurately estimate the confidence the system has in the correctness of each extracted field. The information extraction system we evaluate is based on a linear-chain conditional random field (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a  Markov model. We implement several techniques to estimate the confidence of both extracted fields and entire multi-field records, obtaining an average precision of 98% for retrieving correct fields and 87% for multi-field records.







GE NLTOOLSET: Description Of The System As Used For MUC-4

The GE NLToolset is a set of text interpretation tools designed to be easily adapted to new domains. This report summarizes the system and its performance on the MUC-4 task. 








Exploring And Exploiting The Limited Utility Of Captions In Recognizing Intention In Information Graphics 
This paper presents a corpus study that explores the extent to which captions contribute to recognizing the intended message of an information graphic. It then presents an implemented graphic interpretation system that takes into account a variety of communicative signals, and an evaluation study showing that evidence obtained from shallow processing of the graphic's caption has a significant impact on the system's success. This work is part of a larger project whose goal is to provide sight-impaired users with effective access to information graphics.








 Log-Linear Models For Word Alignment 
We present a framework for word alignment based on log-linear models. All knowledge sources are treated as feature functions, which depend on the source langauge sentence, the target language sentence and possible additional variables. Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information. In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features. Our experiments show that log-linear models significantly outperform IBM translation models.







 Automatic Induction Of A CCG Grammar For Turkish 
This paper presents the results of automatically inducing a Combinatory Categorial Grammar (CCG) lexicon from a Turkish dependency treebank. The fact that Turkish is an agglutinating free word order language presents a challenge for language theories. We explored possible ways to obtain a compact lexicon, consistent with CCG principles, from a treebank which is an order of magnitude smaller than Penn WSJ.








 Two-Phase Shift-Reduce Deterministic Dependency Parser of Chinese 
In the Chinese language, a verb may have its dependents on its left, right or on both sides. The ambiguity resolution of right-side dependencies is essential for dependency parsing of sentences with two or more verbs. Previous works on shift-reduce dependency parsers may not guarantee the connectivity of a dependency tree due to their weakness at resolving the right-side dependencies. This paper proposes a two-phase shift-reduce dependency parser based on SVM learning. The left-side dependents and right-side nominal dependents are detected in Phase I, and right-side verbal dependents are decided in Phase II. In experimental evaluation, our proposed method outperforms previous shift-reduce dependency parsers for the Chine language, showing improvement of dependency accuracy by 10.08%.







Focusing On Focus : A Formalization
We present an operable definition of focus which is argued to be of a cognito-pragmatic nature and explore how it is determined in discourse in a formalized manner. For this purpose, a file card model of discourse model and knowledge store is introduced enabling the decomposition and formal representation of its determination process as a programmable algorithm (FDA). Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of focus via FDA as a discourse-level construct into speech synthesis systems, in particular, concept-to-speech systems, is also briefly discussed.








A Comparison Of Rule-Invocation Strategies In Context-Free Chart Parsing 
Currently several grammatical formalisms converge towards being declarative and towards utilizing context-free phrase-structure grammar as a backbone, e.g. LFG and PATR-II. Typically the processing of these formalisms is organized within a chart-parsing framework. The declarative character of the formalisms makes it important to decide upon an overall optimal control strategy on the part of the processor. In particular, this brings the rule-invocation strategy into critical focus: to gain maximal processing efficiency, one has to determine the best way of putting the rules to use. The aim of this paper is to provide a survey and a practical comparison of fundamental rule-invocation strategies within context-free chart parsing.








A Bidirectional Model For Natural Language Processing 
In this paper I will argue for a model of grammatical processing that is based on uniform processing and knowledge sources. The main feature of this model is to view parsing and generation as two strongly interleaved tasks performed by a single parametrized deduction process. It will be shown that this view supports flexible and efficient natural language processing.








A Probabilistic Context-Free Grammar For Disambiguation In Morphological Parsing 
One of the major problems one is faced with when decomposing words into their constituent parts is ambiguity: the generation of multiple analyses for one input word, many of which are implausible. In order to deal with ambiguity, the MORphological PArser MORPA is provided with a probabilistic context-free grammar (PCFG), i.e. it combines a "conventional" context-free morphological grammar to filter out ungrammatical segmentations with a probability-based scoring function which determines the likelihood of each successful parse. Consequently, remaining analyses can be ordered along a scale of plausibility. Test performance data will show that a PCFG yields good results in morphological parsing. MORPA is a fully implemented parser developed for use in a text-to-speech conversion system.







 Chinese Word Segmentation in FTRD Beijing
This paper presents a word segmentation system in France Telecom R&amp;D Beijing, which uses a unified approach to word breaking and OOV identification. The output can be customized to meet different segmentation standards through the application of an ordered list of transformation. The system participated in all the tracks of the segmentation bakeoff -- PK-open, PK-closed, AS-open, AS-closed, HK-open, HK-closed, MSR-open and MSR- closed -- and achieved the state-of-the-art performance in MSR-open, MSR-close and PK-open tracks. Analysis of the results shows that each component of the system contributed to the scores.








Coping With Derivation In A Morphological Component 
In this paper a morphological component with a limited capability to automatically interpret (and generate) derived words is presented. The system combines an extended two-level morphology [Trost, 1991a; Trost, 1991b] with a feature-based word grammar building on a hierarchical lexicon. Polymorphemic stems not explicitly stored in the lexicon are given a compositional interpretation. That way the system allows to minimize redundancy in the lexicon because derived words that are transparent need not to be stored explicitly. Also, words formed ad-hoc can be recognized correctly. The system is implemented in CommonLisp and has been tested on examples from German derivation.








Full Text Parsing Using Cascades Of Rules: An Information Extraction Perspective
This paper proposes an approach to full parsing suitable for Information Extraction from texts. Sequences of cascades of rules deterministically analyze the text, building unambiguous structures. Initially basic chunks are analyzed; then argumental relations are recognized; finally modifier attachment is performed and the global parse tree is built. The approach was proven to work for three languages and different domains. It was implemented in the IE module of FACILE, a EU project for multilingual text classification and IE.








New Results With The Lincoln Tied-Mixture HMM CSR System 
The following describes recent work on the Lincoln CSR system. Some new variations in semiphone modeling have been tested. A very simple improved duration model has reduced the error rate by about 10% in both triphone and semiphone systems. A new training strategy has been tested which, by itself, did not provide useful improvements but suggests that improvements can be obtained by a related rapid adaptation technique. Finally, the recognizer has been modified to use bigram back-off language models. The system was then transferred from the RM task to the ATIS CSR task and a limited number of development tests performed. Evaluation test results are presented for both the RM and ATIS CSR tasks.






 
Reading more into Foreign Languages

GLOSSER is designed to support reading and learning to read in a foreign language. There are four language pairs currently supported by GLOSSER: English-Bulgarian, English-Estonian, English-Hungarian and French-Dutch. The program is operational on UNIX and Windows '95 platforms, and has undergone a pilot user-study. A demonstration (in UNIX) for Applied Natural Language Processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis (ICALL), including disambiguated morphological analysis and lemmatized indexing for an aligned bilingual corpus of word examples. 






 Identifying Topics By Position
This paper addresses the problem of identifying likely topics of texts by their position in the text. It describes the automated training and evaluation of an Optimal Position Policy, a method of locating the likely positions of topic-bearing sentences based on genre-specific regularities of discourse structure. This method can be used in applications such as information retrieval, routing, and text summarization.








 Hidden-Variable Models For Discriminative Reranking 
We describe a new method for the representation of NLP structures within reranking approaches. We make use of a conditional log-linear model, with hidden variables representing the assignment of lexical items to word clusters or word senses. The model learns to automatically make these assignments based on a discriminative training criterion. Training and decoding with the model requires summing over an exponential number of hidden-variable assignments: the required summations can be computed efficiently and exactly using dynamic programming. As a case study, we apply the model to parse reranking. The model gives an F-measure improvement of ~1.25% beyond the base parser, and an ~0.25% improvement beyond Collins (2000) reranker. Although our experiments are focused on parsing, the techniques described generalize naturally to NLP structures other than parse trees.








 Taiwan Child Language Corpus : Data Collection and Annotation 
Taiwan Child Language Corpus contains scripts transcribed from about 330 hours of recordings of fourteen young children from Southern Min Chinese speaking families in Taiwan. The format of the corpus adopts the Child Language Data Exchange System (CHILDES). The size of the corpus is about 1.6 million words. In this paper, we describe data collection, transcription, word segmentation, and part-of-speech annotation of this corpus. Applications of the corpus are also discussed.






 

 Dynamic Strategy Selection in Flexible Parsing 

Robust natural language interpretation requires strong semantic domain models, fail-soft recovery heuristics, and very flexible control structures. Although single-strategy parsers have met with a measure of success, a multi-strategy approach is shown to provide a much higher degree of flexibility, redundancy, and ability to bring task-specific domain knowledge (in addition to general linguistic knowledge) to bear on both grammatical and ungrammatical input. A parsing algorithm is presented that integrates several different parsing strategies, with case-frame instantiation dominating. Each of these parsing strategies exploits different types of knowledge; and their combination provides a strong framework in which to process conjunctions, fragmentary input, and ungrammatical structures, as well as less exotic, grammatically correct input. Several specific heuristics for handling ungrammatical input are presented within this multi-strategy framework. 





 

 Parsing with Discontinuous Constituents 

By generalizing the notion of location of a constituent to allow discontinuous locations, one can describe the discontinuous constituents of non-configurational languages. These discontinuous constituents can be described by a variant of definite clause grammars, and these grammars can be used in conjunction with a proof procedure to create a parser for non-configurational languages. 





 

The Acquisition and Application of Context Sensitive Grammar for English 

 A system is described for acquiring a context-sensitive, phrase structure grammar which is applied by a best-path, bottom-up, deterministic parser. The grammar was based on English news stories and a high degree of success in parsing is reported. Overall, this research concludes that CSG is a computationally and conceptually tractable approach to the construction of phrase structure grammar for news story text. 





 

A Quantitative Evaluation of Linguistic Tests for the Automatic Prediction of Semantic Markedness 

We present a corpus-based study of methods that have been proposed in the linguistics literature for selecting the semantically unmarked term out of a pair of antonymous adjectives. Solutions to this problem are applicable to the more general task of selecting the positive term from the pair. Using automatically collected data, the accuracy and applicability of each method is quantified, and a statistical analysis of the significance of the results is performed. We show that some simple methods are indeed good indicators for the answer to the problem while other proposed methods fail to perform better than would be attributable to chance. In addition, one of the simplest methods, text frequency, dominates all others. We also apply two generic statistical learning methods for combining the indications of the individual methods, and compare their performance to the simple methods. The most sophisticated complex learning method offers a small, but statistically significant, improvement over the original tests. 





 

Probing the lexicon in evaluating commercial MT systems 

 In the past the evaluation of machine translation systems has focused on single system evaluations because there were only few systems available. But now there are several commercial systems for the same language pair. This requires new methods of comparative evaluation. In the paper we propose a black-box method for comparing the lexical coverage of MT systems. The method is based on lists of words from different frequency classes. It is shown how these word lists can be compiled and used for testing. We also present the results of using our method on 6 MT systems that translate between English and German. 





 

On Interpreting F-Structures as UDRSs 

We describe a method for interpreting abstract flat syntactic representations, LFG f-structures, as underspecified semantic representations, here Underspecified Discourse Representation Structures (UDRSs). The method establishes a one-to-one correspondence between subsets of the LFG and UDRS formalisms. It provides a model theoretic interpretation and an inferential component which operates directly on underspecified representations for f-structures through the translation images of f-structures as UDRSs. 





 

 Construct Algebra : Analytical Dialog Management 

In this paper we describe a systematic approach for creating a dialog management system based on a Construct Algebra, a collection of relations and operations on a task representation. These relations and operations are analytical components for building higher level abstractions called dialog motivators. The dialog manager, consisting of a collection of dialog motivators, is entirely built using the Construct Algebra. 





 

Mining the Web for Bilingual Text 

STRAND (Resnik, 1998) is a language-independent system for automatic discovery of text in parallel translation on the World Wide Web. This paper extends the preliminary STRAND results by adding automatic language identification, scaling up by orders of magnitude, and formally evaluating performance. The most recent end-product is an automatically acquired parallel corpus comprising 2491 English-French document pairs, approximately 1.5 million words per language. 






 Verb-Noun Collocation SyntLex Dictionary : Corpus-Based Approach 

The project presented here is a part of a long term research program aiming at a full lexicon grammar for Polish (SyntLex). The main of this project is computer-assisted acquisition and morpho-syntactic description of verb-noun collocations in Polish. We present methodology and resources obtained in three main project phases which are: dictionary-based acquisition of collocation lexicon, feasibility study for corpus-based lexicon enlargement phase, corpus-based lexicon enlargement and collocation description. In this paper we focus on the results of the third phase. The presented here corpus-based approach permitted us to triple the size the verb-noun collocation dictionary for Polish. In the paper we describe the SyntLex Dictionary of Collocations and announce some future research intended to be a separate project continuation.







 Czech MWE Database 

In this paper we deal with a recently developed large Czech MWE database containing at the moment 160 000 MWEs (treated as lexical units). It was compiled from various resources such as encyclopedias and dictionaries, public databases of proper names and toponyms, collocations obtained from Czech WordNet, lists of botanical and zoological terms and others. We describe the structure of the database and give basic types of MWEs according to domains they belong to. We compare the built MWEs database with the corpus data from Czech National Corpus (approx. 100 mil. tokens) and present results of this comparison in the paper. These MWEs have not been obtained from the corpus since their frequencies in it are rather low. To obtain a more complete list of MWEs we propose and use a technique exploiting the Word Sketch Engine, which allows us to work with statistical parameters such as frequency of MWEs and their components as well as with the salience for the whole MWEs. We also discuss exploitation of the database for working out a more adequate tagging and lemmatization. The final goal is to be able to recognize MWEs in corpus text and lemmatize them as complete lexical units, i. e. to make tagging and lemmatization more adequate.







Using Log-linear Models for Tuning Machine Translation Output 

We describe a set of experiments to explore statistical techniques for ranking and selecting the best translations in a graph of translation hypotheses. In a previous paper (Carl, 2007) we have described how the hypotheses graph is generated through shallow mapping and permutation rules. We have given examples of its nodes consisting of vectors representing morpho-syntactic properties of words and phrases. This paper describes a number of methods for elaborating statistical feature functions from some of the vector components. The feature functions are trained off-line on different types of text and their log-linear combination is then used to retrieve the best M translation paths in the graph. We compare two language modelling toolkits, the CMU and the SRI toolkit and arrive at three results: 1) word-lemma based feature function models produce better results than token-based models, 2) adding a PoS-tag feature function to the word-lemma model improves the output and 3) weights for lexical translations are suitable if the training material is similar to the texts to be translated.






 Chinese Term Extraction Based on Delimiters 

Existing techniques extract term candidates by looking for internal and contextual information associated with domain specific terms. The algorithms always face the dilemma that fewer features are not enough to distinguish terms from non-terms whereas more features lead to more conflicts among selected features. This paper presents a novel approach for term extraction based on delimiters which are much more stable and domain independent. The proposed approach is not as sensitive to term frequency as that of previous works. This approach has no strict limit or hard rules and thus they can deal with all kinds of terms. It also requires no prior domain knowledge and no additional training to adapt to new domains. Consequently, the proposed approach can be applied to different domains easily and it is especially useful for resource-limited domains. Evaluations conducted on two different domains for Chinese term extraction show significant improvements over existing techniques which verifies its efficiency and domain independent nature. Experiments on new term extraction indicate that the proposed approach can also serve as an effective tool for domain lexicon expansion.







From Sentence to Discourse : Building an Annotation Scheme for Discourse Based on Prague Dependency Treebank 

The present paper reports on a preparatory research for building a language corpus annotation scenario capturing the discourse relations in Czech. We primarily focus on the description of the syntactically motivated relations in discourse, basing our findings on the theoretical background of the Prague Dependency Treebank 2.0 and the Penn Discourse Treebank 2. Our aim is to revisit the present-day syntactico-semantic (tectogrammatical) annotation in the Prague Dependency Treebank, extend it for the purposes of a sentence-boundary-crossing representation and eventually to design a new, discourse level of annotation. In this paper, we propose a feasible process of such a transfer, comparing the possibilities the Praguian dependency-based approach offers with the Penn discourse annotation based primarily on the analysis and classification of discourse connectives.







 Unsupervised Acquisition of Verb Subcategorization Frames from Shallow-Parsed Corpora 

In this paper, we reported experiments of unsupervised automatic acquisition of Italian and English verb subcategorization frames (SCFs) from general and domain corpora. The proposed technique operates on syntactically shallow-parsed corpora on the basis of a limited number of search heuristics not relying on any previous lexico-syntactic knowledge about SCFs. Although preliminary, reported results are in line with state-of-the-art lexical acquisition systems. The issue of whether verbs sharing similar SCFs distributions happen to share similar semantic properties as well was also explored by clustering verbs that share frames with the same distribution using the Minimum Description Length Principle (MDL). First experiments in this direction were carried out on Italian verbs with encouraging results.







A Multi-Path Architecture For Machine Translation Of English Text Into American Sign Language Animation 

The translation of English text into American Sign Language (ASL) animation tests the limits of traditional MT architectural designs. A new semantic representation is proposed that uses virtual reality 3D scene modeling software to produce spatially complex ASL phenomena called "classifier predicates." The model acts as an interlingua within a new multi-pathway MT architecture design that also incorporates transfer and direct approaches into a single system.







Integrating Natural Language Components Into Graphical Discourse 

In our current research into the design of cognitively well-motivated interfaces relying primarily on the display of graphical information, we have observed that graphical information alone does not provide sufficient support to users - particularly when situations arise that do not simply conform to the users' expectations. This can occur due to too much information being requested, too little, information of the wrong kind, etc. To solve this problem, we are working towards the integration of natural language generation to augment the interaction






The LIMSI Continuous Speech Dictation System 

A major axis of research at LIMSI is directed at multilingual, speaker-independent, large vocabulary speech dictation. In this paper the LIMSI recognizer which was evaluated in the ARPA NOV93 CSR test is described, and experimental results on the WSJ and BREF corpora under closely matched conditions are reported. For both corpora word recognition experiments were carried out with vocabularies containing up to 20k words. The recognizer makes use of continuous density HMM with Gaussian mixture for acoustic modeling and n-gram statistics estimated on the newspaper texts for language modeling. The recognizer uses a time-synchronous graph-search strategy which is shown to still be viable with a 20k-word vocabulary when used with bigram back-off language models. A second forward pass, which makes use of a word graph generated with the bigram, incorporates a trigram language model. Acoustic modeling uses cepstrum-based features, context-dependent phone models (intra and interword), phone duration models, and sex-dependent models.








Categorizing Unknown Words : Using Decision Trees To Identify Names And Misspellings 

This paper introduces a system for categorizing unknown words. The system is based on a multi-component architecture where each component is responsible for identifying one class of unknown words. The focus of this paper is the components that identify names and spelling errors. Each component uses a decision tree architecture to combine multiple types of evidence about the unknown word. The system is evaluated using data from live closed captions - a genre replete with a wide variety of unknown words.







NEC Corporation And University Of Sheffield: Description Of NEC/Sheffleld System Used For MET Japanese 

Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of morphological analysis in Japanese text processing ([1] [2]). It has also been studied in the framework of Japanese information extraction ([3]) in recent years. Our approach to the Multi-lingual Evaluation Task (MET) for Japanese text is to consider the given task as a morphological analysis problem in Japanese. Our morphological analyzer has done all the necessary work for the recognition and classification of proper names, numerical and temporal expressions, i.e. Named Entity (NE) items in the Japanese text. The analyzer is called "Amorph". Amorph recognizes NE items in two stages: dictionary lookup and rule application. First, it uses several kinds of dictionaries to segment and tag Japanese character strings. Second, based on the information resulting from the dictionary lookup stage, a set of rules is applied to the segmented strings in order to identify NE items. When a segment is found to be an NE item, this information is added to the segment and it is used to generate the final output.







A Practically Unsupervised Learning Method To Identify Single-Snippet Answers To Definition Questions On The Web

We present a practically unsupervised learning method to produce single-snippet answers to definition questions in question answering systems that supplement Web search engines. The method exploits on-line encyclopedias and dictionaries to generate automatically an arbitrarily large number of positive and negative definition examples, which are then used to train an svm to separate the two classes. We show experimentally that the proposed method is viable, that it outperforms the alternative of training the system on questions and news articles from trec, and that it helps the search engine handle definition questions significantly better.







 Lexically-Based Terminology Structuring : Some Inherent Limits

Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying hierarchical (or other types of) relations between these terms. The present paper focusses on terminology structuring by lexical methods, which match terms on the basis on their content words, taking morphological variants into account. Experiments are done on a 'flat' list of terms obtained from an originally hierarchically-structured terminology: the French version of the US National Library of Medicine MeSH thesaurus. We compare the lexically-induced relations with the original MeSH relations: after a quantitative evaluation of their congruence through recall and precision metrics, we perform a qualitative, human analysis ofthe 'new' relations not present in the MeSH. This analysis shows, on the one hand, the limits of the lexical structuring method. On the other hand, it also reveals some specific structuring choices and naming conventions made by the MeSH designers, and emphasizes ontological commitments that cannot be left to automatic structuring.







 Alignment And Extraction Of Bilingual Legal Terminology From Context Profiles 

In this study, we propose a knowledge-independent method for aligning terms and thus extracting translations from a small, domain-specific corpus consisting of parallel English and Chinese court judgments from Hong Kong. With a sentence-aligned corpus, translation equivalences are suggested by analysing the frequency profiles of parallel concordances. The method overcomes the limitations of conventional statistical methods which require large corpora to be effective, and lexical approaches which depend on existing bilingual dictionaries. Pilot testing on a parallel corpus of about 113K Chinese words and 120K English words gives an encouraging 85% precision and 45% recall. Future work includes fine-tuning the algorithm upon the analysis of the errors, and acquiring a translation lexicon for legal terminology by filtering out general terms.







 Coedition To Share Text Revision Across Languages And Improve MT A Posteriori

Coedition of a natural language text and its representation in some interlingual form seems the best and simplest way to share text revision across languages. For various reasons, UNL graphs are the best candidates in this context. We are developing a prototype where, in the simplest sharing scenario, naive users interact directly with the text in their language (L0), and indirectly with the associated graph. The modified graph is then sent to the UNL-L0 deconverter and the result shown. If is is satisfactory, the errors were probably due to the graph, not to the deconverter, and the graph is sent to deconverters in other languages. Versions in some other languages known by the user may be displayed, so that improvement sharing is visible and encouraging. As new versions are added with appropriate tags and attributes in the original multilingual document, nothing is ever lost, and cooperative working on a document is rendered feasible. On the internal side, liaisons are established between elements of the text and the graph by using broadly available resources such as a LO-English or better a L0-UNL dictionary, a morphosyntactic parser of L0, and a canonical graph2tree transformation. Establishing a "best" correspondence between the "UNL-tree+L0" and the "MS-L0 structure", a lattice, may be done using the dictionary and trying to align the tree and the selected trajectory with as few crossing liaisons as possible. A central goal of this research is to merge approaches from pivot MT, interactive MT, and multilingual text authoring.









Unsupervised Learning Of Word Sense Disambiguation Rules By Estimating An Optimum Iteration Number In The EM Algorithm 

In this paper, we improve an unsupervised learning method using the Expectation-Maximization (EM) algorithm proposed by Nigam et al. for text classification problems in order to apply it to word sense disambiguation (WSD) problems. The improved method stops the EM algorithm at the optimum iteration number. To estimate that number, we propose two methods. In experiments, we solved 50 noun WSD problems in the Japanese Dictionary Task in SENSEVAL2. The score of our method is a match for the best public score of this task. Furthermore, our methods were confirmed to be effective also for verb WSD problems.








Modeling User Language Proficiency In A Writing Tutor For Deaf Learners Of English 

In this paper we discuss a proposed user knowledge modeling architecture for the ICICLE system, a language tutoring application for deaf learners of written English. The model will represent the language proficiency of the user and is designed to be referenced during both writing analysis and feedback production. We motivate our model design by citing relevant research on second language and cognitive skill acquisition, and briefly discuss preliminary empirical evidence supporting the design. We conclude by showing how our design can provide a rich and robust information base to a language assessment / correction application by modeling user proficiency at a high level of granularity and specificity.









Using Decision Trees to Construct a Practical Parser 

This paper describes novel and practical Japanese parsers that uses decision trees. First, we construct a single decision tree to estimate modification probabilities; how one phrase tends to modify another. Next, we introduce a boosting algorithm in which several decision trees are constructed and then combined for probability estimation. The two constructed parsers are evaluated by using the EDR Japanese annotated corpus. The single-tree method outperforms the conventional Japanese stochastic methods by 4%. Moreover, the boosting version is shown to have significant advantages; 1) better parsing accuracy than its single-tree counterpart for any amount of training data and 2) no over-fitting to data for various iterations.









 Automatic Estimation of Word Significance oriented for Speech-based Information Retrieval 

Automatic estimation of word significance oriented for speech-based Information Retrieval (IR) is addressed. Since the significance of words differs in IR, automatic speech recognition (ASR) performance has been evaluated based on weighted word error rate (WWER), which gives a weight on errors from the viewpoint of IR, instead of word error rate (WER), which treats all words uniformly. A decoding strategy that minimizes WWER based on a Minimum Bayes-Risk framework has been shown, and the reduction of errors on both ASR and IR has been reported. In this paper, we propose an automatic estimation method for word significance (weights) based on its influence on IR. Specifically, weights are estimated so that evaluation measures of ASR and IR are equivalent. We apply the proposed method to a speech-based information retrieval system, which is a typical IR system, and show that the method works well.









Paraphrasing Depending on Bilingual Context Toward Generalization of Translation Knowledge 

This study presents a method to automatically acquire paraphrases using bilingual corpora, which utilizes the bilingual dependency relations obtained by projecting a monolingual dependency parse onto the other language sentence based on statistical alignment techniques. Since the paraphrasing method is capable of clearly disambiguating the sense of an original phrase using the bilingual context of dependency relation, it would be possible to obtain interchangeable paraphrases under a given context. Also, we provide an advanced method to acquire generalized translation knowledge using the extracted paraphrases. We applied the method to acquire the generalized translation knowledge for Korean-English translation. Through experiments with parallel corpora of a Korean and English language pairs, we show that our paraphrasing method effectively extracts paraphrases with high precision, 94.3% and 84.6% respectively for Korean and English, and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5% compression ratio.








 Statistics Learning And Universal Grammar : Modeling Word Segmentation 

This paper describes a computational model of word segmentation and presents simulation results on realistic acquisition. In particular, we explore the capacity and limitations of statistical learning mechanisms that have recently gained prominence in cognitive psychology and linguistics.








Automatic Construction Of A Transfer Dictionary Considering Directionality 

In this paper, we show how to construct a transfer dictionary automatically. Dictionary construction, one of the most difficult tasks in developing a machine translation system, is expensive. To avoid this problem, we investigate how we build a dictionary using existing linguistic resources. Our algorithm can be applied to any language pairs, but for the present we focus on building a Korean-to-Japanese dictionary using English as a pivot. We attempt three ways of automatic construction to corroborate the effect of the directionality of dictionaries. First, we introduce "one-time look up" method using a Korean-to-English and a Japanese-to-English dictionary. Second, we show a method using "overlapping constraint" with a Korean-to-English dictionary and an English-to-Japanese dictionary. Third, we consider another alternative method rarely used for building a dictionary: an English-to-Korean dictionary and English-to-Japanese dictionary. We found that the first method is the most effective and the best result can be obtained from combining the three methods.








Annotating Discourse Connectives And Their Arguments

This paper describes a new, large scale discourse-level annotation project - the Penn Discourse TreeBank (PDTB). We present an approach to annotating a level of discourse structure that is based on identifying discourse connectives and their arguments. The PDTB is being built directly on top of the Penn TreeBank and Propbank, thus supporting the extraction of useful syntactic and semantic features and providing a richer substrate for the development and evaluation of practical algorithms. We provide a detailed preliminary analysis of inter-annotator agreement - both the level of agreement and the types of inter-annotator variation.








INTEX : A Syntactic Role Driven Protein-Protein Interaction Extractor For Bio-Medical Text 

In this paper, we present a fully automated extraction system, named IntEx, to identify gene and protein interactions in biomedical text. Our approach is based on first splitting complex sentences into simple clausal structures made up of syntactic roles. Then, tagging biological entities with the help of biomedical and linguistic ontologies. Finally, extracting complete interactions by analyzing the matching contents of syntactic roles and their linguistically significant combinations. Our extraction system handles complex sentences and extracts multiple and nested interactions specified in a sentence. Experimental evaluations with two other state of the art extraction systems indicate that the IntEx system achieves better performance without the labor intensive pattern engineering requirement.









 Distributional Measures Of Concept- Distance : A Task-Oriented Evaluation

We propose a framework to derive the distance between concepts from distributional measures of word co-occurrences. We use the categories in a published thesaurus as coarse-grained concepts, allowing all possible distance values to be stored in a concept-concept matrix roughly.01% the size of that created by existing measures. We show that the newly proposed concept-distance measures outperform traditional distributional word-distance measures in the tasks of (1) ranking word pairs in order of semantic distance, and (2) correcting real-word spelling errors. In the latter task, of all the WordNet-based measures, only that proposed by Jiang and Conrath outperforms the best distributional concept-distance measures.








Learning to Transform Linguistic Graphs 

We argue in favor of the the use of labeled directed graph to represent various types of linguistic structures, and illustrate how this allows one to view NLP tasks as graph transformations. We present a general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method: identification of non-local depenencies (using Penn Treebank data) and semantic role labeling (using Proposition Bank data).








Credibility Improves Topical Blog Post Retrieval 

Topical blog post retrieval is the task of ranking blog posts with respect to their relevance for a given topic. To improve topical blog post retrieval we incorporate textual credibility indicators in the retrieval process. We consider two groups of indicators: post level (determined using information about individual blog posts only) and blog level (determined using information from the underlying blogs). We describe how to estimate these indicators and how to integrate them into a retrieval approach based on language models. Experiments on the TREC Blog track test set show that both groups of credibility indicators significantly improve retrieval effectiveness; the best performance is achieved when combining them.








Lyric-based Song Sentiment Classification with Sentiment Vector Space Model 
Lyric-based song sentiment classification seeks to assign songs appropriate sentiment labels such as light-hearted heavy-hearted. Four problems render vector space model (VSM)-based text classification approach ineffective: 1) Many words within song lyrics actually contribute little to sentiment; 2) Nouns and verbs used to express sentiment are ambiguous; 3) Negations and modifiers around the sentiment keywords make particular contributions to sentiment; 4) Song lyric is usually very short. To address these problems, the sentiment vector space model (s-VSM) is proposed to represent song lyric document. The preliminary experiments prove that the s-VSM model outperforms the VSM model in the lyric-based song sentiment classification task. 









Source Language Markers in EUROPARL Translations 

This paper shows that it is very often possible to identify the source language of medium-length speeches in the EUROPARL corpus on the basis of frequency counts of word n-grams (87.2%-96.7% accuracy depending on classification method). The paper also examines in detail which positive markers are most powerful and identifies a number of linguistic aspects as well as culture- and domain-related ones.








Bayesian Semi-Supervised Chinese Word Segmentation for Statistical Machine Translation 

Words in Chinese text are not naturally separated by delimiters, which poses a challenge to standard machine translation (MT) systems. In MT, the widely used approach is to apply a Chinese word segmenter trained from manually annotated data, using a fixed lexicon. Such word segmentation is not necessarily optimal for translation. We propose a Bayesian semi-supervised Chinese word segmentation model which uses both monolingual and bilingual information to derive a segmentation suitable for MT. Experiments show that our method improves a state-of-the-art MT system in a small and a large data environment.








The Impact of Reference Quality on Automatic MT Evaluation 

Language resource quality is crucial in NLP. Many of the resources used are derived from data created by human beings out of an NLP context, especially regarding MT and reference translations. Indeed, automatic evaluations need high-quality data that allow the comparison of both automatic and human translations. The validation of these resources is widely recommended before being used. This paper describes the impact of using different-quality references on evaluation. Surprisingly enough, similar scores are obtained in many cases regardless of the quality. Thus, the limitations of the automatic metrics used within MT are also discussed in this regard.









A Linguistic Knowledge Discovery Tool : Very Large Ngram Database Search with Arbitrary Wildcards 

In this paper, we will describe a search tool for a huge set of ngrams. The tool supports queries with an arbitrary number of wildcards. It takes a fraction of a second for a search, and can provide the fillers of the wildcards. The system runs on a single Linux PC with reasonable size memory (less than 4GB) and disk space (less than 400GB). This system can be a very useful tool for linguistic knowledge discovery and other NLP tasks.








Unsupervised Learning of Bulgarian POS Tags 

This paper presents an approach to the unsupervised learning of parts of speech which uses both morphological and syntactic information. While the model is more complex than those which have been employed for unsupervised learning of POS tags in English, which use only syntactic information, the variety of languages in the world requires that we consider morphology as well. In many languages, morphology provides better clues to a word's category than word order. We present the computational model for POS learning, and present results for applying it to Bulgarian, a Slavic language with relatively free word order and rich morphology.









A Latent Variable Model of Synchronous Parsing for Syntactic and Semantic Dependencies 

We propose a solution to the challenge of the CoNLL 2008 shared task that uses a generative history-based latent variable model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. The submitted model yields 79.1% macro-average F1 performance, for the joint task, 86.9% syntactic dependencies LAS and 71.0% semantic dependencies F1. A larger model trained after the deadline achieves 80.5% macro-average F1, 87.6% syntactic dependencies LAS, and 73.1% semantic dependencies F1.









Integrating Discourse Markers Into A Pipelined Natural Language Generation Architecture 

Pipelined Natural Language Generation (NLG) systems have grown increasingly complex as architectural modules were added to support language functionalities such as referring expressions, lexical choice, and revision. This has given rise to discussions about the relative placement of these new modules in the overall architecture. Recent work on another aspect of multi-paragraph text, discourse markers, indicates it is time to consider where a discourse marker insertion algorithm fits in. We present examples which suggest that in a pipelined NLG architecture, the best approach is to strongly tie it to a revision component. Finally, we evaluate the approach in a working multi-page system.









 Multi-Tagging For Lexicalized-Grammar Parsing 

With performance above 97% accuracy for newspaper text, part of speech (pos) tagging might be considered a solved problem. Previous studies have shown that allowing the parser to resolve pos tag ambiguity does not improve performance. However, for grammar formalisms which use more fine-grained grammatical categories, for example tag and ccg, tagging accuracy is much lower. In fact, for these formalisms, premature ambiguity resolution makes parsing infeasible. We describe a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efficient ccg parsing. We extend this multi-tagging approach to the pos level to overcome errors introduced by automatically assigned pos tags. Although pos tagging accuracy seems high, maintaining some pos tag ambiguity in the language processing pipeline results in more accurate ccg supertagging.









 Discursive Usage Of Six Chinese Punctuation Marks 

Both rhetorical structure and punctuation have been helpful in discourse processing. Based on a corpus annotation project, this paper reports the discursive usage of 6 Chinese punctuation marks in news commentary texts: Colon, Dash, Ellipsis, Exclamation Mark, Question Mark, and Semicolon. The rhetorical patterns of these marks are compared against patterns around cue phrases in general. Results show that these Chinese punctuation marks, though fewer in number than cue phrases, are easy to identify, have strong correlation with certain relations, and can be used as distinctive indicators of nuclearity in Chinese texts.








 Partial Descriptions And Systemic Grammar

This paper examines the properties of feature-based partial descriptions built on top of Halliday's systemic networks. We show that the crucial operation of consistency checking for such descriptions is NP-complete, and therefore probably intractable, but proceed to develop algorithms which can sometimes alleviate the unpleasant consequences of this intractability.








 Classifier Assignment By Corpus-Based Approach

This paper presents an algorithm for selecting an appropriate classifier word for a noun. In Thai language, it frequently happens that there is fluctuation in the choice of classifier for a given concrete noun, both from the point of view of the whole speech community and individual speakers. Basically, there is no exact rule for classifier selection. As far as we can do in the rule-based approach is to give a default rule to pick up a corresponding classifier of each noun. Registration of classifier for each noun is limited to the type of unit classifier because other types are open due to the meaning of representation. We propose a corpus-based method (Biber,1993; Nagao,1993; Smadja,1993) which generates Noun Classifier Associations (NCA) to overcome the problems in classifier assignment and semantic construction of noun phrase. The NCA is created statistically from a large corpus and recomposed under concept hierarchy constraints and frequency of occurrences. 









An Unsupervised Learning Method For Associative Relationships Between Verb Phrases 

This paper describes an unsupervised learning method for associative relationships between verb phrases, which is important in developing reliable Q&amp;A systems. Consider the situation that a user gives a query "How much petrol was imported to Japan from Saudi Arabia?" to a Q&amp;A system, but the text given to the system includes only the description "X tonnes of petrol was conveyed to Japan from Saudi Arabia". We think that the description is a good clue to find the answer for our query, "X tonnes". But there is no large-scale database that provides the associative relationship between "imported" and "conveyed". Our aim is to develop an unsupervised learning method that can obtain such an associative relationship, which we call scenario consistency. The method we are currently working on uses an expectation-maximization (EM) based word-clustering algorithm, and we have evaluated the effectiveness of this method using Japanese verb phrases. 








 Automatic Learning Of Language Model Structure 

Statistical language modeling remains a challenging task, in particular for morphologically rich languages. Recently, new approaches based on factored language models have been developed to address this problem. These models provide principled ways of including additional conditioning variables other than the preceding words, such as morphological or syntactic features. However, the number of possible choices for model parameters creates a large space of models that cannot be searched exhaustively. This paper presents an entirely data-driven model selection procedure based on genetic search, which is shown to outperform both knowledge-based and random selection procedures on two different language modeling tasks (Arabic and Turkish).








 Montagovian Definite Clause Grammar 

This paper reports a completed stage of ongoing research at the University of York. Landsbergen's advocacy of analytical inverses for compositional syntax rules encourages the application of Definite Clause Grammar techniques to the construction of a parser returning  Montague analysis trees. A parser MDCC is presented which implements an augmented Friedman - Warren algorithm permitting post referencing* and interfaces with a language of intenslonal logic translator LILT so as to display the derivational history of corresponding reduced IL formulae. Some familiarity with Montague's PTQ and the basic DCG mechanism is assumed.









An Approach To Sentence-Level Anaphora In Machine Translation 

Theoretical research in the area of machine translation usually involves the search for and creation of an appropriate formalism. An important issue in this respect is the way in which the compositionality of translation is to be defined. In this paper, we will introduce the anaphoric component of the Mimo formalism. It makes the definition and translation of anaphoric relations possible, relations which are usually problematic for systems that adhere to strict compositionality. In Mimo, the translation of anaphoric relations is compositional. The anaphoric component is used to define linguistic phenomena such as wh-movement, the passive and the binding of reflexives and pronouns mono-lingually. The actual working of the component will be shown in this paper by means of a detailed discussion of wh-movement.








 Interpretation Of Nominal Compounds : Combining Domain-Independent And Domain-Specific Information 

A domain independent model is proposed for the automated interpretation of nominal compounds in English. This model is meant to account for productive rules of interpretation which are inferred from the morpho-syntactic and semantic characteristics of the nominal constituents. In particular, we make extensive use of Pustejovsky's principles concerning the predicative information associated with nominals. We argue that it is necessary to draw a line between generalizable semantic principles and domain-specific semantic information. We explain this distinction and we show how this model may be applied to the interpretation of compounds in real texts, provided that complementary semantic information are retrieved.






Modeling Local Coherence: An Entity-Based Approach

This paper considers the problem of automatic assessment of local coherence. We present a novel entity-based representation of discourse which is inspired by Centering Theory and can be computed automatically from raw text. We view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function. Our experiments demonstrate that the induced model achieves significantly higher accuracy than a state-of-the-art coherence model.






Using Conditional Random Fields For Sentence Boundary Detection In Speech

Sentence boundary detection in speech is important for enriching speech recognition output, making it easier for humans to read and downstream modules to process. In previous work, we have developed hidden Markov model (HMM) and maximum entropy (Maxent) classifiers that integrate textual and prosodic knowledge sources for detecting sentence boundaries. In this paper, we evaluate the use of a conditional random field (CRF) for this task and relate results with this model to our prior work. We evaluate across two corpora (conversational telephone speech and broadcast news speech) on both human transcriptions and speech recognition output. In general, our CRF model yields a lower error rate than the HMM and Max-ent models on the NIST sentence boundary detection task in speech, although it is interesting to note that the best results are achieved by three-way voting among the classifiers. This probably occurs because each model has different strengths and weaknesses for modeling the knowledge sources.






Using Emoticons To Reduce Dependency In Machine Learning Techniques For Sentiment Classification

Sentiment Classification seeks to identify a piece of text according to its author's general feeling toward their subject, be it positive or negative. Traditional machine learning techniques have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the training and test data with respect to topic. This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with training data labeled with emoticons, which has the potential of being independent of domain, topic and time.





Trend Survey on Japanese Natural Language Processing Studies over the Last Decade

Using natural language processing, we carried out a trend survey on Japanese natural language processing studies that have been done over the last ten years. We determined the changes in the number of papers published for each research organization and on each research area as well as the relationship between research organizations and research areas. This paper is useful for both recognizing trends in Japanese NLP and constructing a method of supporting trend surveys using NLP.






Finding Content-Bearing Terms Using Term Similarities

This paper explores the issue of using different co-occurrence similarities between terms for separating query terms that are useful for retrieval from those that are harmful. The hypothesis under examination is that useful terms tend to be more similar to each other than to other query terms. Preliminary experiments with similarities computed using first-order and second-order co-occurrence seem to confirm the hypothesis. Term similarities could then be used for determining which query terms are useful and best reflect the user's information need. A possible application would be to use this source of evidence for tuning the weights of the query terms.






The Structure Of Communicative Context Of Dialogue Interaction

We propose a draft scheme of the model formalizing the structure of communicative context in dialogue interaction. The relationships between the interacting partners are considered as system of three automata representing the partners of the dialogue and environment.






Non-Deterministic Recursive Ascent Parsing

A purely functional implementation of LR-parsers is given, together with a simple correctness proof. It is presented as a generalization of the recursive descent parser. For non-LR grammars the time-complexity of our parser is cubic if the functions that constitute the parser are implemented as memo-functions, i.e. functions that memorize the results of previous invocations. Memo-functions also facilitate a simple way to construct a very compact representation of the parse forest. For LR(0) grammars, our algorithm is closely related to the recursive ascent parsers recently discovered by Kruse-man Aretz [1] and Roberts [2]. Extended CF grammars (grammars with regular expressions at the right hand side) can be parsed with a simple modification of the LR-parser for normal CF grammars.







Probabilistic CFG With Latent Annotations

This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA. This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables. Finegrained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm. Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared. In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F1, sentences &lt; 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection.







Exploring Various Knowledge In Relation Extraction

Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.






Automatic Acquisition Of Adjectival Subcategorization From Corpora

This paper describes a novel system for acquiring adjectival subcategorization frames (scfs) and associated frequency information from English corpus data. The system incorporates a decision-tree classifier for 30 scf types which tests for the presence of grammatical relations (grs) in the output of a robust statistical parser. It uses a powerful pattern-matching language to classify grs into frames hierarchically in a way that mirrors inheritance-based lexica. The experiments show that the system is able to detect scf types with 70% precision and 66% recall rate. A new tool for linguistic annotation of scfs in corpus data is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition.






 Automatic recognition of French expletive pronoun occurrences

We present a tool, called ILIMP, which takes as input a raw text in French and produces as output the same text in which every occurrence of the pronoun il is tagged either with tag [ANA] for anaphoric or [IMP] for impersonal or expletive. This tool is therefore designed to distinguish between the anaphoric occurrences of il, for which an anaphora resolution system has to look for an antecedent, and the expletive occurrences of this pronoun, for which it does not make sense to look for an antecedent. The precision rate for ILIMP is 97,5%. The few errors are analyzed in detail. Other tasks using the method developed for ILIMP are described briefly, as well as the use of ILIMP in a modular syntactic analysis system. 





 A PROBLEM SOLVING APPROACH TO GENERATING TEXT FROM SYSTEMIC GRAMMARS

Systemic grammar has been used for AI text generation work in the past, but the implementations have tended be ad hoc or inefficient. This paper presents an approach to systemic text generation where AI problem solving techniques are applied directly to an unadulterated systemic grammar. This approach is made possible by a special relationship between systemic grammar and problem solving: both are organized primarily as choosing from alternatives. The result is simple, efficient text generation firmly based in a linguistic theory. 





User Studies And The Design Of Natural Language Systems

This paper presents a critical discussion of the various approaches that have been used in the evaluation of Natural Language systems. We conclude that previous approaches have neglected to evaluate systems in the context of their use, e.g. solving a task requiring data retrieval. This raises questions about the validity of such approaches. In the second half of the paper, we report a laboratory study using the Wizard of Oz technique to identify NL requirements for carrying out this task. We evaluate the demands that task dialogues collected using this technique, place upon a prototype Natural Language system. We identify three important requirements which arose from the task that we gave our subjects: operators specific to the task of database access, complex contextual reference and reference to the structure of the information source. We discuss how these might be satisfied by future Natural Language systems.







LFG Semantics Via Constraints


Semantic theories of natural language associate meanings with utterances by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts. Traditionally, meanings are combined via function composition, which works well when constituent structure trees are used to guide semantic composition. More recently, the functional structure of LFG has been used to provide the syntactic information necessary for constraining derivations of meaning in a cross-linguistically uniform format. It has been difficult, however, to reconcile this approach with the combination of meanings by function composition. 
In contrast to compositional approaches, we present a deductive approach to assembling meanings, based on reasoning with constraints, which meshes well with the unordered nature of information in the functional structure. Our use of linear logic as a 'glue' for assembling meanings also allows for a coherent treatment of modification as well as of the LFG requirements of completeness and coherence.





Splitting The Reference Time: Temporal Anaphora And Quantification In DRT


This paper presents an analysis of temporal anaphora in sentences which contain quantification over events, within the framework of Discourse Representation Theory. The analysis in (Partee, 1984) of quantified sentences, introduced by a temporal connective, gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after. This problem has been previously analyzed in (de Swart, 1991) as an instance of the proportion problem and given a solution from a Generalized Quantifier approach. By using a careful distinction between the different notions of reference time based on (Kamp and Reyle, 1993), we propose a solution to this problem, within the framework of DRT. We show some applications of this solution to additional temporal anaphora phenomena in quantified sentences.






A Proposal For SLS Evaluation

This paper proposes an automatic, essentially domain-independent means of evaluating Spoken Language Systems (SLS) which combines software we have developed for that purpose (the "Comparator") and a set of specifications for answer expressions (the "Common Answer Specification", or CAS). The Comparator checks whether the answer provided by a SLS accords with a canonical answer, returning either true or false. The Common Answer Specification determines the syntax of answer expressions, the minimal content that must be included in them, the data to be included in and excluded from test corpora, and the procedures used by the Comparator. Though some details of the CAS are particular to individual domains, the Comparator software is domain-independent, as is the CAS approach.





 Speech and Text-Image Processing in Documents

Two themes have evolved in speech and text image processing work at Xerox PARC that expand and redefine the role of recognition technology in document-oriented applications. One is the development of systems that provide functionality similar to that of text processors but operate directly on audio and scanned image data. A second, related theme is the use of speech and text-image recognition to retrieve arbitrary, user-specified information from documents with signal content. This paper discusses three research initiatives at PARC that exemplify these themes: a text-image editor[1], a wordspotter for voice editing and indexing[12], and a decoding framework for scanned-document content retrieval[4]. The discussion focuses on key concepts embodied in the research that enable novel signal-based document processing functionality.






A Statistical Profile Of The Named Entity Task

In this paper we present a statistical profile of the Named Entity task, a specific information extraction task for which corpora in several languages are available. Using the results of the statistical analysis, we propose an algorithm for lower bound estimation for Named Entity corpora and discuss the significance of the cross-lingual comparisons provided by the analysis.






Using Random Walks For Question-Focused Sentence Retrieval

We consider the problem of question-focused sentence retrieval from complex news articles describing multi-event stories published over time. Annotators generated a list of questions central to understanding each story in our corpus. Because of the dynamic nature of the stories, many questions are time-sensitive (e.g. "How many victims have been found?"). Judges found sentences providing an answer to each question. To address the sentence retrieval problem, we apply a stochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization. Currently, we present a topic-sensitive version of our method and hypothesize that it can outperform a competitive baseline, which compares the similarity of each sentence to the input question via IDF-weighted word overlap. In our experiments, the method achieves a TRDR score that is significantly higher than that of the baseline.






A Formal Model For Context-Free Languages Augmented With Reduplication

A model is presented to characterize the class of languages obtained by adding reduplication to context-free languages. The model is a pushdown automaton augmented with the ability to check reduplication by using the stack in a new way. The class of languages generated is shown to lie strictly between the context-free languages and the indexed languages. The model appears capable of accommodating the sort of reduplications that have been observed to occur in natural languages, but it excludes many of the unnatural constructions that other formal models have permitted.






Some remarks on the Annotation of Quantifying Noun Groups in Treebanks


This article is devoted to the problem of quantifying noun groups in German. After a thorough description of the phenomena, the results of corpus-based investigations are described. Moreover, some examples are given that underline the necessity of integrating some kind of information other than grammar sensu stricto into the treebank. We argue that a more sophisticated and fine-grained annotation in the tree-bank would have very positve effects on stochastic parsers trained on the tree-bank and on grammars induced from the treebank, and it would make the treebank more valuable as a source of data for theoretical linguistic investigations. The information gained from corpus research and the analyses that are proposed are realized in the framework of SILVA, a parsing and extraction tool for German text corpora.





 
Formal Constraints on Metarules 

Metagrammatical formalisms that combine context-free phrase structure rules and metarules (MPS grammars) allow concise statement of generalizations about the syntax of natural languages. Unconstrained MPS grammars, unfortunately, are not computationally safe. We evaluate several proposals for constraining them, basing our assessment on computational tractability and explanatory adequacy. We show that none of them satisfies both criteria, and suggest new directions for research on alternative metagrammatical formalisms. 




 
A CENTERING APPROACH TO PRONOUNS 

In this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns. As described in [GJW86], the process of centering attention on entities in the discourse gives rise to the intersentential transitional states of continuing, retaining and shifting. We propose an extension to these states which handles some additional cases of multiple ambiguous pronouns. The algorithm has been implemented in an HPSG natural language system which serves as the interface to a database query application. 




 
Compilation of HPSG to TAG 

We present an implemented compilation algorithm that translates HPSG into lexicalized feature-based TAG, relating concepts of the two theories. While HPSG has a more elaborated principle-based theory of possible phrase structures, TAG provides the means to represent lexicalized structures more explicitly. Our objectives are met by giving clear definitions that determine the projection of structures from the lexicon, and identify maximal projections, auxiliary trees and foot nodes. 




 
Fast Context-Free Parsing Requires Fast Boolean Matrix Multiplication 

Valiant showed that Boolean matrix multiplication (BMM) can be used for CFG parsing. We prove a dual result: CFG parsers running in time O(|G||w|3-e) on a grammar G and a string w can be used to multiply m x m Boolean matrices in time O(m3-e/3). In the process we also provide a formal definition of parsing motivated by an informal notion due to Lang. Our result establishes one of the first limitations on general CFG parsing: a fast, practical CFG parser would yield a fast, practical BMM algorithm, which is not believed to exist. 




 
Efficient Generation in Primitive Optimality Theory 

This paper introduces primitive Optimality Theory (OTP), a linguistically motivated formalization of OT. OTP specifies the class of autosegmental representations, the universal generator Gen, and the two simple families of permissible constraints. In contrast to less restricted theories using Generalized Alignment, OTP's optimal surface forms can be generated with finite-state methods adapted from (Ellison, 1994). Unfortunately these methods take time exponential on the size of the grammar. Indeed the generation problem is shown NP-complete in this sense. However, techniques are discussed for making Ellison's approach fast in the typical case, including a simple trick that alone provides a 100-fold speedup on a grammar fragment of moderate size. One avenue for future improvements is a new finite-state notion, factored automata, where regular languages are represented compactly via formal intersections of FSAs. 




 
Towards resolution of bridging descriptions 

We present preliminary results concerning robust techniques for resolving bridging definite descriptions. We report our analysis of a collection of 20 Wall Street Journal articles from the Penn Treebank Corpus and our experiments with WordNet to identify relations between bridging descriptions and their antecedents. 




 
A semantically-derived subset of English for hardware verification 

To verify hardware designs by model checking, circuit specifications are commonly expressed in the temporal logic CTL. Automatic conversion of English to CTL requires the definition of an appropriately restricted subset of English. We show how the limited semantic expressibility of CTL can be exploited to derive a hierarchy of subsets. Our strategy avoids potential difficulties with approaches that take existing computational semantic analyses of English as their starting point--such as the need to ensure that all sentences in the subset possess a CTL translation. 






What To Do When Lexicalization Fails: Parsing German With Suffix Analysis And Smoothing 
In this paper, we present an unlexicalized parser for German which employs smoothing and suffix analysis to achieve a labelled bracket F-score of 76.2, higher than previously reported results on the NEGRA corpus. In addition to the high accuracy of the model, the use of smoothing in an unlexicalized parser allows us to better examine the interplay between smoothing and parsing results.







 Alignment Model Adaptation For Domain-Specific Word Alignment 
This paper proposes an alignment adaptation approach to improve domain-specific (in-domain) word alignment. The basic idea of alignment adaptation is to use out-of-domain corpus to improve in-domain word alignment results. In this paper, we first train two statistical word alignment models with the large-scale out-of-domain corpus and the small-scale in-domain corpus respectively, and then interpolate these two models to improve the domain-specific word alignment. Experimental results show that our approach improves domain-specific word alignment in terms of both precision and recall, achieving a relative error rate reduction of 6.56% as compared with the state-of-the-art technologies.








An Information-State Approach To Collaborative Reference 
We describe a dialogue system that works with its interlocutor to identify objects. Our contributions include a concise, modular architecture with reversible processes of understanding and generation, an information-state model of reference, and flexible links between semantics and collaborative problem solving.






 
AN APPROACH TO NATURAL LANGUAGE IN THE SI-NETS PARADIGM 

This article deals with the interpretation of conceptual operations underlying the communicative use of natural language (NL) within the Structured Inheritance Network (SI-Nets) paradigm. The operations are reduced to functions of a formal language, thus changing the level of abstraction of the operations to be performed on SI-Nets. In this sense, operations on SI-Nets are not merely isomorphic to single epistemological objects, but can be viewed as a simulation of processes on a different level, that pertaining to the conceptual system of NL. For this purpose, we have designed a version of KL-ONE which represents the epistemological level, while the new experimental language, KL-Conc, represents the conceptual level. KL-Conc would seem to be a more natural and intuitive way of interacting with SI-Nets. 






 Iteration, Habituality And Verb Form Semantics 
The verb forms are often claimed to convey two kinds of information : 1. whether the event described in a sentence is present, past or future (= deictic information) 2. whether the event described in a sentence is presented as completed, going on, just starting or being finished (= aspectual information). It will be demonstrated in this paper that one has to add a third component to the analysis of verb form meanings, namely whether or not they express habituality. The framework of the analysis is model-theoretic semantics. 







A Language For The Statement Of Binary Relations Over Feature Structures 
Unification is often the appropriate method for expressing relations between representations in the form of feature structures; however, there are circumstances in which a different approach is desirable. A declarative formalism is presented which permits direct mappings of one feature structure into another, and illustrative examples are given of its application to areas of current interest.






 
A Discourse Copying Algorithm for Ellipsis and Anaphora Resolution 

 We give an analysis of ellipsis resolution in terms of a straightforward discourse copying algorithm that correctly predicts a wide range of phenomena. The treatment does not suffer from problems inherent in identity-of-relations analyses. Furthermore, in contrast to the approach of Dalrymple et al. [1991], the treatment directly encodes the intuitive distinction between full NPs and the referential elements that corefer with them through what we term role linking. The correct predictions for several problematic examples of ellipsis naturally result. Finally, the analysis extends directly to other discourse copying phenomena. 






Representing Text Chunks 
Dividing sentences in chunks of words is a useful preprocessing step for parsing, information extraction and information retrieval. (Ramshaw and Marcus, 1995) have introduced a "convenient" data representation for chunking by converting it to a tagging task. In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks. We will show that the data representation choice has a minor influence on chunking performance. However, equipped with the most suitabledata representation, our memory-based learning chunker was able to improve the best published chunking results for a standard data set.







Tagging French - Comparing A Statistical And A Constraint-Based Method 
In this paper we compare two competing approaches to part-of-speech tagging, statistical and constraint-based disambiguation, using French as our test language. We imposed a time limit on our experiment: the amount of time spent on the design of our constraint system was about the same as the time we used to train and test the easy-to-implement statistical model. We describe the two systems and compare the results. The accuracy of the statistical method is reasonably good, comparable to taggers for English. But the constraint-based tagger seems to be superior even with the limited time we allowed ourselves for rule development.








An Annotation Scheme For Discourse-Level Argumentation In Research Articles
In order to build robust automatic abstracting systems, there is a need for better training resources than are currently available. In this paper, we introduce an annotation scheme for scientific articles which can be used to build such a resource in a consistent way. The seven categories of the scheme are based on rhetorical moves of argumentation. Our experimental results show that the scheme is stable, reproducible and intuitive to use.







 Automatic Acquisition Of Subcategorization Frames From Tagged Text 
This paper describes an implemented program that takes a tagged text corpus and generates a partial list of the subcategorization frames in which each verb occurs. The completeness of the output list increases monotonically with the total occurrences of each verb in the training corpus. False positive rates are one to three percent. Five subcategorization frames are currently detected and we foresee no impediment to detecting many more. Ultimately, we expect to provide a large subcategorization dictionary to the NLP community and to train dictionaries for specific corpora.






 
Large-Scale Acquisition of LCS-Based Lexicons for Foreign Language Tutoring 

We focus on the problem of building large repositories of lexical conceptual structure (LCS) representations for verbs in multiple languages. One of the main results of this work is the definition of a relation between broad semantic classes and LCS meaning components. Our acquisition program - LEXICALL - takes, as input, the result of previous work on verb classification and thematic grid tagging, and outputs LCS representations for different languages. These representations have been ported into English, Arabic and Spanish lexicons, each containing approximately 9000 verbs. We are currently using these lexicons in an operational foreign language tutoring and machine translation. 








Semi-Automatic Acquisition Of Domain-Specific Translation Lexicons 
We investigate the utility of an algorithm for translation lexicon acquisition (SABLE), used previously on a very large corpus to acquire general translation lexicons, when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons.






 
 SIMULTANEOUS-DISTRIBUTIVE COORDINATION AND CONTEXT-FREENESS 

English is shown to be trans-context-free on the basis of coordinations of the respectively type that involve strictly syntactic cross-serial agreement. The agreement in question involves number in nouns and reflexive pronouns and is syntactic rather than semantic in nature because grammatical number in English, like grammatical gender in languages such as French, is partly arbitrary. The formal proof, which makes crucial use of the Interchange Lemma of Ogden et al., is so constructed as to be valid even if English is presumed to contain grammatical sentences in which respectively operates across a pair of coordinate phrases one of whose members has fewer conjuncts than the other; it thus goes through whatever the facts may be regarding constructions with unequal numbers of conjuncts in the scope of respectively, whereas other arguments have foundered on this problem. 






A Class-oriented Approach to Building a Paraphrase Corpus 
Towards deep analysis of compositional classes of paraphrases, we have examined a class-oriented framework for collecting paraphrase examples, in which sentential paraphrases are collected for each paraphrase class separately by means of automatic candidate generation and manual judgement. Our preliminary experiments on building a paraphrase corpus have so far been producing promising results, which we have evaluated according to cost-efficiency, exhaustiveness, and reliability.






 
A Construction-Specific Approach to Focused Interaction in Flexible Parsing  

A flexible parser can deal with input that deviates from its grammar, in addition to input that conforms to it. Ideally, such a parser will correct the deviant input: sometimes, it will be unable to correct it at all; at other times, correction will be possible, but only to within a range of ambiguous possibilities. This paper is concerned with such ambiguous situations, and with making it as easy as possible for the ambiguity to be resolved through consultation with the user of the parser - we presume interactive use. We show the importance of asking the user for clarification in as focused a way as possible. Focused interaction of this kind is facilitated by a construction-specific approach to flexible parsing, with specialized parsing techniques for each type of construction, and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to. A construction-specific approach also aids in task-specific language development by allowing a language definition that is natural in terms of the task domain to be interpreted directly without compilation into a uniform grammar formalism, thus greatly speeding the testing of changes to the language definition. 





 
 Semantic Caseframe Parsing and Syntactic Generality  

We have implemented a restricted domain parser called Plume. Building on previous work at Carnegie-Mellon University e.g. [4, 5, 8], Plume's approach to parsing is based on semantic caseframe instantiation. This has the advantages of efficiency on grammatical input, and robustness in the face of ungrammatical input. While Plume is well adapted to simple declarative and imperative utterances, it handles passives, relative clauses and interrogatives in an ad hoc manner leading to patchy syntactic coverage. This paper outlines Plume as it currently exists and describes our detailed design for extending Plume to handle passives, relative clauses, and interrogatives in a general manner. 





 

Resolving Translation Mismatches With Information Flow  

Languages differ in the concepts and real-world entities for which they have words and grammatical constructs. Therefore translation must sometimes be a matter of approximating the meaning of a source language text rather than finding an exact counterpart in the target language. We propose a translation framework based on Situation Theory. The basic ingredients are an information lattice, a representation scheme for utterances embedded in contexts, and a mismatch resolution scheme defined in terms of information flow. We motivate our approach with examples of translation between English and Japanese. 





 
 Two-Level, Many-Paths Generation  

Large-scale natural language generation requires the integration of vast amounts of knowledge: lexical, grammatical, and conceptual. A robust generator must be able to operate well even when pieces of knowledge are missing. It must also be robust against incomplete or inaccurate inputs. To attack these problems, we have built a hybrid generator, in which gaps in symbolic knowledge are filled by statistical methods. We describe algorithms and show experimental results. We also discuss how the hybrid generation model can be used to simplify current generators and enhance their portability, even when perfect knowledge is in principle obtainable.





 
 Machine Transliteration  

It is challenging to translate names and technical terms across languages with different alphabets and sound inventories. These items are commonly transliterated, i.e., replaced with approximate phonetic equivalents. For example, computer in English comes out as ~ i/l:::'=--~-- (konpyuutaa) in Japanese. Translating such items from Japanese back to English is even more challenging, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries. We describe and evaluate a method for performing backwards transliterations by machine. This method uses a generative model, incorporating several distinct stages in the transliteration process. 





 
Approximating Context-Free Grammars with a Finite-State Calculus  

Although adequate models of human language for syntactic analysis and semantic interpretation are of at least context-free complexity, for applications such as speech processing in which speed is important finite-state models are often preferred. These requirements may be reconciled by using the more complex grammar to automatically derive a finite-state approximation which can then be used as a filter to guide speech recognition or to reject many hypotheses at an early stage of processing. A method is presented here for calculating such finite-state approximations from context-free grammars. It is essentially different from the algorithm introduced by Pereira and Wright (1991; 1996), is faster in some cases, and has the advantage of being open-ended and adaptable.






 

A Part of Speech Estimation Method for Japanese Unknown Words using a Statistical Model of Morphology and Context  

We present a statistical model of Japanese unknown words consisting of a set of length and spelling models classified by the character types that constitute a word. The point is quite simple: different character sets should be treated differently and the changes between character types are very important because Japanese script has both ideograms like Chinese (kanji) and phonograms like English (katakana). Both word segmentation accuracy and part of speech tagging accuracy are improved by the proposed model. The model can achieve 96.6% tagging accuracy if unknown words are correctly segmented. 





 

A Pylonic Decision-Tree Language Model with Optimal Question Selection  

This paper discusses a decision-tree approach to the problem of assigning probabilities to words following a given text. In contrast with previous decision-tree language model attempts, an algorithm for selecting nearly optimal questions is considered. The model is to be tested on a standard task, The Wall Street Journal, allowing a fair comparison with the well-known tri-gram model. 






 Two-Level Description Of Turkish Morphology 

This poster paper describes a full scale two-level morphological description (Karttunen, 1983; Koskenniemi, 1983) of Turkish word structures. The description has been implemented using the PC-KIMMO environment (Antworth, 1990) and is based on a root word lexicon of about 23,000 roots words. Almost all the special cases of and exceptions to phonological and morphological rules have been implemented. Turkish is an agglutinative language with word structures formed by productive affixations of derivational and inflectional suffixes to root words. Turkish has finite-state but nevertheless rather complex morphotactics. Morphemes added to a root word or a stem can convert the word from a nominal to a verbal structure or vice-versa, or can create adverbial constructs. The surface realizations of morphological constructions are constrained and modified by a number of phonetic rules such as vowel harmony.







 TUIT : A Toolkit For Constructing Multilingual TIPSTER User Interfaces 

The TIPSTER Architecture has been designed to enable a variety of different text applications to use a set of common text processing modules. Since user interfaces work best when customized for particular applications , it is appropriator that no particular user interface styles or conventions are described in the TIPSTER Architecture specification. However, the Computing Research Laboratory (CRL) has constructed several TIPSTER applications that use a common set of configurable Graphical User Interface (GUI) functions. These GUIs were constructed using CRL's TIPSTER User Interface Toolkit (TUIT). TUIT is a software library that can be used to construct multilingual TIPSTER user interfaces for a set of common user tasks. CRL developed TUIT to support their work to integrate TIPSTER modules for the 6 and 12 month TIPSTER II demonstrations as well as their Oleada and Temple demonstration projects. This paper briefly describes TUIT and its capabilities. 







 Phonological Comprehension And The Compilation Of Optimality Theory 

This paper ties up some loose ends in finite-state Optimality Theory. First, it discusses how to perform comprehension under Optimality Theory grammars consisting of finite-state constraints. Comprehension has not been much studied in OT; we show that unlike production, it does not always yield a regular set, making finite-state methods inapplicable. However, after giving a suitably flexible presentation of OT, we show carefully how to treat comprehension under recent variants of OTin which grammars can be compiled into finite-state transducers. We then unify these variants, showing that compilation is possible if all components of the grammar are regular relations, including the harmony ordering on scored candidates. 







Learning Correlations between Linguistic Indicators and Semantic Constraints : Reuse of Context-Dependent Descriptions of Entities 

This paper presents the results of a study on the semantic constraints imposed on lexical choice by certain contextual indicators. We show how such indicators are computed and how correlations between them and the choice of a noun phrase description of a named entity can be automatically established using supervised learning. Based on this correlation, we have developed a technique for automatic lexical choice of descriptions of entities in text generation. We discuss the underlying relationship between the pragmatics of choosing an appropriate description that serves a specific purpose in the automatically generated text and the semantics of the description itself. We present our work in the framework of the more general concept of reuse of linguistic structures that are automatically extracted from large corpora. We present a formal evaluation of our approach and we conclude with some thoughts on potential applications of our method.







 Evaluation In The ARPA Machine Translation Program : 1993 Methodology

In the second year of evaluations of the ARPA HLT Machine Translation (MT) Initiative, methodologies developed and tested in 1992 were applied to the 1993 MT test runs. The current methodology optimizes the inherently subjective judgments on translation accuracy and quality by channeling the judgments of non-translators into many data points which reflect both the comparison of the performance of the research MT systems with production MT systems and against the performance of novice translators. This paper discusses the three evaluation methods used in the 1993 evaluation, the results of the evaluations , and preliminary characterizations of the  Winter 1994 evaluation, now underway. The efforts under discussion focus on measuring the progress of core MT technology and increasing the sensitivity and portability of MT evaluation methodology.







Integrating Shallow Linguistic Processing Into A Unification-Based Spanish Grammar 

This paper describes to what extent deep processing may benefit from shallow techniques and it presents a NLP system which integrates a linguistic PoS tagger and chunker as a preprocessing module of a broad coverage unification based grammar of Spanish. Experiments show that the efficiency of the overall analysis improves significantly and that our system also provides robustness to the linguistic processing while maintaining both the accuracy and the precision of the grammar.







 Parsing And Subcategorization Data 

In this paper, we compare the performance of a state-of-the-art statistical parser (Bikel, 2004) in parsing written and spoken language and in generating sub-categorization cues from written and spoken language. Although Bikel's parser achieves a higher accuracy for parsing written language, it achieves a higher accuracy when extracting subcategorization cues from spoken language. Our experiments also show that current technology for extracting subcategorization frames initially designed for written texts works equally well for spoken language. Additionally, we explore the utility of punctuation in helping parsing and extraction of subcategorization cues. Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language. This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers.







 Character-Based Collocation For Mandarin Chinese 

This paper describes a characters-based Chinese collocation system and discusses the advantages of it over a traditional word-based system. Since wordbreaks are not conventionally marked in Chinese text corpora, a character-based collocation system has the dual advantages of avoiding pre-processing distortion and directly accessing sub-lexical information. Furthermore, word-based collocational properties can be obtained through an auxiliary module of automatic segmentation.







Efficient Parsing Of Highly Ambiguous Context-Free Grammars With Bit Vectors 

An efficient bit-vector-based CKY-style parser for context-free parsing is presented. The parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences. The parser uses bit-vector operations to parallelise the basic parsing operations. The parser is particularly useful when all analyses are needed rather than just the most probable one.







 Automatic Question Answering : Beyond The Factoid

In this paper we describe and evaluate a Question Answering system that goes beyond answering factoid questions. We focus on FAQ-like questions and answers , and build our system around a noisy-channel architecture which exploits both a language model for answers and a transformation model for answer/question terms, trained on a corpus of 1 million question/answer pairs collected from the Web.








Unsupervised Learning Of Field Segmentation Models For Information Extraction 
The applicability of many current information extraction techniques is severely limited by the need for supervised training data. 
We demonstrate that for certain field structured extraction tasks, such as classified advertisements and bibliographic citations, small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion. Although hidden Markov models (HMMs) provide a suitable generative model for field structured text, general unsupervised HMM learning fails to learn useful structure in either of our domains. However, one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions. In both domains, we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.








Joint Learning Improves Semantic Role Labeling 
Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding. This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between arguments. We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative log-linear models. This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a state-of-the art independent classifier for gold-standard parse trees on PropBank.








Organizing English Reading Materials For Vocabulary Learning 
We propose a method of organizing reading materials for vocabulary learning. It enables us to select a concise set of reading texts (from a target corpus) that contains all the target vocabulary to be learned. We used a specialized vocabulary for an English certification test as the target vocabulary and used English Wikipedia, a free-content encyclopedia, as the target corpus. The organized reading materials would enable learners not only to study the target vocabulary efficiently but also to gain a variety of knowledge through reading. The reading materials are available on our web site.






 
 NATURAL LANGUAGE INPUT FOR SCENE GENERATION 

In this paper a system which understands and conceptualizes scenes descriptions in natural language is presented. Specifically, the following components of the system are described: the syntactic analyzer, based on a Procedural Systemic Grammar, the semantic analyzer relying on the Conceptual Dependency Theory, and the dictionary. 





 
 TENSES AS ANAPHORA 

A proposal to deal with French tenses in the framework of Discourse Representation Theory is presented, as it has been implemented for a fragment at the IMS. It is based on the theory of tenses of H. Kamp and Ch. Rohrer. Instead of using operators to express the meaning of the tenses the Reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the meaning of the text is understood as contribution to the integration of the events of a sentence in the event structure of the preceeding text. Thereby a system of relevant times provided by the preceeding text and by the temporal adverbials of the sentence being processed is used. This system consists of one or more reference times and temporal perspective times, the speech time and the location time. The special interest of our proposal is to establish a plausible choice of anchors for the new event out of the system of relevant times and to update this system of temporal coordinates correctly. The problem of choice is largely neglected in the literature. In opposition to the approach of Kamp and Rohrer the exact meaning of the tenses is fixed by the resolution component and not in the process of syntactic analysis. 







Talking About Trees 
In this paper we introduce a modal language LTfor imposing constraints on trees, and an extension LT (LF) for imposing constraints on trees decorated with feature structures. The motivation for introducing these languages is to provide tools for formalising grammatical frameworks perspicuously, and the paper illustrates this by showing how the leading ideas of GPSG can be captured in LT (LF). In addition, the role of modal languages (and in particular, what we have called as constraint formalisms for linguistic theorising is discussed in some detail.






 
Parsing with an Extended Domain of Locality 

One of the claimed benefits of Tree Adjoining Grammars is that they have an extended domain of locality (EDOL). We consider how this can be exploited to limit the need for feature structure unification during parsing. We compare two wide-coverage lexicalized grammars of English, LEXSYS and XTAG, finding that the two grammars exploit EDOL in different ways. 







ParseTalk About Sentence- And Text-Level Anaphora
We provide a unified account of sentence-level and text-level anaphora within the framework of a dependency-based grammar model. Criteria for anaphora resolution within sentence boundaries rephrase major concepts from GB's binding theory, while those for text-level anaphora incorporate an adapted version of a Grosz-Sidner-style focus model.








The MIT Summit Speech Recognition System : A Progress Report
Recently, we initiated a project to develop a phonetically-based spoken language understanding system called SUMMIT. In contrast to many of the past efforts that make use of heuristic rules whose development requires intense knowledge engineering, our approach attempts to express the speech knowledge within a formal framework using well-defined mathematical tools. In our system, features and decision strategies are discovered and trained automatically, using a large body of speech data. This paper describes the system, and documents its current performance.








A Proposal For Lexical Disambiguation 
A method of sense resolution is proposed that is based on WordNet, an on-line lexical database that incorporates semantic relations (synonymy, antonymy, hyponymy, meronymy, causal and troponymic entailment) as labeled pointers between word senses. With WordNet, it is easy to retrieve sets of semantically related words, a facility that will be used for sense resolution during text processing, as follows. When a word with multiple senses is encountered, one of two procedures will be followed. Either, (1) words related in meaning to the alternative senses of the polysemous word will be retrieved; new strings will be derived by substituting these related words into the context of the polysemous word; a large textual corpus will then be searched for these derived strings; and that sense will be chosen that corresponds to the derived string that is found most often in the corpus. Or, (2) the context of the polysemous word will be used as a key to search a large corpus; all words found to occur in that context will be noted; WordNet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful, this procedure could have practical applications to problems of information retrieval, mechanical translation, intelligent tutoring systems, and elsewhere.








Dutch Sublanguage Semantic Tagging Combined With Mark-Up Technology 
In this paper, we want to show how the morphological component of an existing NLP-system for Dutch (Dutch Medical Language Processor - DMLP) has been extended in order to produce output that is compatible with the language independent modules of the LSP-MLP system (Linguistic String Project - Medical Language Processor) of the New York University. The former can take advantage of the language independent developments of the latter, while focusing on idiosyncrasies for Dutch. This general strategy will be illustrated by a practical application, namely the highlighting of relevant information in a patient discharge summary (PDS) by means of modern HyperText Mark-Up Language (HTML) technology. Such an application can be of use for medical administrative purposes in a hospital environment.







 Automatic Extraction Of Subcategorization From Corpora 

We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount





 
PROCESSING DICTIONARY DEFINITIONS WITH PHRASAL PATTERN HIERARCHIES 

This paper shows how dictionary word sense definitions can be analysed by applying a hierarchy of phrasal patterns. An experimental system embodying this mechanism has been implemented for processing definitions from the Longman Dictionary of Contemporary English. A property of this dictionary, exploited by the system, is that it uses a restricted vocabulary in its word sense definitions. The structures generated by the experimental system are intended to be used for the classification of new word senses in terms of the senses of words in the restricted vocabulary. Examples illustrating the output generated are presented, and some qualitative performance results and problems that were encountered are discussed. The analysis process applies successively more specific phrasal analysis rules as determined by a hierarchy of patterns in which less specific patterns dominate more specific ones. This ensures that reasonable incomplete analyses of the definitions are produced when more complete analyses are not possible, resulting in a relatively robust analysis mechanism. Thus the work reported addresses two robustness problems faced by current experimental natural language processing systems: coping with an incomplete lexicon and with incomplete knowledge of phrasal constructions. 







Evaluating Contextual Dependency of Paraphrases using a Latent Variable Model 
This paper presents an evaluation method employing a latent variable model for paraphrases with their contexts. We assume that the context of a sentence is indicated by a latent variable of the model as a topic and that the likelihood of each variable can be inferred. A paraphrase is evaluated for whether its sentences are used in the same context. Experimental results showed that the proposed method achieves almost 60% accuracy and that there is not a large performance difference between the two models. The results also revealed an upper bound of accuracy of 77% with the method when using only topic information.






 
 Crossed Serial Dependencies : A low-power parseable extension to GPSG  

An extension to the GPSG grammatical formalism is proposed, allowing non-terminals to consist of finite sequences of category labels, and allowing schematic variables to range over such sequences. The extension is shown to be sufficient to provide a strongly adequate grammar for crossed serial dependencies, as found in e.g. Dutch subordinate clauses. The structures induced for such constructions are argued to be more appropriate to data involving conjunction than some previous proposals have been. The extension is shown to be parseable by a simple extension to an existing parsing method for GPSG. 












A LAF/GrAF based Encoding Scheme for underspecified Representations of syntactic Annotations.
Data models and encoding formats for syntactically annotated text corpora need to deal with syntactic ambiguity; underspecified representations are particularly well suited for the representation of ambiguousdata because they allow for high informational efficiency. We discuss the issue of being informationally efficient, and the trade-off between efficient encoding of linguistic annotations and complete documentation of linguistic analyses. The main topic of this article is adata model and an encoding scheme based on LAF/GrAF ( Ide and  Romary, 2006 ; Ide and  Suderman, 2007 ) which provides a flexible framework for encoding underspecified representations. We show how a set of dependency structures and a set of TiGer graphs ( Brants et al., 2002 ) representing the readings of an ambiguous sentence can be encoded, and we discuss basic issues in querying corpora which are encoded using the framework presented here.






A Study of Parentheticals in Discourse Corpora - Implications for NLG Systems
This paper presents a corpus study of parenthetical constructions in two different corpora: the Penn Discourse Treebank (PDTB, (PDTB- Group, 2008 )) and the RST Discourse Treebank ( Carlson et al., 2001 ). The motivation for the study is to gain a better understanding of the rhetorical properties of parentheticals in order to enable a natural language generation system to produce parentheticals as part of a rhetorically well-formed output. We argue that there is a correlation between syntactic and rhetorical types of parentheticals and establish two main categories: elaboration/expansion-type NP-modifier parentheticals and non-elaboration/expansion-type VP- or S-modifier parentheticals. We show several strategies for extracting these from the two corpora and discuss how the seemingly contradictory results obtained can be reconciled in light of the rhetorical and syntactic properties of parentheticals as well as the decisions taken in the annotation guidelines.






Machine Learning Approach to Augmenting News Headline Generation
In this paper, we present the HybridTrim system which uses a machine learning technique to combine linguistic, statistical and positional information to identify topic labels for headlines in a text. We compare our system with the Topiary system which, in contrast, uses a statistical learning approach to finding topic descriptors for headlines. The Topiary system, developed at the University of Maryland with BBN, was the top performing headline generation system at DUC 2004. Topiary-style headlines consist of a number of general topic labels followed by a compressed version of the lead sentence of a news story. The Topiary system uses a statistical learning approach to finding topic labels. The performance of these systems is evaluated using the ROUGE evaluation suite on the DUC 2004 news stories collection.






Inducing History Representations For Broad Coverage Statistical Parsing
We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser. The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge. Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions.





Precision And Recall Of Machine Translation
Machine translation can be evaluated using precision, recall, and the F-measure. These standard measures have significantly higher correlation with human judgments than recently proposed alternatives. More importantly, the standard measures have an intuitive interpretation, which can facilitate insights into how MT systems might be improved. The relevant software is publicly available.





Learning Morphological Disambiguation Rules For Turkish

In this paper, we present a rule based model for morphological disambiguation of Turkish. The rules are generated by a novel decision list learning algorithm using supervised training. Morphological ambiguity (e.g. lives = live+s or life+s) is a challenging problem for agglutinative languages like Turkish where close to half of the words in running text are morphologically ambiguous. Furthermore, it is possible for a word to take an unlimited number of suffixes, therefore the number of possible morphological tags is unlimited. We attempted to cope with these problems by training a separate model for each of the 126 morphological features recognized by the morphological analyzer. The resulting decision lists independently vote on each of the potential parses of a word and the final parse is selected based on our confidence on these votes. The accuracy of our model (96%) is slightly above the best previously reported results which use statistical models. For comparison, when we train a single decision list on full tags instead of using separate models on each feature we get 91% accuracy.






SRA SOLOMON: MUC-4 Test Results And Analysis
In this paper, we report SRA's results on the MUC-4 task and describe how we trained our natural language processing system for MUC-4. We also report on what worked, what didn't work, and lessons learned. Our MUC-4 system embeds the SOLOMON knowledge-based NLP shell which is designed for both and We are currently using SOLOMON for a Spanish and Japanese text understanding project in a different domain. Although this was our first year participating in MUC, we have built and are currently building otherdata extraction systems.





Corpus-Based Method For Automatic Identification Of Support Verbs For Nominalizations

Nominalization is a highly productive phenomena in most languages. The process of nominalization ejects a verb from its syntactic role into a nominal position. The original verb is often replaced by a semantically emptied support verb (e.g., make a proposal).






Aggressive Morphology For Robust Lexical Coverage
This paper describes an approach to providing lexical information for natural language processing in unrestricted domains. A system of approximately 1200 morphological rules is used to extend a core lexicon of 39,000 words to provide lexical coverage that exceeds that of a lexicon of 80,000 words or 150,000 word forms. The morphological system is described, and lexical coverage is evaluated for random words chosen from a previously unanalyzed corpus.






Predicting Automatic Speech Recognition Performance Using Prosodic Cues

In spoken dialogue systems, it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can reprompt for fresh input, or, in cases where many errors have occurred, change its interaction strategy or switch the caller to a human attendant. We have discovered prosodie features which more accurately predict when a recognition hypothesis contains a word error than the acoustic confidence score thresholds traditionally used in automatic speech recognition. We present analytic results indicating that there are significant prosodie differences between correctly and incorrectly recognized turns.





Evaluating The Use Of Prosodic Information In Speech Recognition And Understanding
The goal of this project is to investigate the use of different levels of prosodie information in speech recognition and understanding. In particular, the current focus of the work is the use of prosodie phrase boundary information in parsing. The research involves determining a representation of prosodie information suitable for use in a speech understanding system, developing reliable algorithms for detection of the prosodie cues in speech, investigating architectures for integrating prosodie cues in a parser, and evaluating the potential improvements of prosody in the context of the SRI Spoken Language System. This research is sponsored jointly by DARPA and NSF.





Prediction Of Lexicalized Tree Fragments In Text
There is a mismatch between the distribution of information in text, and a variety of grammatical formalisms for describing it, including ngrams, context-free grammars, and dependency grammars. Rather than adding probabilities to existing grammars, it is proposed to collect the distributions of flexibly sized partial trees. These can be used to enhance an ngram model, and in analogical parsing.






The COMLEX Syntax Project
"Developing more shareable resources to support natural language analysis will make it easier and cheaper to create new language processing applications and to support research in computational linguistics. One natural candidate for such a resource is a broad-coverage dictionary, since the work required to create such a dictionary is large but there is general agreement on at least some of the information to be recorded for each word. The Linguistic Data Consortium has begun an effort to create several such lexical resources, under the rubric ""COMLEX"" (COMmon LEXicon); one of these projects is the COMLEX Syntax Project. The goal of the COMLEX Syntax Project is to create a moderately-broad-coverage shareable dictionary containing the syntactic features of English words, intended for automatic language analysis. We are initially aiming for a dictionary of 35,000 to 40,000 base forms, although this of course may be enlarged if the initial effort is positively received. The dictionary should include detailed syntactic specifications, particularly for subcategorization; our intent is to provide sufficient detail so that the information required by a number of major English analyzers can be automatically derived from the information we provide. As with other Linguistic Data Consortium resources, our intent is to provide a lexicon available without license constraint to all Consortium members. Finally, our goal is to provide an initial lexicon relatively quickly 
 within about a year, funding permitting. This implies a certain flexibility, where some of the features will probably be changed and refined as the coding is taking place. "






An Evaluation Of Strategies For Selective Utterance Verification For Spoken Natural Language Dialog
As with human-human interaction, spoken human-computer dialog will contain situations where there is miscommunication. In experimental trials consisting of eight different users, 141 problem-solving dialogs, and 2840 user utterances, the Circuit Fix-It Shop natural language dialog system misinterpreted 18.5% of user utterances. These miscommunications created various problems for the dialog interaction, ranging from repetitive dialog to experimenter intervention to occasional failure of the dialog. One natural strategy for reducing the impact of miscommunication is selective verification of the user's utterances. This paper reports on both context-independent and context-dependent strategies for utterance verification that show that the use of dialog context is crucial for intelligent selection of which utterances to verify.





Computing Prosodic Properties In A Data-To-Speech System
We propose a set of rules for the computation of prosody which are implemented in an existing generic Data-to-Speech system. The rules make crucial use of both sentence-internal and sentence-external semantic and syntactic information provided by the system. In a Text-to-Speech system, this information would have to be obtained through text analysis, but in Data-to-Speech it is readily available, and its reliable and detailed character makes it possible to compute the prosodie properties of generated sentences in a sophisticated way. This in turn allows for a close control of prosodie realization, resulting in natural-sounding intonation.






Standardisation Efforts On The Level Of Dialogue Act In The MATE Project
This paper describes the state of the art of coding schemes for dialogue acts and the efforts to establish a standard in this field. We present a review and comparison of currently available schemes and outline the comparison problems we had due to domain, task, and language dependencies of schemes. We discuss solution strategies which have in mind the reusability of corpora. Reusability is a crucial point because production and annotation of corpora is very time and cost consuming but the current broad variety of schemes makes reusability of annotated corpora very hard. The work of this paper takes place in the framework of the European Union funded MATE project. MATE aims to develop general methodological guidelines for the creation, annotation, retrieval and analysis of annotated corpora.






ADAM - An Architecture For XML-Based Dialogue Annotation On Multiple Levels

In this paper annotation modularity and use of annotation meta-schemes are identified as basic requirements for achieving actual corpora reusability. We discuss these concepts and the way they are implemented in the architectural framework of the ADAM corpus, which is a corpus of 450 Italian spontaneous dialogues. The design of ADAM architecture is compatible with as many practices of dialogue annotation as possible, as well as approaches to annotation at different levels.






Annotating Information Structures In Chinese Texts Using HowNet

This paper reported our work on annotating Chinese texts with information structures derived from HowNet. An information structure consists of two components: HowNet definitions and dependency relations. It is the unit of representation of the meaning of texts. This work is part of a multi-sentential approach to Chinese text understanding. An overview of HowNet and information structure are described in this paper.





Training A Dialogue Act Tagger For Human-Human And Human-Computer Travel Dialogues

While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues, their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme. In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues. We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE (Dialogue Act Tagging for Evaluation) dialogue act tags. We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora, and the CMU human-human corpus in the travel planning domain. Our results show that we can achieve high accuracies on the humancomputer data, and surprisingly, that the human-computerdata improves accuracy on the human-human data, when only small amounts of human-human trainingdata are available.





Machine Translation As A Testbed For Multilingual Analysis
We propose that machine translation (MT) is a useful application for evaluating and deriving the development of NL components, especially in a wide-coverage analysis system. Given the architecture of our MT system, which is a transfer system based on linguistic modules, correct analysis is expected to be a prerequisite for correct translation, suggesting a correlation between the two, given relatively mature transfer and generation components. We show through error analysis that there is indeed a strong correlation between the quality of the translated output and the subjectively determined goodness of the analysis. We use this correlation as a guide for development of a coordinated parallel analysis effort in 7 languages.






A Simple Named Entity Extractor Using AdaBoost

This paper presents a Named Entity Extraction (NEE) system for the CoNLL-2003 shared task competition. As in the past year edition ( Carreras et al., 2002a ), we have approached the task by treating the two main sub-tasks of the problem, recognition (NER) and classification (NEC), sequentially and independently with separate modules. Both modules are machine learning based systems, which make use of binary and multiclass AdaBoost classifiers. Named Entity recognition is performed as a greedy sequence tagging procedure under the well-known BIO labelling scheme. This tagging process makes use of three binary classifiers trained to be experts





WHAT: An XSLT-Based Infrastructure For The Integration Of Natural Language Processing Components

The idea of the Whiteboard project is to integrate deep and shallow natural language processing components in order to benefit from their synergy. The project came up with the first fully integrated hybrid system consisting of a fast HPSG parser that utilizes tokenization, PoS, morphology, lexical, named entity, phrase chunk and (for German) topological sentence field analyses from shallow components. This integration increases robustness, directs the search space and hence reduces processing time of the deep parser. In this paper, we focus on one of the central integration facilities, the XSLT-based Whiteboard Annotation Transformer (WHAT), report on the benefits of XSLT-based NLP component integration, and present examples of XSL transformation of shallow and deep annotations used in the integrated architecture. The infrastructure is open, portable and well suited for, but not restricted to the development of hybrid NLP architectures as well as NLP applications.






Detecting Sub-Topic Correspondence Through Bipartite Term Clustering

This paper addresses a novel task of detecting sub-topic correspondence in a pair of text fragments, enhancing common notions of text similarity. This task is addressed by coupling corresponding term subsets through bipartite clustering. The paper presents a cost-based clustering scheme and compares it with a bipartite version of the single-link method, providing illustrating results.






Can we Relearn an RBMT System?

This paper describes SYSTRAN submissions for the shared task of the third Workshop on Statistical Machine Translation at ACL. Our main contribution consists in a French-English statistical model trained without the use of any human-translated parallel corpus. In substitution, we translated a monolingual corpus with SYSTRAN rule-based translation engine to produce the parallel corpus. The results are provided herein, along with a measure of error analysis.






An Automatic Procedure For Topic-Focus Identification
The dichotomy of topic and focus, based, in the Praguean Functional Generative Description, on the scale of communicative dynamism, is relevant not only for a possible placement of the sentence in a context, but also for its semantic interpretation. An automatic identification of topic and focus may use the input information on word order, on the systemic ordering of kinds of complementations (reflected by the underlying order of the items included in the focus), on definiteness, and on lexical semantic properties of words. An algorithm for the analysis of English sentences has been implemented and is discussed and illustrated on several examples.






Efficient Multilingual Phoneme-To-Grapheme Conversion Based On HMM

Grapheme-to-phoneme conversion (GTPC) has been achieved in most European languages by dictionary look-up or using rules. The application of these methods, however, in the reverse process, (i.e., in phoneme-to-grapheme conversion [PTGC]) creates serious problems, especially in inflectionally rich languages. In this paper the PTGC problem is approached from a completely different point of view. Instead of rules or a dictionary, the statistics of language connecting pronunciation to spelling are exploited. The novelty lies in modeling the natural language intraword features using the theory of hidden Markov models (HMM) and performing the conversion using the Viterbi algorithm. The PTGC system has been established and tested on various multilingual corpora. Initially, the first-order HMM and the common Viterbi algorithm were used to obtain a single transcription for each word. Afterwards, the second-order HMM and the N-best algorithm adapted to PTGC were implemented to provide one or more transcriptions for each word input (homophones). This system gave an average score of more than 99% correctly transcribed words (overall success in the first four candidates) for most of the seven languages it was tested on (Dutch, English, French, German, Greek, Italian, and Spanish). The system can be adapted to almost any language with little effort and can be implemented in hardware to serve in real-time speech recognition systems.






Should Computers Write Spoken Language?






From Information Structure to Intonation: A Phonological Interface for Concept-to-Speech
The paper describes an interface between generator and synthesizer of the German language concept-to-speech system VieCtoS. It discusses phenomena in German intonation that depend on the interaction between grammatical dependencies (projection of information structure into syntax) and prosodie context (performance-related modifications to intonation patterns). Phonological processing in our system comprises segmental as well as suprasegmental dimensions such as syllabification, modification of word stress positions, and a symbolic encoding of intonation. Phonological phenomena often touch upon more than one of these dimensions, so that mutual accessibility of thedata structures on each dimension had to be ensured. We present a linear representation of the multidimensional phonologicaldata based on a straightforward linearization convention, which suffices to bring this conceptually multilineardata set under the scope of the well-known processing techniques for two-level morphology.






An Intelligent Multi-Dictionary Environment
An open, extendible multi-dictionary system is introduced in the paper. It supports the translator in accessing adequate entries of various bi- and monolingual dictionaries and translation examples from parallel corpora. Simultaneously an unlimited number of dictionaries can be held open, thus by a single interrogation step, all the dictionaries (translations, explanations, synonyms, etc.) can be surveyed. The implemented system (called MoBiDic) knows morphological rules of the dictionaries' languages. Thus, never the actual (inflected) words, but always their lemmas - that is, the right dictionary entries - are looked up. MoBiDic has an open, multimedial architecture, thus it is suitable for handling not only textual, but speaking or picture dictionaries, as well. The same system is also able to find words and expressions in corpora, dynamically providing the translators with examples from their earlier translations or other translators' works. MoBiDic has been designed for translator workgroups, where the translators' own glossaries (built also with the help of the system) may also be disseminated among the members of the group, with different access rights, if needed. The system has a TCP/IP-based client-server implementation for various platforms and available with a gradually increasing number of dictionaries for numerous language pairs.






A Lemmatization Method for Modern Mongolian and its Application to Information Retrieval
In Modern Mongolian, a content word can be inflected when concatenated with suffixes. Identifying the original forms of content words is crucial for natural language processing and information retrieval. We propose a lemmatization method for Modern Mongolian and apply our method to indexing for information retrieval. We use technical abstracts to show the effectiveness of our method experimentally.






A Robust And Hybrid Deep-Linguistic Theory Applied To Large-Scale Parsing
Modern statistical parsers are robust and quite fast, but their output is relatively shallow when compared to formal grammar parsers. We suggest to extend statistical approaches to a more deep-linguistic analysis while at the same time keeping the speed and low complexity of a statistical parser. The resulting parsing architecture suggested, implemented and evaluated here is highly robust and hybrid on a number of levels, combining statistical and rule-based approaches, constituency and dependency grammar, shallow and deep processing, full and near-full parsing. With its parsing speed of about 300,000 words per hour and state-of-the-art performance the parser is reliable for a number of large-scale applications discussed in the article.






Semi-Automatic Generation Of Dialogue Applications In The GEMINI Project
GEMINI (Generic Environment for Multilingual Interactive Natural Interfaces) is an EC funded research project, which has two main objectives: First, the development of a flexible platform able to produce user-friendly interactive multilingual and multi-modal dialogue interfaces to databases with a minimum of human effort, and, second, the demonstration of the platform's efficiency through the development of two different applications based on this platform: EG-Banking, a voice-portal for high-quality interactions for bank customers, and CitizenCare, an e-government platform framework for citizen-to-administration interaction which are available for spoken and web-based user interaction.





Strategies For Advanced Question Answering
Progress in Question Answering can be achieved by (1) combining multiple strategies that optimally resolve different question classes of various degrees of complexity; (2) enhancing the precision of question interpretation and answer extraction; and (3) question decomposition and answer fusion. In this paper we also present the impact of modeling the user background on Q/A and discuss the pragmatics pf processing negation in Q/A.





Morphology Induction From Term Clusters

We address the problem of learning a morphological automaton directly from a monolingual text corpus without recourse to additional resources. Like previous work in this area, our approach exploits orthographic regularities in a search for possible morphological segmentation points. Instead of affixes, however, we search for affix transformation rules that express correspondences between term clusters induced from thedata. This focuses the system on substrings having syntactic function, and yields cluster-to-cluster transformation rules which enable the system to process unknown morphological forms of known words accurately. A stem-weighting algorithm based on Hubs and Authorities is used to clarify ambiguous segmentation points. We evaluate our approach using the CELEX database.





Cross Language Text Categorization By Acquiring Multilingual Domain Models From Comparable Corpora
In a multilingual scenario, the classical monolingual text categorization problem can be reformulated as a cross language TC English Italian). English), Italian).





Chunk Parsing Revisited
Chunk parsing is conceptually appealing but its performance has not been satisfactory for practical use. In this paper we show that chunk parsing can perform significantly better than previously reported by using a simple sliding-window method and maximum entropy classifiers for phrase recognition in each level of chunking. Experimental results with the Penn Treebank corpus show that our chunk parser can give high-precision parsing outputs with very high speed (14 msec/sentence). We also present a parsing method for searching the best parse by considering the probabilities output by the maximum entropy classifiers, and show that the search method can further improve the parsing accuracy.





Automatic Knowledge Representation Using A Graph-Based Algorithm For Language-Independent Lexical Chaining
Lexical Chains are powerful representations of documents. In particular, they have successfully been used in the field of Automatic Text Summarization. However, until now, Lexical Chaining algorithms have only been proposed for English. In this paper, we propose a greedy Language-Independent algorithm that automatically extracts Lexical Chains from texts. For that purpose, we build a hierarchical lexico-semantic knowledge base from a collection of texts by using the Pole-Based Overlapping Clustering Algorithm. As a consequence, our methodology can be applied to any language and proposes a solution to language-dependent Lexical Chainers.






The Role Of Lexical Resources In CJK Natural Language Processing
The role of lexical resources is often understated in NLP research. The complexity of Chinese, Japanese and Korean (CJK) poses special challenges to developers of NLP tools, especially in the area of word segmentation (WS), information retrieval (IR), named entity extraction (NER), and machine translation (MT). These difficulties are exacerbated by the lack of comprehensive lexical resources, especially for proper nouns, and the lack of a standardized orthography, especially in Japanese. This paper summarizes some of the major linguistic issues in the development NLP applications that are dependent on lexical resources, and discusses the central role such resources should play in enhancing the accuracy of NLP tools.






Multilingual Collocation Extraction: Issues And Solutions

Although traditionally seen as a language-independent task, collocation extraction relies nowadays more and more on the linguistic preprocessing of texts (e.g., lemmatization, POS tagging, chunking or parsing) prior to the application of statistical measures. This paper provides a language-oriented review of the existing extraction work. It points out several language-specific issues related to extraction and proposes a strategy for coping with them. It then describes a hybrid extraction system based on a multilingual parser. Finally, it presents a case-study on the performance of an association measure across a number of languages.





Building Effective Question Answering Characters

In this paper, we describe methods for building and evaluation of limited domain question-answering characters. Several classification techniques are tested, including text classification using support vector machines, language-model based retrieval, and cross-language information retrieval techniques, with the latter having the highest success rate. We also evaluated the effect of speech recognition errors on performance with users, finding that retrieval is robust until recognition reaches over 50% WER.






A Co-occurrence Graph-based Approach for Personal Name Alias Extraction from Anchor Texts
A person may have multiple name aliases on the Web. Identifying aliases of a name is important for various tasks such as information retrieval, sentiment analysis and name disambiguation. We introduce the notion of a word co-occurrence graph to represent the mutual relations between words that appear in anchor texts. Words in anchor texts are represented as nodes in the co-occurrence graph and an edge is formed between nodes which link to the same url. For a given personal name, its neighboring nodes in the graph are considered as candidates of its aliases. We formalize alias identification as a problem of ranking nodes in this graph with respect to a given name. We integrate various ranking scores through support vector machines to leverage a robust ranking function and use it to extract aliases for a given name. Experimental results on a dataset of Japanese celebrities show that the proposed method outperforms all baselines, displaying a MRR score of 0.562.





Text Linkage In The Wiki Medium - A Comparative Study
We analyze four different types of document networks with respect to their small world characteristics. These characteristics allow distinguishing wiki-based systems from citation and more traditional text-based networks augmented by hyperlinks. The study provides evidence that a more appropriate network model is needed which better reflects the specifics of wiki systems. It puts emphasize on their topological differences as a result of wiki-related linking compared to other text-based networks.






Errors In Wikis

This discussion document concerns the challenges to assessments of reliability posed by wikis and the potential for language processing techniques for aiding readers to decide whether to trust particular text.






NTT System Description For The WMT2006 Shared Task
"We present two translation systems experimented for the shared-task of ""Workshop on Statistical Machine Translation,"" a phrase-based model and a hierarchical phrase-based model. The former uses a phrasal unit for translation, whereas the latter is conceptualized as a synchronous-CFG in which phrases are hierarchically combined using non-terminals. Experiments showed that the hierarchical phrase-based model performed very comparable to the phrase-based model. We also report a phrase/rule extraction technique differentiating tokenization of corpora. "






Web Corpus Mining By Instance Of Wikipedia

In this paper we present an approach to structure learning in the area of web documents. This is done in order to approach the goal of webgenre tagging in the area of web corpus linguistics. A central outcome of the paper is that purely structure oriented approaches to web document classification provide an information gain which may be utilized in combined approaches of web content and structure analysis.






BRUJA: Question Classification For Spanish Using Machine Translation and An English Classifier
Question Classification is an important task in Question Answering Systems. This paper presents a Spanish Question Classifier based on machine learning, automatic online translators and different language features. Our system works with English collections and bilingual questions (English/Spanish). We have tested two Spanish-English online translators to identify the lost of precision. We have made experiments using lexical, syntactic and semantic features to test which ones made a better performance. The obtained results show that our system makes good classifications, over a 80% in terms of accuracy using the original English questions and over a 65% using Spanish questions and machine translation systems. Our conclusion about the features is that a lexical, syntactic and semantic features combination obtains the best result.






Achilles: NiCT/ATR Chinese Morphological Analyzer for the Fourth Sighan Bakeoff

We created a new Chinese morphological analyzer, Achilles , by integrating rule-based, dictionary-based, and statistical machine learning method, conditional random fields (CRF). The rule-based method is used to recognize regular expressions: numbers, time and alphabets. The dictionary-based method is used to find in-vocabulary (IV) words while out-of-vocabulary (OOV) words are detected by the CRFs. At last, confidence measure based approach is used to weigh all the results and output the best ones. Achilles was used and evaluated in the bakeoff. We participated the closed tracks of word segmentation and part-of-speech tagging for all the provided corpus. In spite of an unexpected file encoding errors, the system exhibited a top level performance. A higher word segmentation accuracy for the corpus ckip and ncc were achieved. We are ranked at the fifth and eighth position out of all 19 and 26 submissions respectively for the two corpus. Achilles uses a feature combined approach for part-of-speech tagging. Our post-evaluation results prove the effectiveness of this approach for POS tagging.






Self-Training for Enhancement and Domain Adaptation of Statistical Parsers Trained on Small Datasets

Creating large amounts of annotated data to train statistical PCFG parsers is expensive, and the performance of such parsers declines when training and testdata are taken from different domains. In this paper we use self-training in order to improve the quality of a parser and to adapt it to a different domain, using only small amounts of manually annotated seeddata. We report significant improvement both when the seed and testdata are in the same domain and in the out-of-domain adaptation scenario. In particular, we achieve 50% reduction in annotation cost for the in-domain case, yielding an improvement of 66% over previous work, and a 20-33% reduction for the domain adaptation case. This is the first time that self-training with small labeled datasets is applied successfully to these tasks. We were also able to formulate a characterization of when self-training is valuable.






Designing an Extensible API for Integrating Language Modeling and Realization
"We present an extensible API for integrating language modeling and realization, describing its design and efficient implementation in the OpenCCG surface realizer. With OpenCCG, language models may be used to select realizations with preferred word orders, promote alignment with a conversational partner, avoid repetitive language use, and increase the speed of the best-first anytime search. The API enables a variety of n-gram models to be easily combined and used in conjunction with appropriate edge pruning strategies. The n-gram models may be of any order, operate in reverse (""right-to-left""), and selectively replace certain words with their semantic classes. Factored language models with generalized backoff may also be employed, over words represented as bundles of factors such as form, pitch accent, stem, part of speech, supertag, and semantic class. "






A Semantically Annotated Swedish Medical Corpus

With the information overload in the life sciences there is an increasing need for annotated corpora, particularly with biological and biomedical entities, which is the driving force for data-driven language processing applications and the empirical approach to language study. Inspired by the work in the GENIA Corpus, MEDLEX Corpus.





Ontology Search with the OntoSelect Ontology Library

OntoSelect is a dynamic web-based ontology library that harvests, analyzes and organizes ontologies published on the Semantic Web. OntoSelect allows searching as well as browsing of ontologies according to size (number of classes, properties), representation format (DAML, RDFS, OWL), connectedness (score over the number of included and referring ontologies) and human languages used for class- and object property-labels. Ontology search in OntoSelect is based on a combined measure of coverage, structure connectedness.





Speech Recognition And The Frequency Of Recently Used Words: A Modified Markov Model For Natural Language
Speech recognition systems incorporate a language model which, at each stage of the recognition task, assigns a probability of occurrence to each word in the vocabulary. A class of Markov language models identified by Jelinek has achieved consider able success in this domain. A modification of the Markov approach, which assigns higher probabilities to recently used words, is proposed and tested against a pure Markov model. Parameter calculation and comparison of the two models both involve use of the LOB Corpus of tagged modern English.






Experimenting With The Interaction Between Aggregation And Text Structuring


In natural language generation, different generation tasks often interact with each other in a complex way, which is hard to capture in the pipeline architecture described by Reiter ( Reiter, 1994 ). This paper focuses on the interaction between a specific type of aggregation and text planning, in particular, maintaining local coherence, and tries to explore what preferences exist among the factors related to the two tasks. The evaluation result shows that it is these preferences that decide the quality of the generated text and capturing them properly in a generation system could lead to coherent text.







Quantitative Modeling Of Segmental Duration

In natural speech, durations of phonetic segments are strongly dependent on contextual factors. Quantitative descriptions of these contextual effects have applications in text-to-speech synthesis and in automatic speech  recognition. In this paper, we describe a speaker-dependent system for predicting segmental duration from text, with emphasis on the statistical methods used for its construction. We also report results of a subjective listening experiment evaluating an implementation of this system for text-to-speech synthesis purposes.






Flexible Mixed-Initiative Dialogue Management Using Concept-Level Confidence Measures Of Speech Recognizer Output

We present a method to realize flexible mixed-initiative dialogue, in which the system can make effective confirmation and guidance using concept-level confidence measures (CMs) derived from speech recognizer output in order to handle speech recognition errors. We define two concept-level CMs, which are on content-words and on semantic-attributes, using 10-best outputs of the speech recognizer and parsing with phrase-level grammars. Content-word CM is useful for selecting plausible interpretations. Less confident interpretations are given to confirmation process. The strategy improved the interpretation accuracy by 11.5%. Moreover, the semantic-attribute CM is used to estimate user's intention and generates system-initiative guidances even when successful interpretation is not obtained.







The Automatic Acquisition Of Frequencies Of Verb Subcategorization Frames From Tagged Corpora

We describe a mechanism for automatically acquiring verb subcategorization frames and their frequencies in a large corpus. A tagged corpus is first partially parsed to identify noun phrases and then a linear grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus. In an experiment involving the identification of six fixed subcategorization frames, our current system showed more than 80% accuracy. In addition, a new statistical approach substantially improves the accuracy of the frequency estimation.







The Semantic Representation Of Spatial Configurations: A Conceptual Motivation For Generation In Machine Translation

This paper deals with the automatic translation of prepositions, which are highly polysemous. Moreover, the same real situation is often expressed by different prepositions in different languages. We proceed from the hypothesis that different usage patterns are due to different conceptualizations of the same real situation. Following cognitive principles of spatial conceptualization, we design a semantic interpretation process for spatial relations in which our translation system uses semantic features derived from a semantic sort hierarchy. Thus we can differentiate subtle distinctions between spatially significant configurations.







Investigating Complementary Methods For Verb Sense Pruning


We present an approach for tagging verb sense that combines a domain-independent method based on subcategorization and alternations with a domain-dependent method utilizing statistically extracted verb clusters. Initial results indicate that verb senses can be pruned for highly polysemous verbs by up to 74% by the first method and by up to 85% by the second method.







Building A Sense Tagged Corpus With Open Mind Word Expert

Open Mind Word Expert is an implemented active learning system for collecting word sense tagging from the general public over the Web. It is available at http://teach-computers.org. We expect the system to yield a large volume of high-quality training data at a much lower cost than the traditional method of hiring lexicographers. We thus propose a Senseval-3 lexical sample activity where the training data is collected via Open Mind Word Expert. If successful, the collection process can be extended to create the definitive corpus of word sense information.






Mapping Collocational Properties Into Machine Learning Features

This paper investigates interactions between collocational properties and methods for organizing them into features for machine learning. In experiments performing an event categorization task, Wiebe et al. (1997a) found that different organizations are best for different properties. This paper presents a statistical analysis of the results across different machine learning algorithms. In the experiments, the relationship between property and organization was strikingly consistent across algorithms. This prompted further analysis of this relationship, and an investigation of criteria for recognizing beneficial ways to include collocational properties in machine learning experiments. While many types of collocational properties and methods of organizing them into features have been used in NLP, systematic investigations of their interaction are rare.








KnoFusius: a New Knowledge Fusion System for Interpretation of Gene Expression Data

This paper introduces a new architecture that aims at combining molecular biology data with information automatically extracted from relevant scientific literature ( using text mining techniques on PubMed abstracts and fulltext papers ) to help biomedical experts to interpret experimental results in hand . The infrastructural level bears on semantic-web technologies and standards that facilitate the actual fusion of the multi-source knowledge .







Modelling Word Similarity : an Evaluation of Automatic Synonymy Extraction Algorithms .

Vector-based models of lexical semantics retrieve semantically related words automatically from large corpora by exploiting the property that words with a similar meaning tend to occur in similar contexts . Despite their increasing popularity , it is unclear which kind of semantic similarity they actually capture and for which kind of words . In this paper , we use three vector-based models to retrieve semantically related words for a set of Dutch nouns and we analyse whether three linguistic  properties of the nouns influence the results . In particular, we compare results from a dependency-based model  with those from a 1st and 2nd order bag-of-words model and we examine the effect of the nouns ' frequency , semantic speficity and semantic class . We find that all three models find more synonyms for high-frequency nouns and those belonging to abstract semantic classses. Semantic specificty does not have a clear influence .







Childrens Oral Reading Corpus (CHOREC): Description and Assessment of Annotator Agreement

Within the scope of the SPACE project , the CHildren's Oral REading Corpus (CHOREC) is developed . This database contains recorded , transcribed and annotated read speech  (42 GB or 130 hours) of 400 Dutch speaking elementary school children with or without reading difficulties .  Analyses of inter- and intra-annotator agreement are carried out in order to investigate the  consistency  with which reading errors are detected, orthographic and phonetic transcriptions are made, and reading errors and reading strategies are labeled. Percentage agreement scores and kappa values both show that agreement between annotations, and therefore the quality of the annotations, is high. Taken all double or triple annotations (for 10% resp. 30% of the corpus ) together, % agreement varies between 86.4% and 98.6%, whereas kappa varies between 0.72 and 0.97 depending on the annotation tier that is being assessed. School type and reading  type seem to account for systematic differences in % agreement , but these differences disappear when kappa values are calculated that correct for chance agreement . To conclude, an  analysis of the annotation differences  with respect to the '*s' label (i.e. a label that is used to annotate undistinguishable spelling behaviour ), phoneme labels, reading strategy and error labels is given.







A Bilingual  Corpus of Inter-linked Events

This paper describes the creation  of a bilingual corpus  of inter-linked events  for Italian and English . Linkage is accomplished through the Inter- Lingual Index (ILI) that links ItalWordNet with WordNet. The availability of this resource , on the one hand , enables  contrastive analysis  of the linguistic phenomena  surrounding events in both languages , and on the other hand , can be used to perform multilingual temporal analysis of texts . In addition to describing the methodology for construction of the inter-linked corpus and the analysis of the data collected, we demonstrate that the ILI could potentially be used to bootstrap the creation of comparable corpora by exporting layers of annotation for words that have the same sense .







New Resources for Document Classification , Analysis and Translation Technologies


The goal of the DARPA MADCAT (Multilingual Automatic Document Classification Analysis and Translation ) Program is to automatically convert foreign language text images into English transcripts , for use by humans and downstream applications . The first phase the program focuses on translation of handwritten Arabic documents . Linguistic Data Consortium (LDC) is creating publicly available linguistic resources for MADCAT technologies , on a scale and richness not previously available. Corpora will consist of existing LDC corpora and data donations from MADCAT partners, plus new data collection to provide high quality material for evaluation and to address strategic gaps (for genre , dialect , image quality , etc.) in the existing resources . Training and test data properties will expand over time to encompass a wide range of topics and genres : letters, diaries, training manuals , brochures, signs, ledgers, memos, instructions , post
cards and forms among others. Data will be ground truthed, with line, word and token segmentation and zoning, and translations and word alignments will be produced for a subset. Evaluation data will be carefully selected from the available data pools and high quality references will be produced, which can be used to compare MADCAT system performance against the human-produced gold standard .







Approximating Learning Curves for Active- Learning- Driven Annotation

Active learning (AL) is getting more and more popular as a methodology to considerably reduce the annotation effort when building training material for statistical learning methods for various NLP tasks . A crucial issue rarely addressed, however, is when to actually stop the annotation process to profit from the savings in efforts . This question is tightly related to estimating the classifier performance after a certain amount of data has already been annotated. While learning curves are the default means to monitor the progress of the annotation process in terms of classifier performance , this requires a labeled gold standard which - in realistic annotation settings, at least - is often unavailable. We here propose a method for committee-based AL to approximate the progression of the learning curve based on the disagreement among the committee members. This  method relies on a separate, unlabeled corpus and is thus well suited for situations where a labeled gold standard is not available or would be too expensive to obtain. Considering named entity recognition as a test case we provide empirical evidence that this approach works well under simulation as well as under real-world annotation conditions.






Lexicon Schemas and Related Data Models : when Standards Meet Users

Lexicon schemas and their use are discussed in this paper from the perspective of lexicographers and field linguists . A variety of lexicon schemas have been developed , with goals ranging from computational lexicography (DATR) through archiving (LIFT, TEI) to standardization (LMF, FSR). A number of requirements for lexicon schemas are given. The lexicon schemas are introduced and compared to each other in terms of conversion and usability for this particular user group, using a common lexicon entry and providing examples for each schema under consideration . The formats are assessed and the final recommendation is given for the potential users , namely to request standard compliance from the developers of the tools used. This paper should foster a discussion between authors of standards , lexicographers and field linguists .







Arabic WordNet: Semi-automatic Extensions using Bayesian Inference
This presentation focuses on the semi-automatic extension of Arabic WordNet (AWN) using lexical and morphological rules and applying Bayesian inference . We briefly report on the current status of AWN and propose a way of extending its coverage by taking advantage of a limited set of highly productive Arabic morphological rules for deriving a range of semantically related word forms from verb entries . The application of this set of rules , combined with the use of bilingual Arabic- English resources and Princeton's WordNet, allows the generation of a graph representing the semantic neighbourhood of the original word . In previous work, a set of associations between the hypothesized Arabic words and English synsets was proposed on the basis of this graph. Here, a novel approach to extending AWN is presented whereby a Bayesian Network is automatically built from the graph and then the net is used as an inferencing mechanism for scoring the set of candidate associations . Both on its own and in combination with the previous technique , this new approach has led to improved results .







Subjective Evaluation of an Emotional Speech Database for Basque
///q

This paper describes the evaluation process of an emotional speech database recorded for standard Basque, in order to determine its adequacy for the analysis of emotional models and its use in speech synthesis . The corpus consists of seven hundred semantically neutral sentences that were recorded for the Big Six emotions and neutral style, by two professional actors. The test results show that every emotion is readily recognized far above chance level for both speakers. Therefore the database is a valid linguistic resource for the research and development purposes it was designed for.







How to Compare Treebanks


Recent years have seen an increasing interest in developing standards for linguistic annotation, with a focus on the interoperability of the resources . This effort , however, requires a profound knowledge of the advantages and disadvantages of linguistic annotation schemes in order to avoid importing the flaws and weaknesses of existing encoding schemes into the new standards . This paper addresses the question how to compare syntactically annotated corpora and gain insights into the usefulness of specific design decisions . We present an exhaustive evaluation of two German treebanks with crucially different encoding schemes . We evaluate three different parsers trained on the two treebanks and compare results using EvalB, the Leaf-Ancestor metric , and a dependency-based evaluation . Furthermore, we present TePaCoC, a new testsuite for the evaluation of parsers on complex German grammatical constructions . The testsuite provides a well thought-out error classification , which enables us to compare parser output for parsers trained on treebanks with different encoding schemes and provides interesting insights into the impact of treebank annotation schemes on specific constructions like PP attachment or non-constituent coordination .







The INFILE Project : a Crosslingual Filtering Systems Evaluation Campaign


The InFile project ( INformation , FILtering, Evaluation ) is a cross-language adaptive filtering evaluation campaign, sponsored by the French National Research Agency. The campaign is organized by the CEA LIST , ELDA and the University of Lille3-GERiiCO. It has an international scope as it is a pilot track of the CLEF 2008 campaigns. The corpus is built from a collection of about 1,4 millions newswires (10 GB) in three languages , Arabic, English and French provided by Agence France Press (AFP) and selected from a 3 years period. The profiles corpus is made of 50 profiles from which 30 concern general news and events (national and international affairs, politics, sports ...) and 20 concern scientific and technical subjects.







DIAC+: a Professional Diacritics Recovering System

In languages that use diacritical characters, if these special signs are stripped-off from a word , the resulted string of characters may not exist in the language , and therefore its normative form is, in general, easy to recover. However, this is not always the case , as presence or absence of a diacritical sign attached to a base letter of a word which exists in both variants , may change its grammatical properties or even the meaning, making the recovery of the missing diacritics a difficult task , not only for a program but sometimes even for a human reader. We describe and evaluate an accurate knowledge-based system for automatic recovering the missing diacritics in MS-Office documents written in Romanian. For the rare cases when the system is not able to reliably make a decision,it either provides the user a list of words with their recovery suggestions , or probabilistically choose one of the possible changes, but leaves a trace (a highlighted comment) on each word the modification of which was uncertain.







Annotating an Arabic Learner Corpus for Error

This paper describes an ongoing project in which we are collecting a learner corpus of Arabic, developing a tagset for error annotation and performing Computer-aided Error Analysis (CEA) on the data . We adapted the French Interlanguage Database FRIDA tagset ( Granger, 2003a ) to the data . We chose FRIDA in order to follow a known standard and to see whether the changes needed to move from a French to an Arabic tagset would give us a measure of the distance between the two languages with respect to learner difficulty . The current collection of texts , which is constantly growing, contains intermediate and advanced-level student writings . We describe the need for such corpora , the learner data we have collected and the tagset we have developed . We also describe the error frequency distribution of both proficiency levels and the ongoing work.







All, and only, the Errors: more Complete and Consistent Spelling and OCR-Error Correction Evaluation

Some time in the future, some spelling error correction system will correct all the errors , and only the errors . We need evaluation metrics that will tell us when this has been achieved and that can help guide us there. We survey the current practice in the form of the evaluation scheme of the latest major publication on spelling correction in a leading journal . We are forced to conclude that while the metric used there can tell us exactly when the ultimate goal of spelling correction research has been achieved, it offers little in the way of directions to be followed to eventually get there. We propose to consistently use the well-known metrics Recall and Precision , as combined in the F score, on 5 possible levels of measurement that should guide us more informedly along that path . We describe briefly what is then measured or measurable at these levels and propose a framework that should allow for concisely stating what it is one performs in one's evaluations . We finally contrast our preferred metrics to Accuracy , which is widely used in this field to this day and to the Area- Under-the-Curve, which is increasingly finding acceptance in other fields .







Using Movie Subtitles for Creating a Large- Scale Bilingual Corpora

This paper presents a method for compiling a large-scale bilingual corpus from a database of movie subtitles . To create the corpus , we propose an algorithm based on Gale and Church's sentence alignment algorithm (1993). However, our algorithm not only relies on character length information , but also uses subtitle-timing information , which is encoded in the subtitle files. Timing is highly correlated between subtitles in different versions (for the same movie), since subtitles that match should be displayed at the same time . However, the absolute time values can't be used for alignment , since the timing is usually specified by frame numbers and not by real time , and converting it to real time values is not always possible, hence we  use normalized subtitle duration instead . This results in a significant reduction in the alignment error rate.







The IFADV Corpus : a Free Dialog Video Corpus

"Research into spoken language has become more visual over the years. Both fundamental and applied research have progressively included gestures , gaze, and facial expression . Corpora of multi-modal conversational speech are rare and frequently difficult to use due to privacy and copyright restrictions . A freely available annotated corpus is presented, gratis and libre, of high quality video recordings of face-to-face conversational speech . Within the bounds of the law, everything has been done to remove copyright and use restrictions . Annotations have been processed to RDBMS tables that allow SQL queries and direct connections to statistical software . From our experiences we would like to advocate the formulation of ""best practises"" for both legal handling and database storage of recordings and annotations. "







WOZ Acoustic Data Collection for Interactive TV

This paper describes a multichannel acoustic data collection recorded under the European DICIT project , during the Wizard of Oz (WOZ) experiments carried out at FAU and FBK-irst laboratories . The scenario is a distant-talking interface for interactive control of a TV. The experiments involve the acquisition of multichannel data for signal processing front-end and were carried out due to the need to collect a database for testing acoustic pre-processing algorithms . In this way, realistic scenarios can be simulated at a preliminary stage, instead of real-time implementations , allowing for repeatable experiments . To match the project requirements , the WOZ experiments were recorded in three languages : English , German and Italian. Besides the user inputs , the database also contains non-speech related acoustic events , room impulse response measurements and video data, the latter used to compute 3D labels . Sessions were manually transcribed and segmented at word level , introducing also specific labels for acoustic events .






Process Model for Composing High-quality Text Corpora
The Teko corpus composing model offers a decentralized, dynamic way of collecting high-quality text corpora for linguistic research . The  resulting corpus consists of independent text sets. The sets are composed in cooperation with linguistic research projects , so each of them responds to a specific research need. The corpora are morphologically annotated and XML-based, with in-built compatibilty with the Kaino user interface used in the corpus server of the Research Institute for the Languages of Finland. Furthermore, software for extracting standard quantitative reports from the text sets has been created during the project . The paper describes the project , and estimates its benefits and problems . It also gives an overview of the technical qualities of the corpora and corpus interface connected to the Teko project .







AnCora: Multilevel Annotated Corpora for Catalan and Spanish


This paper presents AnCora, a multilingual corpus annotated at different linguistic levels consisting of 500,000 words in Catalan (AnCora-Ca) and in Spanish (AnCora-Es). At present AnCora is the largest multilayer annotated corpus of these languages freely available from http://clic. ub. edu/ancora. The two corpora consist mainly of newspaper texts annotated at different levels of linguistic description : morphological (PoS and lemmas ), syntactic ( constituents and functions ), and semantic ( argument structures , thematic roles , semantic verb classes , named entities , and WordNet nominal senses). All resulting layers are independent of each other, thus making easier the data management . The annotation was performed manually, semiautomatically, or fully automatically, depending on the encoded linguistic information . The development of these basic resources constituted a primary objective , since there was a lack of such resources for these languages . A second goal was the definition of a consistent methodology that can be followed in further annotations. The current versions of AnCora have been used in several international evaluation competitions






Studying Discourse and Dialogue with SIDGrid


Teaching Computational Linguistics is inherently multi-disciplinary and frequently poses challenges and provides opportunities in teaching to a student body with diverse educational backgrounds and goals . This paper describes the use of a computational environment (SIDGrid) that facilitates interdisciplinary instruction by providing support for students with little computational background as well as extending the scale of projects accessible to students with more advanced computational skills. The environment facilitates the use of hands-on exercises and is being applied to interdisciplinary instruction in Discourse and Dialogue .






A Combined Memory-Based Semantic Role Labeler of English
We describe the system submitted to the closed challenge of the CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. Syntactic dependencies are processed with the Malt-Parser 0.4. Semantic dependencies are processed with a combination of memory-based classifiers. The system achieves 78.43 labeled macro Fl for the complete problem, 86.07 labeled attachment score for syntactic dependencies, and 70.51 labeled Fl for semantic dependencies.





Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme ConversionLetter-to-phoneme conversion generally requires aligned trainingdata of letters and phonemes. Typically, the alignments are limited to one-to-one alignments. We present a novel technique of training with many-to-many alignments. A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists. We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word. The many-to-many alignments result in significant improvements over the traditional one-to-one approach. Our system achieves state-of-the-art performance on several languages anddata sets.






Conquest---An Open-Source Dialog System for Conferences

We describe ConQuest, an open-source, reusable spoken dialog system that provides technical program information during conferences. The system uses a transparent, modular and open infrastructure, and aims to enable applied research in spoken language interfaces. The conference domain is a good platform for applied research since it permits periodical redeployments and evaluations with a real user-base. In this paper, we describe the system's functionality, overall architecture, and we discuss two initial deployments.






Attention Shifting For Parsing Speech
We present a technique that improves the efficiency of word-lattice parsing as used in speech recognition language modeling. Our technique applies a probabilistic parser iteratively where on each iteration it focuses on a different subset of the word-lattice. The parser's attention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited. This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model.






A Hybrid Relational Approach For WSD - First Results
We present a novel hybrid approach for Word Sense Disambiguation (WSD) which makes use of a relational formalism to represent instances and background knowledge. It is built using Inductive Logic Programming techniques to combine evidence coming from both sources during the learning process, producing a rule-based WSD model. We experimented with this approach to disambiguate 7 highly ambiguous verbs in English-Portuguese translation. Results showed that the approach is promising, achieving an average accuracy of 75%, which outperforms the other machine learning techniques investigated (66%).





Machine Translation Based On Logically Isomorphic Montague Grammars

"Usually two approaches to machine translation are distinguished: the interlingual approach and the transfer approach ( cf. Hutchins [1]). In the interlingual approach translation is a two-stage process: from source language to interlingua and from interlingua to target language. In the transfer approach there are three stages: source language analysis, transfer and target language generation. The approach advanced in this paper is a variant of the interlingual one. It requires that 'logically isomorphic grammars' are written for the languages,under consideration. The syntactic rules of these grammars must correspond with logical operations, in accordance with the compositionality principle of Montague grammar. Moreover, the grammars must be attuned to each other as follows: if one grammar contains a rule corresponding with a particular logical operation, the other grammars must contain rules corresponding with the same operation. Syntactically, these rules may differ considerably. If the grammars are attuned to each other in this way,  'logical derivation trees', representations of both the syntactical and the logical structure of sentences, can be used as intermediate expressions. The paper is organized as follows. In section 2 the relevant concepts of Montague grammar and the notion 'logically isomorphic grammars' are introduced. In section 3 a version of Montague grammar is described, called M-grammar, which is more suitable for computational use than Montague 's original proposals. The property of logical isomorphy is then defined for M-grammars. In section 4 the design of the Rosetta translation system, based on this approach, is outlined, followed by a brief discussion in section 5. "






A Compositional Semantics For Directional Modifiers - Locative Case Reopened

This paper presents a model-theoretic semantics for directional modifiers in English. The semantic theory presupposed for the analysis is that of Montague Grammar (cf. Montague 1970 , 1973) which makes it possible to develop a strongly compositional treatment of directional modifiers. Such a treatment has significant computational advantages over case-based treatments of directional modifiers that are advocated in the Al literature.






Stylistic Grammars In Language Translation
We are developing stylistic grammars to provide the basis for a French and English stylistic parser. Our stylistic grammar is a branching stratificational model, built upon a foundation dealing with lexical, syntactic, and semantic stylistic realizations. Its central level uses a vocabulary of constituent stylistic elements common to both English and French, while the top level correlates stylistic goals, such as clarity and concreteness, with patterns of these elements. Overall, we are implementing a computational schema of stylistics in French-to-English translation. We believe that the incorporation of stylistic analysis into machine translation systems will significantly reduce the current reliance on human post-editing and improve the quality of the systems' output.






Segmenting A Sentence Into Morphemes Using Statistic Information Between Words
This paper is on dividing non-separated language sentences (whose words are not separated from each other with a space or other separater
) into morphemes using statistical information, not grammatical information which is often used in NLP. In this paper we describe our method and experimental result on Japanese and Chinese sentences. As will be seen in the body of this paper, the result shows that this system is efficient for most of the sentences.






A System Of Verbal Semantic Attributes Focused On The Syntactic Correspondence Between Japanese And English
This paper proposes a system of 97 verbal semantic attributes for Japanese verbs which considers both dynamic characteristics and the relationship of verbs to cases. These attribute values are used to disambiguate the meanings of all Japanese and English pattern pairs in a Japanese to English transfer pattern dictionary consisting of 15,000 pairs of Japanese valence patterns and equivalent English syntactic structures.





Dependency Unification Grammar

This paper describes the analysis component of the language processing system PLAIN from the viewpoint of unification grammars. The principles of Dependency Unification Grammar (DUG) are discussed. The computer language DRL (Dependency Representation Language) is introduced in which DUGs can be formulated. A unification-based parsing procedure is part of the formalism. PLAIN is implemented at. the universities of Heidelberg, Bonn, Flensburg, Kiel, Zurich and Cambridge U.K.






Acquiring Domain-Specific Dialog Information from Task-Oriented Human-Human Interaction through an Unsupervised Learning
We describe an approach for acquiring the domain-specific dialog knowledge required to configure a task-oriented dialog system that uses human-human interactiondata. The key aspects of this problem are the design of a dialog information representation and a learning approach that supports capture of domain information from in-domain dialogs. 






A Development Environment for Configurable Meta-Annotators in a Pipelined NLP Architecture
Information extraction from largedata repositories is critical to Information Management solutions. In addition to prerequisite corpus analysis, to determine domain-specific characteristics of text resources, developing, refining and evaluating analytics entails a complex and lengthy process, typically requiring more than just domain expertise. Modern architectures for text processing, while facilitating reuse and (re-)composition of analytical pipelines, place additional constraints upon the analytics development, as domain experts need not only configure individual annotator components, but situate these within a fully functional annotator pipeline. We present the design, and current status, of a tool for configuring model-driven annotators, which abstracts away from annotator implementation details, pipeline composition constraints, anddata management. Instead, the tool embodies support for all stages of ontology-centric model development cycle - from corpus analysis and concept definition, to model development and testing, to large scale evaluation, to easy and rapid composition of text applications deploying these concept models. With our design, we aim to meet the needs of domain experts, who are not necessarily expert NLP practitioners.





Combining Multiple Models for Speech Information Retrieval
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a Speech Information Retrieval task. The formulas for combining the models are tuned on trainingdata. Then the system is evaluated on testdata. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain good results on this data set, except for the case when manual summaries are included.






Subdomain Sensitive Statistical Parsing using Raw Corpora
Modern statistical parsers are trained on large annotated corpora (treebanks). These treebanks usually consist of sentences addressing different subdomains (e.g. sports, politics, music), which implies that the statistics gathered by current statistical parsers are mixtures of subdomains of language use. In this paper we present a method that exploits raw subdomain corpora gathered from the web to introduce subdomain sensitivity into a given parser. We employ statistical techniques for creating an ensemble of domain sensitive parsers, and explore methods for amalgamating their predictions. Our experiments show that introducing domain sensitivity by exploiting raw corpora can improve over a tough, state-of-the-art baseline.






An Experiment On The Upper Bound Of Interjudge Agreement: The Case Of Tagging
We investigate the controversial issue about the upper bound of interjudge agreement in the use of a low-level grammatical representation. Pessimistic views suggest that several percent of words in running text are undecidable in terms of part-of-speech categories. Our experiments with 55kWdata give reason for optimism: linguists with only 30 hours' training apply the EngCG-2 morphological tags with almost 100% interjudge agreement.






Pointing To Events
Although there is an extensive body of research concerned with anaphora resolution (e.g. ( Fox, 1987 ; Grosz et al., 1995 )), event anaphora has been widely neglected. This paper describes the results of an empirical study regarding event reference. The experiment investigated event anaphora in narrative discourse via a sentence completion task.





Learning PP Attachment For Filtering Prosodic Phrasing

We explore learning prepositional-phrase attachment in Dutch, to use it as a filter in prosodic phrasing. From a syntactic treebank of spoken Dutch we extract instances of the attachment of prepositional phrases to either a governing verb or noun. Using cross-validated parameter and feature selection, we train two learning algorithms, iBl and RIPPER, on making this distinction, based on unigram and bigram lexical features and a cooccurrence feature derived from WWW counts. We optimize the learning on noun attachment, since in a second stage we use the attachment decision for blocking the incorrect placement of phrase boundaries before prepositional phrases attached to the preceding noun. On noun attachment, IBl attains an F-score of 82; RIPPER an F-score of 78. When used as a filter for prosodic phrasing, using attachment decisions from IBl yields the best improvement on precision (by six points to 71) on phrase boundary placement.






A Distributed Architecture For Text Analysis In French: An Application To Complex Linguistic Phenomena Processing
Most Natural Language Processing systems use a sequential architecture embodying classical linguistic layers. When one works with a general language; and not a sublanguage, there are different cases of ambiguities at different classical levels; and more particularly when one works on complex language phenomena analysis (coordination, ellipsis, negation...) it is difficult to take into account all the different types of these constructions with a general grammar. Indeed, the inconvenience of this approach is the possible risk of a combinatory explosion. So, we have defined the TALISMAN architecture that includes linguistic agents that correspond either to classical levels in linguistics (morphology, syntax, semantic) or to complex language phenomena analysis.





Automatic Thesaurus Generation Through Multiple Filtering

In this paper, we propose a method of generating bilingual keyword clusters or thesauri from parallel or comparable bilingual corpora. The method combines morphological and lexical processing, bilingual word alignment, and graph-theoretic cluster generation. An experiment shows that the method is promising.






Implicit Ambiguity Resolution Using Incremental Clustering In Korean-To-English Cross-Language Information Retrieval
This paper presents a method to implicitly resolve ambiguities using dynamic incremental clustering in Korean-to-English cross-language information retrieval. In the framework we propose, a query in Korean is first translated into English by looking up Korean-English dictionary, then documents are retrieved based on the vector space retrieval for the translated query terms. For the top-ranked retrieved documents, query-oriented document clusters are incrementally created and the weight of each retrieved document is re-calculated by using clusters. In experiment on TREC-6 CLIR test collection, our method achieved 28.29% performance improvement for translated queries without ambiguity resolution for queries. This corresponds to 97.27% of the monolingual performance for original queries. When we combine our method with query ambiguity resolution, our method even outperforms the monolingual retrieval.






An Algorithmic Framework For Solving The Decoding Problem In Statistical Machine Translation
The decoding problem in Statistical Machine Translation (SMT) is a computationally hard combinatorial optimization problem. In this paper, we propose a new algorithmic framework for solving the decoding problem and demonstrate its utility. In the new algorithmic framework, the decoding problem can be solved both exactly and approximately. The key idea behind the framework is the modeling of the decoding problem as one that involves alternating maximization of two relatively simpler subproblems. We show how the subproblems can be solved efficiently and how their solutions can be combined to arrive at a solution for the decoding problem. A family of provably fast decoding algorithms can be derived from the basic techniques underlying the framework and we present a few illustrations. Our first algorithm is a prov-ably linear time search algorithm. We use this algorithm as a subroutine in the other algorithms. We believe that decoding algorithms derived from our framework can be of practical significance.






A Flexible Example Annotation Schema: Translation Corresponding Tree Representation
This paper presents work on the task of constructing an example base from a given bilingual corpus based on the annotation schema of Translation Corresponding Tree (TCT). Each TCT describes a translation example (a pair of bilingual sentences). It represents the syntactic structure of source language sentence, and more importantly is the facility to specify the correspondences between string (both the source and target sentences) and the representation tree. Furthermore, syntax transformation clues are also encapsulated at each node in the TCT representation to capture the differentiation of grammatical structure between the source and target languages. With this annotation schema, translation examples are effectively represented and organized in the bilingual knowledge database that we need for the Portuguese to Chinese machine translation system.






Deep Linguistic Analysis For The Accurate Identification Of Predicate-Argument Relations
This paper evaluates the accuracy of HPSG parsing in terms of the identification of predicate-argument relations. We could directly compare the output of HPSG parsing with Prop-Bank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model, an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations.






Pearl: A Probabilistic Chart Parser
This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the ""best"" parse of a sentence. The parser, Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context, of the sentence predicts that interpretation. This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context, to predict, likelihood. 'Pearl also provides a framework for incorporating the results of previous work in pait-ol'-speech assignment, unknown word models, and other probabilistic models of linguistic features into one parsing tool, interleaving these techniques instead of using the traditional pipeline architecture. In preliminary tests, 'Pearl has been successful at resolving part-of-speech and word (in speech processing) ambiguity, determining categories for unknown words, and selecting correct parses first using a very loosely fitting covering grammar."







Relationships between Nursing Converstaions and Activities


In this paper , we determine the relationships between nursing activities and nurseing conversations based on the principle of maximum entropy . For analysis of the features of nursing activities, we built nursing corpora from actual nursing conversation sets collected in hospitals that involve various information about nursing activities. Ex-nurses manually assigned nursing activity information to the nursing conversations in the corpora . Since it is inefficient and too expensive to attach all information manually, we introduced an automatic nursing activity determination method for which we built models of relationships between nursing conversations and activities. In this paper , we adopted a maximum entropy approach for learning. Even though the conversation data set is not large enough for learning, acceptable results were obtained.






Forest-based Translation Rule Extraction 
Translation rule extraction is a fundamental problem in machine translation , especially for linguistically syntax-based packed forest





Automatic induction of FrameNet lexical units

Most attempts to integrate FrameNet in NLP systems have so far failed because of its limited coverage . In this paper , we investigate the applicability of distributional and WordNet-based models on the task of lexical unit induction ,






Classification of Multiple- Sentence Questions 
Abstract .






Using the Structure of a Conceptual Network in Computing Semantic Relatedness 
Abstract . proper







Investigating NLG Architectures: Taking Style Into Consideration

In this paper we propose a methodology for investigating the relationship between architectures of natural language generation (NLG)






Automatic Evaluation For A Palpable Measure Of A Speech Translation System 's Capability

The main goal of this paper is to propose automatic schemes for the translation paired comparison method . This method was proposed to precisely evaluate a speech translation system 's capability . Furthermore, the method gives an objective evaluation result , i.e., a score of the Test of English for International Communication (TOEIC). The TOEIC score is used as a measure of one's speech translation capability . However, this method requires tremendous evaluation costs . Accordingly, automatization of this method is an important subject for study . In the proposed method , currently available automatic evaluation methods are applied to automate the translation paired comparison method . In the experiments , several automatic evaluation methods (BLEU, NIST, DP-based method ) are applied . The experimental results of these automatic measures show a good correlation with evaluation results of the translation paired comparison method .







Arabic Syntactic Trees: From Constituency To Dependency

This research note reports on the work in progress which regards automatic transformation of phrase-structure syntactic trees of Arabic into dependency-driven analytical ones. Guidelines for these descriptions have been developed at the Linguistic Data Consortium, University of Pennsylvania, and at the Faculty of Mathematics and Physics and the Faculty of Arts, Charles University in Prague, respectively. The transformation consists of (i) a recursive function translating the topology of a phrase tree into a corresponding dependency tree , and (ii) a procedure assigning analytical functions to the nodes of the dependency tree . Apart from an outline of the annotation schemes and a deeper insight into these procedures , model application of the transformation is given herein.







Latent Semantic Information In Maximum Entropy Language Models For Conversational Speech Recognition 
Latent semantic analysis (LSA), first exploited in indexing documents for information retrieval , has since been used by several researchers to demonstrate impressive reductions in the perplexity of statistical language models on text corpora such as the Wall Street Journal . In this paper we present an investigation into the use of LSA in language modeling for conversational speech recognition . We find that previously proposed methods of combining an LSA-based unigram model with an N- feature






A Categorial Variation Database For English

"We describe our approach to the construction and evaluation of a large-scale database called ""CatVar"" which contains categorial variations of English lexemes. Due to the prevalence of cross-language categorial variation in multilingual applications , our categorial-variation resource may serve as an integral part of a diverse range of natural language applications . Thus, the research reported herein overlaps heavily with that of the machine-translation , lexicon-construction , and information-retrieval communities . We apply the information-retrieval metrics of precision and recall to evaluate the accuracy and coverage of our database with respect to a human-produced gold standard . This evaluation reveals that the categorial database achieves a high degree of precision and recall . Additionally, we demonstrate that the database improves on the linkability of Porter stemmer by over 30%. "







GE NLTOOLSET: MUC-3 Test Results And Analysis

This paper reports on the GE NLTOOLSET customization effort for MUC-3, and analyzes the results of the TST2 run. Although our own tests had shown steady improvement between TST1 and TST2, our official scores on TST2 were lo wer than on TST1. The analysis of this unexpected result explains some of the details of th e MUC-3 test , and we propose ways of looking at the scores to distinguish different aspects of system performance .






Text Filtering In MUC-3 And MUC-4


"One of the changes from the Third (MUC-3) to the Fourth (MUC-4) Message Understanding Conference was the emergence of text filtering as an explicit topic of discussion . In this paper we examine text filtering in MUC systems with three goals in mind. First, we clarify the difference between two uses of the term ""text filtering"" in the context of data extraction systems , and put these phenomena in the context of prior research on information retrieval (IR). Secondly, we discuss the use of text filtering components in MUC-3 and MUC-4 systems , and present a preliminary scheme for classifying data extraction systems in terms of the features over which they do text filtering. Finally, we examine the text filtering effectiveness of MUC-3 and MUC-4 systems , and introduce some approaches to the evaluation of text filtering systems which may be of interest themselves. Two questions of crucial interest are whether sites improved their system level text filtering effectiveness from MUC-3 to MUC-4, and what the effectiveness of MUC systems would be on real world data streams . Because of changes in both test set and system design since MUC-3 we were not able to address the first question . However, with respect to the second question , we present preliminary evidence suggesting that the text filtering precision of MUC systems declines with the generality of the data stream they process , i.e. the proportion of relevant documents . The ramifications of this for future research and for operational systems are discussed. "








Template Design For Information Extraction

The design of the template for an information extraction application (or exercise) reflects the nature of the task and therefore crucially affects the success of the attempt to capture information from text . This paper addresses the template design requirement by discussing the general principles or template definition effort which is explicitly discussed in a Case Study in the last section of this paper .






Selection : Salience, Relevance And The Coupling Between Domain- Level Tasks And Text Planning


In this paper we examine some issues pertaining to the task of selection in text planning. We attempt to distinguish salience and relevance , and characterize their role as important fundamental notions governing selection . We also formulate the problem of selection of text content in terms of the coupling between domain-level tasks and text planning tasks . We describe our research on generating bus route descriptions .Keywords: Natural Language Generation , Text Planning, Selection , Salience, Relevance , Coupling, Route Descriptions







Using Cue Phrases To Determine Rhetorical Relations

'Relation based ' approaches to discourse analysis and text generation suffer from a common problem : there is considerable disagreement between researchers over the set of relations which is proposed . Few researchers use identical sets of relations , and many use relations not found in any other sets. This proliferation of relations has been pointed out before (eg Hovy [1]), and several methods for justifying a standard set of relations have been proposed : this paper reviews some of these, and presents a new method of justification which overcomes some awkward problems .






Domain Structure , Rhetorical Structure , And Text Structure

It is generally agreed that text has structure (at least, coherent text does). Therefore, an understanding and appreciation of text structure must play some role in building computational systems that are capable of using text as people do. What is less clear is what are necessary and sufficient sources of structure for a text-using system , and further, what such a system needs to know about and do with these structures in the process of using text . By using text , I mean understanding it or producing it; speaking it, writing it, or thinking about it. Li this paper , I present a case for the importance of domain structure in structuring text , and discuss the role of rhetorical structure and intentionality.






Corpus- Based Adaptation Mechanisms For Chinese Homophone Disambiguation 
Based on the concepts of bidirectional conversion and automatic evaluation , we propose two nser-adapiation mechanisms , character-preference learning and pstlido-word learning , for resolving Chinese homophone ambiguities in syllable-to-character conversion . The 1991  United Daily corpus oj approximately 10 million Chinese characters is used for extraction of 10 reporter-specific article databases and for computation of word frequencies and character bi-grams. Experiments show that 20.5 percent (testing sets) to 71.8 percent ( training sets ) of conversion errors can be eliminated through the proposed mechanisms . These concepts are thus very useful m applications such as Chinese input methods and speech recognition systems





Automatic Construction Of A Chinese Electronic Dictionary

    In this paper , an unsupervised approach for constructing a large-scale Chinese electronic dictionary is surveyed . The main purpose is to enable cheap and quick acquisition of a large-scale dictionary from a large untagged text corpus with the aid of the information in a small tagged seed corpus . The basic model is based on a Viterbi reestimation technique . During the dictionary construction process , it tries to optimize the automatic segmentation and tagging process by repeatedly refining the set of parameters of the underlying language model . The refined parameters are then used to further get a better tagging result . In addition , a two-class classifier , which is capable of classifying an n-gram either as a word or a non-word , is used in combination with the Viterbi training module to improve the system performance . Two different system configurations had been developed to construct the dictionary . The configurations include (1) a Viterbi word identification module followed by a Viterbi POS tagging module and (2) a two-class classification module as the postfilter for the above Viterbi word identification module . With a seed of 1,000 sentences and an untagged corpus of 311,591 sentences , the performance for bigram word identification is 56.88% in precision and 77.37% in recall when the two-class classifier is applied to the word list suggested by the Viterbi word identification module . The Viterbi part of speech tag reestimation stage gives the figures of 71.16% and 71.81% weighted precision rates and 73.42% and 73.83% weighted recall rates for the 2 different configurations when using a seed corpus of 9676 sentences .







A Geometric Approach To Mapping Bitext Correspondence

The first step in most corpus-based multilingual NLP work is to construct a detailed map of the correspondence between a text and its translation . Several automatic methods for this task have been proposed in recent years. Yet even the best of these methods can err by several typeset pages . The Smooth Injective Map Recognizer (SIMR) is a new bitext mapping algorithm . SIMR's errors are smaller than those of the previous front-runner by more than a factor of 4. Its robustness has enabled new commercial-quality applications . The greedy nature of the algorithm makes it independent of memory resources . Unlike other bitext mapping algorithms , SIMR allows crossing correspondences to account for word order differences . Its output can be converted quickly and easily into a sentence alignment . SIMR's output has been used to align more than 200 megabytes of the Canadian Hansards for publication by the Linguistic Data Consortium.







The Measure Of A Model

This paper describes measures for evaluating the three determinants of how well a probabilistic classifier performs on a given test set . These determinants are the appropriateness , for the test set , of the results of (1) feature selection , (2) formulation of the parametric form of the model , and (3) parameter estimation . These are part of any model formulation procedure , even if not broken out as separate steps , so the tradeoffs explored in this paper are relevant to a wide variety of methods . The measures are demonstrated in a large experiment , in which they are used to analyze the results of roughly 300 classifiers that perform word-sense disambiguation .







Approximate Generation From Non-Hierarchical Representations


This paper presents a technique for sentence generation . We argue that the input to generators should have a non-hierarchical nature . This allows us to investigate a more general version of the sentence generation problem where one is not pre-committed to a choice of the syntactically prominent elements in the initial semantics . We also consider that a generator can happen to convey more (or less) information than is originally specified in its semantic input . In order to constrain this approximate matching of the input we impose additional restrictions on the semantics of the generated sentence . Our technique provides flexibility to address cases where the entire input cannot be precisely expressed in a single sentence . Thus the generator does not rely on the strategic component having linguistic knowledge . We show clearly how the semantic structure is declaratively related to linguistically motivated syntactic representation .






Input Specification In The WAG Sentence Generation System

This paper describes the input specification language of the WAG Sentence Generation system . The input is described in terms of Halliday 's (1978) three meaning components , ideational meaning (the propositional content to be expressed), interactional meaning (what the speaker intends the listener to do in making the utterance ), and textual meaning (how the content is structured as a message , in terms of theme, reference , etc.).






Dealing With Multilinguality In A Spoken Language Query Translator 
Robustness is an important issue for multilingual speech interfaces for spoken language translation systems . We have studied three aspects of robustness in such a system : accent differences , mixed language input , and the use of common feature sets for HMM-based speech recognizers for English and Cantonese. The results of our preliminary experiments show that accent differences cause recognizer performance to degrade . A rather surprising finding is that for mixed language input , a straight forward implementation of a mixed language model-based speech recognizer performs less well than the concatenation of pure language recognizers. Our experimental results also show that a common feature set , parameter set, and common algorithm lead to different performance output for Cantonese and English speech recognition modules .






English- To-Mandarin Speech Translation With Head Transducers


We describe the head transducer model used in an experimental English-to- Mandarin speech translation system . Head transduction is a translation method in which weighted finite state transducers are associated with source-target word pairs . The method is suitable for speech translation because it allows efficient bottom up processing . The head transducers in the experimental system have a wider range of output positions than input positions. This asymmetry is motivated by a tradeoff between model complexity and search efficiency .







Filtering Errors And Repairing Linguistic Anomalies For Spoken Dialogue Systems

Our work addresses the integration of speech recognition and language processing for whole spoken dialogue systems .To filter ill-recognized words , we design an on-line computing of word confidence scores based on the recognizer output hypothesis . To infer as much information as possible from the retained sequence of words , we propose a bottom-up syntactico-semantic robust parsing relying on a lexi-calized tree grammar and on integrated repairing strategies .







A Programmable Multi-Blackboard Architecture For Dialogue Processing Systems

In current Natural Language Processing Systems , different components for different processing tasks and input / output modalities have to be integrated. Once integrated, the interactions between the components have to be specified. Interactions in dialogue systems can be complex due in part to the many states the system can be in. When porting the system to another domain , parts of the integration process have to be repeated. To overcome these difficulties , we propose a multi-blackboard architecture that is controlled by a set of expert-system like rules . These rules may contain typed variables . Variables can be substituted by representations with an appropriate type stored in the blackboards. Furthermore, the representations in the blackboards allow to represent partial information and to leave disjunctions unresolved. Moreover, the conditions of the rule may depend on the specificity of the representations with which the variables are instantiated. For this reason , the interaction is information-driven . The described system has been implemented and has been integrated with the speech recognizer JANUS.






Phrase Structure Trees Bear More Fruit Than You Would Have Thought


In this paper we will present several results concerning phrase structure trees . These results show that phrase structure trees , when viewed in certain ways, have much more descriptive power than one would have thought. We have given a brief account of local constraints on structural descriptions and an intuitive proof of a theorem about local constraints . We have compared the local constraints approach to some aspects of Gazdar 's framework and that of Peters and Ritchie and of Karttunen . We have also presented some results on skeletons ( phrase structure trees without labels) which show that phrase structure trees , even when deprived of the labels, retain in a certain sense all the structural information . This result has implications for grammatical inference procedures .






Natural- Language Interface

A major problem faced by would-be users of computer systems is that computers generally make use of special-purpose languages familiar only to those trained in computer science . For a large number of applications requiring interaction between humans and computer systems , it would be highly desirable for machines to converse in English or other natural languages familiar to their human users . Over the last decade, in laboratories around the world, several computer systems have been developed that support at least elementary levels of natural-language interaction . Among these are such systems as those described in the several references at the end of this paper .






Combining Multiple, Large- Scale Resources in a Reusable Lexicon for Natural Language Generation

A lexicon is an essential component in a generation system but few efforts have been made to build a rich, large-scale lexicon and make it reusable for different generation applications . In this paper , we describe our work to build such a lexicon by combining multiple, heterogeneous linguistic resources which have been developed for other purposes . Novel transformation and integration of resources is required to reuse them for generation . We also applied the lexicon to the lexical choice and realization component of a practical generation application by using a multi-level feedback architecture . The integration of the lexicon and the architecture is able to effectively improve the system paraphrasing power, minimize the chance of grammatical errors , and simplify the development process substantially.







A Method for Correcting Errors in Speech Recognition using the Statistical Features of Character Co-occurrence

It is important to correct the errors in the results of speech recognition to increase the performance of a speech translation system . This paper proposes a method for correcting errors using the statistical features of character co-occurrence , and evaluates the method . The proposed method comprises two successive correcting processes . The first process uses pairs of strings : the first string is an erroneous substring of the utterance predicted by speech recognition , the second string is the corresponding section of the actual utterance . Errors are detected and corrected according to the database learned from erroneous-correct utterance pairs . The remaining errors are passed to the posterior process which uses a string in the corpus that is similar to the string including recognition errors . The results of our evaluation show that the use of our proposed method as a post-processor for speech recognition is likely to make a significant contribution to the performance of speech translation systems . method also obtains reliably recognized partial segments of an utterance by cooperatively using both grammatical and n-gram based statistical language constraints , and uses a robust parsing technique to apply the grammatical constraints described by context-free grammar ( Tsukada






Bitext Correspondences through Rich Mark-up

Rich mark-up can considerably benefit the process of establishing bitext correspondences , that is, the task of providing correct identification and alignment methods for text segments that are translation equivalences of each other in a parallel corpus . We present a sentence alignment algorithm that, by taking advantage of previously annotated texts , obtains accuracy rates close to 100%. The algorithm evaluates the similarity of the linguistic and extra-linguistic mark-up in both sides of a bitext. Given that annotations are neutral with respect to typological, grammatical and orthographical differences between languages , rich mark-up becomes an optimal foundation to support bitext correspondences . The main originality of this approach is that it makes maximal use of annotations, which is a very sensible and efficient method for the exploitation of parallel corpora when annotations exist.







Translating a Unification Grammar with Disjunctions into Logical Constraints


This paper proposes a method for generating a logical-constraint-based internal representation from a unification grammar formalism with disjunctive information . Unification grammar formalisms based on path equations and lists of pairs of labels and values are better than those based on first-order terms in that the former is easier to describe and to understand. Parsing with term-based internal representations is more efficient than parsing with graph-based representations . Therefore, it is effective to translate unification grammar formalism based on path equations and lists of pairs of labels and values into a term-based internal representation . Previous translation methods cannot deal with disjunctive feature descriptions, which reduce redundancies in the grammar and make parsing efficient. Since the proposed method translates a formalism without expanding disjunctions , parsing with the resulting representation is efficient.







ParaMetric: An Automatic Evaluation Metric for Paraphrasing


We present ParaMetric, an automatic evaluation metric for data-driven approaches to paraphrasing. ParaMetric provides an objective measure of quality using a collection of multiple translations whose paraphrases have been manually annotated. ParaMetric calculates precision and recall scores by comparing the paraphrases discovered by automatic paraphrasing techniques against gold standard alignments of words and phrases within equivalent sentences . We report scores for several established paraphrasing techniques .






Re-estimation of Lexical Parameters for Treebank PCFGs


We present procedures which pool lexical information estimated from unlabeled data via the Inside-Outside algorithm , with lexical information from a treebank PCFG. The procedures produce substantial improvements (up to 31.6% error reduction ) on the task of determining subcategorization frames of novel verbs , relative to a smoothed Penn Treebank-trained PCFG. Even with relatively small quantities of unlabeled training data, the re-estimated models show promising improvements in labeled bracketing /scores on Wall Street Journal parsing , and substantial benefit in acquiring the subcategorization preferences of low-frequency verbs .






Semantic Classification with Distributional Kernels


Distributional measures of lexical similarity and kernel methods for classification are well-known tools in Natural Language Processing . We bring these two methods together by introducing distributional kernels






Domain Adaptation for Statistical Machine Translation with Domain Dictionary and Monolingual Corpora 
Statistical machine translation systems are usually trained on large amounts of bilingual text and monolingual text . In this paper , we propose a method to perform domain adaptation for statistical machine translation , where in-domain bilingual corpora do not exist. This method first uses out-of-domain corpora to train a baseline system and then uses in-domain translation dictionaries and in-domain monolingual corpora to improve the indomain performance . We propose an algorithm to combine these different resources in a unified framework . Experimental results indicate that our method achieves absolute improvements of 8.16 and 3.36 BLEU scores on Chinese to English translation and English to French translation respectively, as compared with the baselines using only out-of-domain corpora .






Sentence Type Based Reordering Model for Statistical Machine Translation

Many reordering approaches have been proposed for the statistical machine translation (SMT) system . However, the information about the type of source sentence is ignored in the previous works. In this paper , we propose a group of novel reordering models based on the source sentence type for Chinese-to- English translation . In our approach , an SVM-based classifier is employed to classify the given Chinese sentences into three types : special interrogative sentences , other interrogative sentences , and non-question sentences . The different reordering models are developed oriented to the different sentence types . Our experiments show that the novel reordering models have obtained an improvement of more than 2.65% in BLEU for a phrase-based spoken language translation system .






Automatic Processing Of Written French Language

An automatic processor of written French language is described. This processor uses syntactic and semantic informations about words in order to construct a semantic net representing the meaning of the sentences . The structure of the network and the principles of the parser are explained. An application to the processing of the medical records is then discussed.






Computer- Aided Grammatical Tagging Of Spoken English








A Rule- Based Approach To Ill- Formed Input

Though natural language understanding systems have improved markedly in recent years, they have only begun to consider a major problem of truly natural input : ill-formedness. Quite often natural language input is ill-formed in the sense of being misspelled, ungrammatical, or not entirely meaningful. A requirement for any successful natural language interface must be that the system either intelligently guesses at a user 's intent, requests direct clarification , or at the very least, accurately identifies the ill-formedness. This paper presents a proposal for the proper treatment of ill-formed input . Our conjecture is that ill-formedness should be treated as rule-based . Violation of the rules of normal processing should be used to signal ill-formedness. Meta-rules modifying the rules of normal processing should be used for error identification and recovery . These meta-rules correspond to types of errors . Evidence for this conjecture is presented as well as some open questions .







The Knowledge Representation For A Story Understanding And Simulation System

TOYONAKA,  OSAKA 560, JAPAN !!!! MATSUSHITA ELECTRIC  INDUSTRIAL CO.,LTD. KADOMA,  OSAKA 571, JAPAN Abstruet





Language Resources and Chemical Informatics


Chemistry research papers are a primary source of information about chemistry, as in any scientific field . The presentation of the data is, predominantly, unstructured information , and so not immediately susceptible to processes developed within chemical informatics for carrying out chemistry research by information processing techniques . At one level , extracting the relevant information from research papers is a text mining task , requiring both extensive language resources and specialised knowledge of the subject domain . However, the papers also encode information about the way the research is conducted and the structure of the field itself. Applying language technology to research papers in chemistry can facilitate eScience on several different levels . The SciBorg project sets out to provide an extensive, analysed corpus of published chemistry research . This relies on the cooperation of several journal publishers to provide papers in an appropriate form . The work is carried out as a collaboration involving the Computer Laboratory , Chemistry Department and eScience Centre at Cambridge University , and is funded under the UK eScience programme.







MeSH: from a Controlled Vocabulary to a Processable Resource

Large repositories of life science data in the form of domain-specific literature , textual databases and other large specialised textual collections ( corpora ) in electronic form increase on a daily basis to a level beyond the human mind can grasp and interpret. As the volume of data continues to increase , substantial support from new information technologies and computational techniques grounded in the form of the ever increasing applications of the mining paradigm






An Evaluation of Spoken and Textual Interaction in the RITEL Interactive Question Answering System

The RITEL project aims to integrate a spoken language dialogue system and an open-domain information retrieval system in order to enable human users to ask a general question and to refine their search for information interactively. This type of system is often referred to as an Interactive Question Answering (IQA) system . In this paper , we present an evaluation of how the performance of the RITEL system differs when users interact with it using spoken versus textual input and output . Our results indicate that while users do not perceive the two versions to perform significantly differently, many more questions are asked in a typical text-based dialogue .






Automatic Prediction of Parser Accuracy 
Statistical parsers have become increasingly accurate, to the point where they are useful in many natural language applications . However, estimating parsing accuracy on a wide variety of domains and genres is still a challenge in the absence of gold-standard parse trees . In this paper , we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains . As a result , we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain .







French- English Terminology Extraction from Comparable Corpora 
Abstract .







Turn-taking in Mandarin Dialogue : Interactions of Tone and Intonation

Fluent dialogue requires that speakers successfully negotiate and signal turn-taking. While many cues to turn change have been proposed , especially in multi-modal frameworks , here we focus on the use ofprosodic cues to these functions . In particular, we consider the use of prosodic cues in a tone language , Mandarin Chinese , where variations in pitch height and slope additionally serve to determine word meaning . Within a corpus of spontaneous Chinese dialogues , we find that turn-unit final syllables are significantly lower in average pitch and intensity than turn-unit initial syllables in both smooth turn changes and segments ended by speaker overlap. Interruptions are characterized by significant prosodic differences from smooth turn initiations. Furthermore, we demonstrate that these contrasts correspond to an overall lowering across all tones in final position, which largely preserves the relative heights of the lexical tones. In classification tasks , we contrast the use of text and prosodic features . Finally, we demonstrate that, on balanced training and test sets , we can distinguish turn-unit final words from other words at 
 93% accuracy and interruptions from smooth turn unit initiations at 62% accuracy .







Multi-Modal Combinatory Categorial Grammar

The paper shows how Combinatory Categorial Grammar (CCG) can be adapted to take advantage of the extra resource-sensitivity provided by the Categorial Type Logic framework . The resulting reformulation, Multi-Modal CCG, supports lexically specified control over the applicability of combinatory rules , permitting a universal rale component and shedding the need for language-specific restrictions on rules . We discuss some of the linguistic motivation for these changes, define the Multi-Modal CCG system and demonstrate how it works on some basic examples . We furthermore outline some possible extensions and address computational aspects of Multi-Modal CCG.






Automatic Acquisition Of Script Knowledge From A Text Collection

In this paper , we describe a method for automatic acquisition of script knowledge from a Japanese text collection . Script knowledge represents a typical sequence of actions that occur in a particular situation . We extracted sequences ( pairs ) of actions occurring in time order from a Japanese text collection and then chose those that were typical of certain situations by ranking these sequences ( pairs ) in terms of the frequency of their occurrence . To extract sequences of actions occurring in time order , we constructed a text collection in which texts describing facts relating to a similar situation were clustered together and arranged in time order . We also describe a preliminary experiment with our acquisition system and discuss the results .







Comparing Automatic And Human Evaluation Of NLG Systems

We consider the evaluation problem in Natural Language Generation (nlg) and present results for evaluating several nlg systems with similar functionality , including a knowledge-based generator and several statistical systems . We compare evaluation results for these systems by human domain experts , human non-experts, and several automatic evaluation metrics , including nist, bleu, and rouge . We find that nist scores correlate best (> 0.8) with human judgments , but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone. We conclude that automatic evaluation of nlg systems has considerable potential, in particular where high-quality reference texts and only a small number of human evalua-tors are available. However, in general it is probably best for automatic evaluations to be supported by human-based evaluations , or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain .






Word Alignment With Cohesion Constraint

We present a syntax-based constraint for word alignment , known as the cohesion constraint . It requires disjoint English phrases to be mapped to non-overlapping intervals in the French sentence . We evaluate the utility of this constraint in two different algorithms . The results show that it can provide a significant improvement in alignment quality .







Bayesian Nets For Syntactic Categorization Of Novel Words

This paper presents an application of a Dynamic Bayesian Network (DBN) to the task of assigning Part-of- Speech (PoS) tags to novel text . This task is particularly challenging for non-standard corpora , such as Internet lingo, where a large proportion of words are unknown. Previous work reveals that PoS tags depend on a variety of morphological and contextual features . Representing these dependencies in a DBN results into an elegant and effective PoS tagger.







Detecting Structural Metadata With Decision Trees And Transformation- Based Learning


The regular occurrence of disfluencies is a distinguishing characteristic of spontaneous speech . Detecting and removing such disfluencies can substantially improve the usefulness of spontaneous speech transcripts . This paper presents a system that detects various types of disfluencies and other structural information with cues obtained from lexical and prosodic information sources . Specifically, combinations of decision trees and language models are used to predict sentence ends and interruption points and, given these events , transformation-based learning is used to detect edit disfluencies and conversational fillers . Results are reported on human and automatic transcripts of conversational telephone speech .






Feature- Based Pronunciation Modeling For Speech Recognition

We present an approach to pronunciation modeling in which the evolution of multiple linguistic feature streams is explicitly represented. This differs from phone-based models in that pronunciation variation is viewed as the result of feature asynchrony and changes in feature values , rather than phone substitutions , insertions , and deletions . We have implemented a flexible feature-based pronunciation model using dynamic Bayesian networks . In this paper , we describe our approach and report on a pilot experiment using phonetic transcriptions of utterances from the Switchboard corpus . The experimental results , as well as the model 's qualitative behavior , suggest that this is a promising way of accounting for the types of pronunciation variation often seen in spontaneous speech .






Confidence Estimation For Information Extraction 
Information extraction techniques automatically create structured databases from unstructured data sources , such as the Web or newswire documents . Despite the successes of these systems , accuracy will always be imperfect. For many reasons , it is highly desirable to accurately estimate the confidence the system has in the correctness of each extracted field . The information extraction system we evaluate is based on a linear-chain conditional random field (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a Markov model . We implement several techniques to estimate the confidence of both extracted fields and entire multi-field records , obtaining an average precision of 98% for retrieving correct fields and 87% for multi-field records .







PRC Inc: Description Of The PAKTUS System Used For MUC-3


The PRC Adaptive Knowledge-based Text Understanding System (PAKTUS) has been under development as an Independent Research and Development project at PRC since 1984. The objective is a generic system of tools , including a core English lexicon , grammar, and concept representations , for building natural language processing (NLP) systems for text understanding . Systems built with PAKTUS are intended to generate input to knowledge based systems or data base systems . Input to the NLP system is typically derived from an existing electronic message stream , such as a news wire. PAKTUS supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , news reports about a specific type of event , such as financial transfers or terrorist acts, etc., by acquiring sublanguage and domain-specific grammar, words , conceptual mappings , and discourse patterns . The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success .







Practical World Modeling For NLP Applications







Lexicon Design Using A Paradigmatic Approach

The paper describes models for representation and methods to handle lexicographic structures supplied by the







Practical Issues In Automatic Documentation Generation

PLANDoc, a system under joint development by Columbia and Bellcore, documents the activity of planning engineers as they study telephone routes . It takes as input a trace of the engineer's interaction with a network planning tool and produces 1-2 page summary . In this paper , we describe the user needs analysis we performed and how it influenced the development of PLANDoc. In particular, we show how it pinpointed the need for a sublanguage specification , allowing us to identify input messages and to characterize the different sentence paraphrases for realizing them. We focus on the systematic use of conjunction in combination with paraphrase that we developed for PLANDoc, which allows for the generation of summaries that are both concise-avoiding repetition of similar information , and fluent-avoiding repetition of similar phrasing.







The Dragon Continuous Speech Recognition System : A Real- Time Implementation

We present a 1000- word continuous speech recognition (CSR) system that operates in real time on a personal computer (PC). The system , designed for large vocabulary natural language tasks , makes use of phonetic Hidden Markov models (HMM) and incorporates acoustic, phonetic, and linguistic sources of knowledge to achieve high recognition performance . We describe the various components of this system . We also present our strategy for achieving real time recognition on the PC. Using a 486- based PC with a 29K- based add-on board, the recognizer has been timed at 1.1 times real time .







Preliminary ATIS Development At MIT


"DARPA has recently initiated a plan for a common spoken language task , to be developed independently by all members of the DARPA community , with the hope that it will provide a mechanism leading to appropriate formal evaluation procedures at the level of spoken language . The task that was selected for this purpose is the Air Travel Information System (ATIS) task , based on selected tables from the Official Airline Guide (OAG). It was decided that the first evaluation would be limited in scope to deal with text input only, and to cover only sentences that could be understood unambiguously out of context . Data have been recorded over the past several months at Texas Instruments, using an interface that involves a ""wizard"" who fully interprets the meaning of the subject's sentences , and generates database responses using a menu driven data access system . We have been actively engaged in the last few months in developing the natural language and back end portions of the MIT version of the ATIS domain . This paper describes our progress to date on this effort , including an evaluation of the performance of the system on the recently released designated DARPA test set . The remainder of this paper is organized as follows. First we will give a general description of the system we are developing , emphasizing those aspects that differ from the current general conception of the common task . Next we will describe in greater detail certain aspects of the back end, including knowledge representation , control strategy , the user interface , and our preliminary treatment of discourse history. This is followed by a section describing changes made in the parser , in the areas of semantics , the interface with the back-end, and a preliminary new-word treatment . This section also includes a brief discussion of some interesting phenomena that occurred in the training sentences . An evaluation section follows, discussing our system 's performance on both training and test data, as well as a preliminary assessment of the perplexity of the system . We conclude with a summary of our results and our position on the nature of the common task . "







Session 3: Machine Translation 
Semantic analysis to resolve lexical and syntactic ambiguities during parsing , and thus reduce translation errors very significantly. 
 Unification grammars allowing syntactic and semantic constraints to be checked in a unified manner while parsing , and permitting reversible grammars
i.e., the same grammars to be used for generation as well as for analysis . 
 Advanced parsing methodologies , including augmented-LR compilation where knowledge sources ( syntactic grammars, lexicons , and semantic ontologies ) can be defined and maintained separately but are jointly compiled to apply simultaneously at run time , both in parsing and in generation . 
 Natural language generation , focusing on how to structure fluent target-language output , an activity not truly investigated in the pre-ALPAC days. 
 Automated corpus analysis tools , statistical and other means of extracting useful information from large bi- or multi-lingual corpora , including collocations , transfers , and contextual cues for disambiguation . 
 MRDs => MTDs, use of electronic machine-readable dictionaries (MRDs) to partially automate the creation of machine-tractable dictionaries (MTDs) in processable internal form for parsers and generators , permitting principled scaling up in MT configurations .







Calculating The Probability Of A Partial Parse Of A Sentence

A standard problem iu parsiug algorithms is the organization o[ branched searches lo deal with ambiguous sentences . We discuss shift-reduce parsiug of stochastic context-free grammars and show how to construct a probabilistic score for ranking competing parse hypotheses . The score we use is the likelihood that the collection of subtrees can be completed into a full parse tree by means of the steps the parser is constrained lo follow.






Discourse Structure In The TRAINS Project

In a natural dialog , a considerable proportion of the utterances actually relate to the maintenance of the dialog itself rather than to furthering the task or goals motivating the conversation . For example , many utterances serve to acknowledge, clarify, correct a previous utterance rather than pursue some goal in the domain . In addition , natural dialog is full of false starts, ungrammatical sentences and other complexities not found in in written language . This paper describes our recent efforts to define and construct a model of discourse interaction that handle dialogs that are rich in these natural dialog-related phenomena .







Session 11: Prosody

This paper provides a brief introduction to prosody research in the context of human-computer communication and an overview of the contributions of the papers in the session.







Gisting Continuous Speech

"The objective of this woik is automatic , real-time ""gisting"" of voice traffic for updating of information in databases , for producing timely reports , and for prompt notification of events of interest. Specifically, the goal is to build a prototype , real-time system capable of processing radio communication between air traffic controllers and pilots , identifying dialogs and extracting their ""gist"" (e.g., identifying flights, determining whether they are landing or taking off), and producing a continuous output stream with that information . The approach is intended to be general and applicable to other domains . The system is built upon state-of-the-art techniques in speech recognition , speaker identification , natural language analysis , and topic statistical classification . These techniques have been extended where necessary to address specific aspects of the gisting problem . Because various sources of information must be combined, the system design features a high degree of interaction between the natural language and domain-knowledge components and the speech processing components . "







The Automatic Component Of The LINGSTAT Machine- Aided Translation System

We present the newest implementation of the LINGSTAT machine-aided translation system . The most significant change from earlier versions is a new set of modules that produce a draft translation of the document for the user to refer to or modify. This paper describes these modules , with special emphasis on an automatically trained lexicalized grammar used in the parsing module . Some preliminary results from the January  1994  ARPA evaluation are reported .







Contextual Spelling Correction Using Latent Semantic Analysis
Contextual spelling errors are denned as the use of an incorrect, though valid, word in a particular sentence or context . Traditional spelling checkers flag misspelled words , but they do not typically attempt to identify words that are used incorrectly in a sentence . We explore the use of







The Text REtrieval Conferences (TRECs) - Summary Results Of TREC-3 And TREC-4


"There have been four Text REtrieval Conferences (TRECs); TREC-1 in November  1992 , TREC-2 in August  1993 , TREC-3 in November  1994  and TREC-4 in November  1995 . The number of participating systems has grown from 25 in TREC-1 to 36 in TREC-4, including most of the major text retrieval software companies and most of the universities doing research in text retrieval (see table for some of the participants ). The diversity of the participating groups has ensured that TREC represents many different approaches to text retrieval , while the emphasis on individual experiments evaluated in a common setting has proven to be a major strength of TREC. The test design and test collection used for document detection in TIPSTER was also used in TREC. The participants ran the various tasks , sent results into NIST for evaluation , presented the results at the TREC conferences, and submitted papers for a proceedings . The test collection consists of over 1 million documents from diverse full-text sources , 250 topics , and the set of relevant documents or ""right answers"" to those topics . A Spanish collection has been built and used during TREC-3 and TREC-4, with a total of 50 topics . TREC-1 required significant system rebuilding by most groups due to the huge increase in the size of the document collection (from a traditional test collection of several megabytes in size to the 2 gigabyte TIPSTER collection ). The results from TREC-2 showed significant improvements over the TREC-1 results , and should be viewed as the appropriate baseline representing state-of-the-art retrieval techniques as scaled up to handling a 2 gigabyte collection . TREC-3 therefore provided the first opportunity for more complex experimentation . The major experiments in TREC-3 included the development of automatic query expansion techniques , the use of passages or sub-documents to increase the precision of retrieval results , and the use of the training information to select only the best terms for routing queries . Some groups explored hybrid approaches (such as the use of the Rocchio methodology in systems not using a vector space model ), and others tried approaches that were radically different from their original approaches . TREC-4 allowed a continuation of many of these complex experiments . The topics were made much shorter and this change triggered extensive investigations in automatic query expansion . There were also five new tasks , called tracks. These were added to help focus research on certain known problem areas , and included such issues as investigating searching as an interactive task by examining the process as well as the outcome , investigating techniques for merging results from the various TREC subcollections, examining the effects of corrupted data, and evaluating routing systems using a specific effectiveness measure. Additionally more groups participated in a track for Spanish retrieval . The TREC conferences have proven to be very successful, allowing broad participation in the overall DARPA TIPSTER effort , and causing widespread use of a very large test collection . All conferences have had very open, honest discussions of technical issues , and there have been large amounts of ""cross-fertilization "" of ideas. This will be a continuing effort , with a TREC-5 conference scheduled in November of 1996. "







Acquiring Predicate- Argument Mapping Information From Multilingual Texts

This paper discusses automatic acquisition of predicate-argument mapping information from multilingual texts . The lexicon of our NLP system abstracts the language-dependent portion of predicate-argument mapping information from the core meaning of verb senses (i.e.







Constraints, Exceptions And Representations


This paper shows that default-based phonologies have the potential to capture morphophonological generalisations which cannot be captured by non-default theories . In achieving this result , I offer a characterisation of Underspecification Theory and Optimality Theory in terms of their methods for ordering defaults . The result means that machine learning techniques for building declarative analyses may not provide an adequate basis for morphophonological analysis .









A Czech Morphological Lexicon

In this paper , a treatment of Czech phonological rules in two-level morphology approach is described . First the possible phonological alternations in Czech are listed and then their treatment in a practical application of a Czech morphological lexicon .






Semantic Visualization

This paper summarizes several initiatives at MITRE that are investigating the visualization of a range of content . We present results of our work in relevancy visualization , news visualization , world events visualization and sensor/battlefield visualization to enhance user interaction in information access and exploitation tasks . We summarize several initiatives we are currently pursuing and enumerate unsolved problems .







Nominal Metonymy Processing 
Abstract . We argue for the necessity of resolution of metonymies for nominals (and other cases ) in the context of semantics-based machine translation . By using an ontology as a search space , we are able to identify and resolve m
tonymie expressions with significant accuracy , both for a pre-deterrnined inventory of metonymie types and for previously unseen cases . The entity replaced by the metonymy is made explicitly available in our meaning representation , to support translation , anaphora, and other mechanisms .







Towards Multimodal Spoken Language Corpora : TransTool And SyncTool


This paper argues for the usefulness of multimodal spoken language corpora and specifies components of a platform for the creation , maintenance and exploitation of such corpora . Two of the components , which have already been implemented as prototypes , are described in more detail : TransTool and SyncTool. TransTool is a transcription editor meant to facilitate and partially automate the task of a human transcriber, while SyncTool is a tool for aligning the resulting transcriptions with a digitized audio and video recording in order to allow synchronized presentation of different representations (e.g., text , audio, video , acoustic analysis ). Finally, a brief comparison is made between these tools and other programs developed for similar purposes .







Discovering Lexical Information By Tagging Arabic Newspaper Text

In this paper we describe a system for building an Arabic lexicon automatically by tagging Arabic newspaper text . In this system we are using several techniques for tagging the words in the text and figuring out their types and their features . The major techniques that we are using are: finding phrases , analyzing the affixes of the words , and analyzing their patterns . Proper nouns are particularly difficult to identify in the Arabic language ; we describe techniques for isolating them.







A Recognition- Based Meta- Scheme For Dialogue Acts Annotation


The paper describes a new formal framework for comparison , design and standardization of annotation schemes for dialogue acts . The framework takes a recognition-based approach to dialogue tagging and defines four independent taxonomies of tags , one for each orthogonal dimension of linguistic and contextual analysis assumed to have a bearing on identification of illocutionary acts. The advantages and limitations of this proposal over other previous attempts are discussed and concretely exemplified.







Multi- Document Summarization By Visualizing Topical Content 
Mani , Inderjeet; House, David ; Klein , Gary ; Hirschman , Lynette ; Firmin Hand , Therese ; Sundheim , Beth M. ,The TIPSTER SUMMAC Text Summarization Evaluation ,Conference Of The European Association For Computation al Linguistics ,1999 *** Nagao , Katashi; Hasida, Koiti, Automatic Text Summarization Based on the Global Document Annotation,COLING-ACL,1998***Power, Richard ; Scott , Donia R.,Multilingual Authoring using Feedback Texts ,COLING-ACL,1998***Mani, Inderjeet; Gates , Barbara ; Bloedorn , Eric ,Improving Summaries By Revising Them,Annual Meeting Of The Association For Computation al Linguistics ,1999






A Common Theory Of Information Fusion From Multiple Text Sources Step One: Cross- Document Structure

We introduce CST ( cross-document structure theory ), a paradigm for multi-document analysis . CST takes into account the rhetorical structure of clusters of related textual documents . We present a taxonomy of cross-document relationships . We argue that CST can be the basis for multi-document summarization guided by user preferences for summary length , information provenance, cross-source agreement , and chronological ordering of facts.






Error- Driven HMM-Based Chunk Tagger With Context- Dependent Lexicon

This paper proposes a new error-driven HMM-based text chunk tagger with context-dependent lexicon . Compared with standard HMM-based tagger, this tagger uses a new Hidden Markov Modelling approach which incorporates more contextual information into a lexical entry . Moreover, an error-driven learning approach is adopted to decrease the memory requirement by keeping only positive lexical entries and makes it possible to further incorporate more context-dependent lexical entries . Experiments show that this technique achieves overall precision and recall rates of 93.40% and 93.95% for all chunk types , 93.60% and 94.64% for noun phrases , and 94.64% and 94.75% for verb phrases when trained on PENN WSJ TreeBank section 00-19 and tested on section 20-24, while 25-fold validation experiments of PENN WSJ TreeBank show overall precision and recall rates of 96.40% and 96.47% for all chunk types , 96.49% and 96.99% for noun phrases , and 97.13% and 97.36% for verb phrases .







One Sense Per Collocation And Genre / Topic Variations


This paper revisits the one sense per collocation hypothesis using fine-grained sense distinctions and two different corpora . We show that the hypothesis is weaker for fine-grained sense distinctions (70% vs. 99% reported earlier on 2-way ambiguities ). We also show that one sense per collocation does hold across corpora , but that collocations vary from one corpus to the other, following genre and topic variations . This explains the low results when performing word sense disambiguation across corpora . In fact, we demonstrate that when two independent corpora share a related genre / topic , the word sense disambiguation results would be better. Future work on word sense disambiguation will have to take into account genre and topic as important parameters on their models .







Multidimensional Transformation- Based Learning


This paper presents a novel method that allows a machine learning algorithm following the transformation-based learning paradigm ( Brill, 1995 ) to be applied to multiple classification tasks by training jointly and simultaneously on all fields . The motivation for constructing such a system stems from the observation that many tasks in natural language processing are naturally composed of multiple subtasks which need to be resolved simultaneously; also tasks usually learned in isolation can possibly benefit from being learned in a joint framework , as the signals for the extra tasks usually constitute inductive bias . The proposed algorithm is evaluated in two experiments : in one, the system is used to jointly predict the part-of-speech and text chunks /baseNP chunks of an English corpus ; and in the second it is used to learn the joint prediction of word segment boundaries and part-of-speech tagging for Chinese . The results show that the simultaneous learning of multiple tasks does achieve an improvement in each task upon training the same tasks sequentially. The part-of-speech tagging result of 96.63% is state-of-the-art for individual systems on the particular train / test split.






Contrast And Variability In Gene Names


We studied contrast and variability in a corpus of gene names to identify potential heuristics for use in performing entity identification in the molecular biology domain . Based on our findings , we developed heuristics for mapping weakly matching gene names to their official gene names . We then tested these heuristics against a large body of Medline abstracts , and found that using these heuristics can increase recall , with varying levels of precision . Our findings also underscored the importance of good information retrieval and of the ability to disambiguate between genes , proteins , RNA, and a variety of other referents for performing entity identification with high precision .







XML-Based NLP Tools For Analysing And Annotating Medical Language

We describe the use of a suite of highly flexible xml-based nlp tools in a project for processing and interpreting text in the medical domain . The main aim of the paper is to demonstrate the central role that xml mark-up and xml nlp tools have played in the analysis process and to describe the resultant annotated corpus of medline abstracts . In addition to the xml tools , we have succeeded in integrating a variety of non-xml 'off the shelf' nlp tools into our pipelines , so that their output is added into the mark-up. We demonstrate the utility of the annotations that result in two ways. First, we investigate how they can be used to improve parse coverage ofa hand-crafted grammar that generates logical forms . And second, we investigate how they contribute to automatic lexical semantic acquisition processes .







Reducing Parameter Space For Word Alignment

This paper presents the experimental results of our attemps to reduce the size of the parameter space in word alignment algorithm . We use IBM Model 4 as a baseline. In order to reduce the parameter space , we pre-processed the training corpus using a word lemmatizer and a bilingual term extraction algorithm . Using these additional components , we obtained an improvement in the alignment error rate .






Extracting And Evaluating General World Knowledge From The Brown Corpus

"We have been developing techniques for extracting general world knowledge from miscellaneous texts by a process of approximate interpretation and abstraction , focusing initially on the Brown corpus . We apply interpretive rules to clausal patterns and patterns of modification , and concurrently abstract general ""possi-bilistic"" propositions from the resulting formulas . Two examples are ""A person may believe a proposition"", and ""Children may live with relatives "". Our methods currently yield over 117,000 such propositions (of variable quality ) for the Brown corpus (more than 2 per sentence ). We report here on our efforts to evaluate these results with a judging scheme aimed at determining how many ofthese propositions pass muster as ""reasonable general claims "" about the world in the opinion of humanjudges. We find that nearly 60% of the extracted propositions are favorably judged according to our scheme by any given judge . The percentage unanimously judged to be reasonable claims by multiple judges is lower, but still sufficiently high to suggest that our techniques may be of some use in tackling the long-standing ""knowledge acquisition bottleneck"" in AI. "







How To Get A Chinese Name ( Entity ): Segmentation And Combination Issues


When building a Chinese named entity recognition system , one must deal with certain language-specific issues such as whether the model should be based on characters or words . While there is no unique answer to this question , we discuss in detail advantages and disadvantages of each model , identify problems in segmentation and suggest possible solutions , presenting our observations , analysis , and experimental results . The second topic of this paper is classifier combination . We present and describe four classifiers for Chinese named entity recognition and describe various methods for combining their outputs . The results demonstrate that classifier combination is an effective technique of improving system performance : experiments over a large annotated corpus of fine-grained entity types exhibit a 10% relative reduction in F-measure error .







Improving Summarization Performance By Sentence Compression - A Pilot Study

In this paper we study the effectiveness of applying sentence compression on an extraction based multi-document summarization system . Our results show that pure syntactic-based compression does not improve system performance . Topic signature-based reranking of compressed sentences does not help much either. However reranking using an oracle showed a significant improvement remains possible. Keywords: Text Summarization , Sentence Extraction , Sentence Compression , Evaluation .






Noun Phrase Coreference As Clustering


This paper introduces a new, unsupervised algorithm for noun phrase coreference resolution . It differs from existing methods in that it views coreference resolution as a clustering task . In an evaluation on the MUC-6 coreference resolution corpus , the algorithm achieves an F-measure of 53.6%, placing it firmly between the worst (40%) and best (65%) systems in the MUC-6 evaluation . More importantly, the clustering approach outperforms the only MUC-6 system to treat coreference resolution as a learning problem . The clustering algorithm appears to provide a flexible mechanism for coordinating the application of context-independent and context-dependent constraints and preferences for accurate partitioning of noun phrases into coreference equivalence classes .







Psychocomputational Linguistics : A Gateway to the Computational Linguistics Curriculum

Computational modeling of human language processes is a small but growing subfield of computational linguistics . This paper describes a course that makes use of recent research in psychocomputational modeling as a framework to introduce a number of mainstream computational linguistics concepts to an audience of linguistics , cognitive science and computer science doctoral students. The emphasis on what I take to be the largely interdisciplinary nature of computational linguistics is particularly germane for the computer science students. Since 2002  the course has been taught three times under the auspices of the MA/PhD program in Linguistics at The City University of New York's Graduate Center . A brief description of some of the students' experiences after having taken the course is also provided .







A Plan- Based Analysis Of Indirect Speech Act

We propose an account of indirect forms of speech acts to request and inform based on the hypothesis that language users can recognize actions being performed by others, infer goals being sought, and cooperate in their achievement. This cooperative behaviour is independently motivated and may or may not be intended by speakers. If the hearer believes it is intended, he or she can recognize the speech act as indirect; otherwise it is interpreted directly. Heuristics are suggested to decide among the interpretations .







Ambiguous Noun Phrases In Logical Form

	"Schubert , Lenhart K.; Pelletier , Francis Jeffry ,From English To Logic : Context- Free Computation Of ""Conventional"" Logical Translation ,American Journal Of Computation al Linguistics ,1982 *** Hobbs , Jerry R. ,An Improper Treatment Of Quantification In Ordinary English ,Annual Meeting Of The Association For Computation al Linguistics ,1983 *** Pollack , Martha E. ; Pereira , Fernando ,An Integrated Framework For Semantic And Pragmatic Interpretation ,Annual Meeting Of The Association For Computation al Linguistics ,1988 *** Alshawi , Hiyan ; Van Eijck , Jan,Logical Forms In The Core Language Engine ,Annual Meeting Of The Association For Computation al Linguistics ,1989 "








Extracting The Lowest- Frequency Words : Pitfalls And Possibilities


	Church, Kenneth Ward ; Hanks , Patrick , Word Association Norms , Mutual Information , And Lexicography, Computation al Linguistics ,1990 *** Dunning , Ted E. ,Accurate Methods For The Statistics Of Surprise And Coincidence, Computation al Linguistics ,1993 *** Smadja , Frank A. ,Retrieving Collocations From Text : Xtract, Computation al Linguistics ,1993





Word Sense Disambiguation using Optimised Combinations of Knowledge Sources 
Word sense disambiguation algorithms , with few exceptions , have made use of only one lexical knowledge source . We describe a system which performs word sense disambiguation on all content words in free text by combining different knowledge sources : semantic preferences , dictionary definitions and subject/ domain codes along with part-of-speech tags , optimised by means of a learning algorithm . We also describe the creation of a new sense tagged corpus by combining existing resources . Tested accuracy of our approach on this corpus exceeds 92% , demonstrating the viability of all-word disambiguation rather than restricting oneself to a small sample .







GRAPH: The Costs of Redundancy in Referring Expressions


We describe a graph-based generation system that participated in the Tuna attribute selection and realisation task of the reg 2008 Challenge . Using a stochastic cost function (with certain properties for free), and trying attributes from cheapest to more expensive, the system achieves overall .76 dice and .54 masi scores for attribute selection on the development set. For realisation , it turns out that in some cases higher attribute selection accuracy leads to larger differences between system-generated and human descriptions .







Colouring Summaries BLEU


In this paper we attempt to apply the IBM algorithm , BLEU, to the output of four different summarizers in order to perform an intrinsic evaluation of their output . The objective of this experiment is to explore whether a metric , originally developed for the evaluation of machine translation output , could be used for assessing another type of output reliably. Changing the type of text to be evaluated by BLEU into automatically generated extracts and setting the conditions and parameters of the evaluation experiment according to the idiosyncrasies of the task , we put the feasibility of porting BLEU in different Natural Language Processing research areas under test . Furthermore, some important conclusions relevant to the resources needed for evaluating summaries have come up as a side-effect of running the whole experiment .






Automatic Prosodic Labeling with Conditional Random Fields and Rich Acoustic Features

Many acoustic approaches to prosodic labeling in English have employed only local classifiers , although text-based classification has employed some sequential models . In this paper we employ linear chain and factorial conditional random fields (CRFs) in conjunction with rich, contextually-based prosodic features , to exploit sequential dependencies and to facilitate integration with lexical features . Integration of lexical and prosodic features improves pitch accent prediction over either feature set alone, and for lower accuracy feature sets , factorial CRF models can improve over linear chain based prediction of pitch accent.







WSD Based On Mutual Information And Syntactic Patterns

This paper describes a hybrid system for WSD, presented to the English all-words and lexical-sample tasks , that relies on two different unsupervised approaches . The first one selects the senses according to mutual information proximity between a context word a variant of the sense . The second heuristic analyzes the examples of use in the glosses of the senses so that simple syntactic patterns are inferred. This patterns are matched against the disambiguation contexts . We show that the first heuristic obtains a precision and recall of .58 and .35 respectively in the all words task while the second obtains .80 and .25. The high precision obtained recommends deeper research of the techniques . Results for the lexical sample task are also provided .







Joining Forces To Resolve Lexical Ambiguity : East Meets West In Barcelona


"This paper describes the component models and combination model built as a joint effort between Swarthmore College, Hong Kong PolyU, and HKUST. Though other models described elsewhere contributed to the final combination model , this paper focuses solely on the joint contributions to the ""Swat-HK"" effort . "






Semantic Forensics : An Application Of Ontological Semantics To Information Assurance


"The paper deals with the latest application of natural language processing (NLP), specifically of ontological semantics (ONSE) to natural language information assurance and security (NL IAS). It demonstrates how the existing ideas, methods , and resources of ontological semantics can be applied to detect deception in NL text (and, eventually, in data and other media as well). After stating the problem , the paper proceeds to a brief introduction to ONSE, followed by an equally brief survey of our 5-year-old effort in ""colonizing"" IAS. The main part of the paper deals with the following issues : 
 human deception detection abilities and NLP modeling of it; 
 manipulation of fact repositories for this purpose beyond the current state of the art; 
 acquisition of scripts for complex ontological concepts ; 
 degrees of lying complexity and feasibility of their automatic detection . This is not a report on a system implementation but rather an application-establishing proof-of-concept effort based on the algorithmic and machine-tractable recombination and extension of the previously implemented ONSE modules . The strength of the approach is that it emphasizes the use of the existing NLP applications , with very few domain- and goal-specific adjustments, in a most promising and growing new area of IAS. So, while clearly dealing with a new application , the paper addresses theoretical and methodological extensions of ONSE, as defined currently, that will be useful for other applications as well. 1"







Exploiting Context For Biomedical Entity Recognition : From Syntax To The Web


We describe a machine learning system for the recognition of names in biomedical texts . The system makes extensive use of local and syntactic features within the text , as well as external resources including the web and gazetteers . It achieves an F-score of 70% on the Coling 2004  NLPBA/BioNLP shared task of identifying five biomedical named entities in the GENIA corpus .







Biomedical Named Entity Recognition Using Conditional Random Fields And Rich Feature Sets


As the wealth of biomedical knowledge in the form of literature increases , there is a rising need for effective natural language processing tools to assist in organizing, curating, and retrieving this information . To that end, named entity recognition (the task of identifying words and phrases in free text that belong to certain classes of interest) is an important first step for many of these larger information management goals . In recent years, much attention has been focused on the problem of recognizing gene and protein mentions in biomedical abstracts . This paper presents a framework for simultaneously recognizing occurrences of PROTEIN , DNA, RNA, CELL-LINE, CELL-TYPE F1









Audio Hot Spotting And Retrieval Using Multiple Features

This paper reports our on-going efforts to exploit multiple features derived from an audio stream using source material such as broadcast news , teleconferences, and meetings . These features are derived from algorithms including automatic speech recognition , automatic speech indexing , speaker identification , prosodic and audio feature extraction . We describe our research prototype - the Audio Hot Spotting System -that allows users to query and retrieve data from multimedia sources utilizing these multiple features . The system aims to accurately find segments of user interest, i.e., audio hot spots within seconds of the actual event . In addition to spoken keywords , the system also retrieves audio hot spots by speaker identity, word spoken by a specific speaker, a change of speech rate , and other non-lexical features , including applause and laughter. Finally, we discuss our approach to semantic , morphological, phonetic query expansion to improve audio retrieval performance and to access cross-lingual data .






Error Detection And Recovery In Spoken Dialogue Systems

"This paper describes our research on both the detection and subsequent resolution of recognition errors in spoken dialogue systems . The paper consists of two major components . The first half concerns the design of the error detection mechanism for resolving city names in our mercury flight reservation system , and an investigation of the behavioral patterns of users in subsequent subdialogues involving keypad entry for disambiguation . An important observation is that, upon a request for keypad entry , users are frequently unresponsive to the extent of waiting for a time-out or hanging up the phone . The second half concerns a pilot experiment investigating the feasibility of replacing the solicitation of a keypad entry with that of a ""speak-and-spell "" entry . A novelty of our work is the introduction of a speech synthesizer to simulate the user , which facilitates development and evaluation of our proposed strategy . We have found that the speak-and-spell strategy is quite effective in simulation mode , but it remains to be tested in real user dialogues . "






Part Of Speech Tagging For Amharic Using Conditional Random Fields


We applied Conditional Random Fields (CRFs) to the tasks of Amharic word segmentation and POS tagging using a small annotated corpus of 1000 words . Given the size of the data and the large number of unknown words in the test corpus (80%), an accuracy of 84% for Amharic word segmentation and 74% for POS tagging is encouraging, indicating the applicability of CRFs for a morphologically complex language like Amharic.






Association- Based Bilingual Word Alignment

Bilingual word alignment forms the foundation of current work on statistical machine translation . Standard word-alignment methods involve the use of probabilistic generative models that are complex to implement and slow to train . In this paper we show that it is possible to approach the alignment accuracy of the standard models using algorithms that are much faster, and in some ways simpler, based on basic word-association statistics .







Mining Atomic Chinese Abbreviation Pairs: A Probabilistic Model For Single Character Word Recovery

"An HMM-based Single Character Recovery (SCR) Model is proposed in this paper to extract a large set of ""atomic abbreviation pairs "" from a large text corpus . By an ""atomic abbreviation pair ,"" it refers to an abbreviated word and its root word (i.e., unabbreviated form ) in which the abbreviation is a single Chinese character. This task is interesting since the abbreviation process for Chinese compound words seems to be ""compositional""; in other words , one can often decode an abbreviated word , such as ""nX"" (Taiwan University ), character-by-character back to its root form . With a large atomic abbreviation dictionary , one may be able to recover multiple-character abbreviations more easily. With only a few training iterations , the acquisition accuracy of the proposed SCR model achieves 62% and 50 % precision for training set and test set , respectively, from the ASWSC-2001 corpus . "







Towards A Validated Model For Affective Classification Of Texts

In this paper , we present the results of experiments aiming to validate a two-dimensional typology of affective states as a suitable basis for affective classification of texts . Using a corpus of English weblog posts, annotated for mood by their authors, we trained support vector machine binary classifiers to distinguish texts on the basis of their affiliation with one region of the space . We then report on experiments which go a step further, using four-class classifiers based on automated scoring of texts for each dimension of the typology . Our results indicate that it is possible to extend the standard binary sentiment analysis (positive/negative) approach to a two dimensional model (positive/negative; active/passive), and provide some evidence to support a more fine-grained classification along these two axes.






Automatic Dating Of Documents And Temporal Text Classification

The frequency of occurrence of words in natural languages exhibits a periodic and a non-periodic component when analysed as a time series . This work presents an unsupervised method of extracting periodicity information from text , enabling time series creation and filtering to be used in the creation of sophisticated language models that can discern between repetitive trends and non-repetitive writing patterns . The algorithm performs in O(n






Evaluation Of Several Phonetic Similarity Algorithms On The Task Of Cognate Identification

We investigate the problem of measuring phonetic similarity , focusing on the identification of cognates, words of the same origin in different languages . We compare representatives of two principal approaches to computing phonetic similarity : manually-designed metrics , and learning algorithms . In particular, we consider a stochastic transducer, a Pair HMM, several DBN models , and two constructed schemes . We test those approaches on the task of identifying cognates among Indoeuropean languages , both in the supervised and unsupervised context . Our results suggest that the averaged context DBN model and the Pair HMM achieve the highest accuracy given a large training set of positive examples .







Multilingual Deep Lexical Acquisition For HPSGs Via Supertagging


We propose a conditional random field-based method for supertagging, and apply it to the task of learning new lexical items for HPSG-based precision grammars of English and Japanese . Using a pseudo-likelihood approximation we are able to scale our model to hundreds of supertags and tens-of-thousands of training sentences . We show that it is possible to achieve start-of-the-art results for both languages using maximally language-independent lexical features . Further, we explore the performance of the models at the type- and token-level , demonstrating their superior performance when compared to a unigram-based baseline and a transformation-based learning approach .







Paraphrasing Of Japanese Light- Verb Constructions Based On Lexical Conceptual Structure

Some particular classes of lexical paraphrases such as verb alteration and compound noun decomposition can be handled by a handful of general rules and lexical semantic knowledge . In this paper , we attempt to capture the regularity underlying these classes of paraphrases, focusing on the paraphrasing of Japanese light-verb constructions (LVCs). We propose a paraphrasing model for LVCs that is based on transforming the Lexical Conceptual Structures (LCSs) of verbal elements. We also propose a refinement of an existing LCS dictionary . Experimental results show that our LCS-based paraphrasing model characterizes some of the semantic features of those verbs required for generating paraphrases, such as the direction of an action and the relationship between arguments and surface cases .







A Standoff Annotation Interface Between DELPH-In Components


We present a standoff annotation framework for the integration of NLP components , currently implemented in the context of the DELPH-IN tools






Lexicalized Tree Automata- Based Grammars For Translating Conversational Texts

We propose a new lexicalized grammar formalism called Lexicalized Tree Automata-based Grammar, which lexicalizes tree acceptors instead of trees themselves. We discuss the properties of the grammar and present a chart parsing algorithm . We have implemented a translation module for conversational texts using this formalism , and applied it to an experimental automatic interpretation system (speech translation system ).






Induction Of Greedy Controllers For Deterministic Treebank Parsers


Most statistical parsers have used the grammar induction approach , in which a stochastic grammar is induced from a treebank. An alternative approach is to induce a controller for a given parsing automaton. Such controllers may be stochastic; here, we focus on greedy controllers, which result in deterministic parsers . We use decision trees to learn the controllers. The resulting parsers are surprisingly accurate and robust , considering their speed and simplicity . They are almost as fast as current part-of-speech taggers, and considerably more accurate than a basic unlexicalized PCFG parser . We also describe Markov parsing models , a general framework for parser modeling and control , of which the parsers reported here are a special case .







Classifying The Semantic Relations In Noun Compounds Via A Domain- Specific Lexical Hierarchy


We are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames , but more general than those used in traditional knowledge representation systems ). In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds. We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances , performing better on previously unseen words than a baseline consisting of training on the words themselves.






Chunk- Level Reordering of Source Language Sentences with Automatically Learned Rules for Statistical Machine Translation

In this paper , we describe a source-side reordering method based on syntactic chunks for phrase-based statistical machine translation . First, we shallow parse the source language sentences . Then, reordering rules are automatically learned from source-side chunks and word alignments . During translation , the rules are used to generate a reordering lattice for each sentence . Experimental results are reported for a Chinese-to- English task , showing an improvement of 0.5%-1.8% BLEU score absolute on various test sets and better computational efficiency than reordering during decoding. The experiments also show that the reordering at the chunk-level performs better than at the POS-level .






Generating Lexical Analogies Using Dependency Relations

A lexical analogy is a pair of word-pairs that share a similar semantic relation . Lexical analogies occur frequently in text and are useful in various natural language processing tasks . In this study , we present a system that generates lexical analogies automatically from text data . Our system discovers semantically related pairs of words by using dependency relations , and applies novel machine learning algorithms to match these word-pairs to form lexical analogies . Empirical evaluation shows that our system generates valid lexical analogies with a precision of 70% , and produces quality output although not at the level of the best human-generated lexical analogies .







An Optimal Order of Factors for the Computational Treatment of Personal Anaphoric Devices in Urdu Discourse

Handling of human language by computer is a very intricate and complex task . In natural languages , sentences are usually part of discourse units just as words are part of sentences . Anaphora resolution plays a significant role in discourse analysis for chopping larger discourse units into smaller ones. This process is done for the purpose of better understanding and making easier the further processing of text by computer . This paper is focused on the discussion of various factors and their optimal order that play an important role in personal anaphora resolution in Urdu. Algorithms are developed that resolves pronominal anaphoric devices with 77-80% success rate .







An Effective Hybrid Machine Learning Approach for Coreference Resolution

We present a hybrid machine learning approach for coreference resolution . In our method , we use CRFs as basic training model , use active learning method to generate combined features so as to make existed features used more effectively; at last, we proposed a novel clustering algorithm which used both the linguistics knowledge and the statistical knowledge . We built a coreference resolution system based on the proposed method and evaluate its performance from three aspects : the contributions of active learning ; the effects of different clustering algorithms ; and the resolution performance of different kinds of NPs. Experimental results show that additional performance gain can be obtained by using active learning method ; clustering algorithm has a great effect on coreference resolution 's performance and our clustering algorithm is very effective; and the key of coreference resolution is to improve the performance of the normal noun 's resolution , especially the pronoun's resolution .







Generalizing Tree Transformations for Inductive Dependency Parsing

Previous studies in data-driven dependency parsing have shown that tree transformations can improve parsing accuracy for specific parsers and data sets. We investigate to what extent this can be generalized across languages /treebanks and parsers , focusing on pseudo-projective parsing , as a way of capturing non-projective dependencies , and transformations used to facilitate parsing of coordinate structures and verb groups. The results indicate that the beneficial effect of pseudo-projective parsing is independent of parsing strategy but sensitive to language or treebank specific properties . By contrast , the construction specific transformations appear to be more sensitive to parsing strategy but have a constant positive effect over several languages .






Construction of Domain Dictionary for Fundamental Vocabulary 
Guthrie , Joe A. ; Guthrie , Louise ; Aidinejad, Homa; Wilks , Yorick,Subject-Dependent Co- Occurrence And Word Sense Disambiguation ,Annual Meeting Of The Association For Computation al Linguistics ,1991





Extracting a Representation from Text for Semantic Analysis

We present a novel fine-grained semantic representation of text and an approach to constructing it. This representation is largely extractable by today's technologies and facilitates more detailed semantic analysis . We discuss the requirements driving the representation , suggest how it might be of value in the automated tutoring domain , and provide evidence of its validity .







KnowNet: Building a Large Net of Knowledge from the Web


This paper presents a new fully automatic method for building highly dense and accurate knowledge bases from existing semantic resources . Basically, the method uses a wide-coverage and accurate knowledge-based Word Sense Disambiguation algorithm to assign the most appropriate senses to large sets of topically related words acquired from the web. KnowNet, the resulting knowledge-base which connects large sets of semantically-related concepts is a major step towards the autonomous acquisition of knowledge from raw corpora . In fact, KnowNet is several times larger than any available knowledge resource encoding relations between synsets, and the knowledge KnowNet contains outperform any other resource when is empirically evaluated in a common framework .







A Two-Stage Parser for Multilingual Dependency Parsing

We present a two-stage multilingual dependency parsing system submitted to the Multilingual Track of CoNLL-2007. The parser first identifies dependencies using a deterministic parsing method and then labels those dependencies as a sequence labeling problem . We describe the features used in each stage. For four languages with different values of ROOT, we design some special features for the ROOT labeler. Then we present evaluation results and error analyses focusing on Chinese .






Parsing The Wall Street Journal Using A Lexical- Functional Grammar And Discriminative Estimation Techniques

We present a stochastic parsing system consisting of a Lexical- Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model . We report on the results of applying this system to parsing the UPenn Wall Street Journal (WSJ) treebank. The model combines full and partial parsing techniques to reach full grammar coverage on unseen data . The treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models . Disambiguation performance is evaluated by measuring matches of predicate-argument relations on two distinct test sets . On a gold standard of manually annotated f-structures for a subset of the WSJ treebank, this evaluation reaches 79% F-score. An evaluation on a gold standard of dependency relations for Brown corpus data achieves 76% F-score.







Thumbs Up Or Thumbs Down? Semantic Orientation Applied To Unsupervised Classification Of Reviews


This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended not recommended semantic orientation






Self-Organizing Markov Models And Their Application To Part- Of- Speech Tagging

This paper presents a method to develop a class of variable memory Markov models that have higher memory capacity than traditional (uniform memory ) Markov models . The structure of the variable memory models is induced from a manually annotated corpus through a decision tree learning algorithm . A series of comparative experiments show the resulting models outperform uniform memory Markov models in a part-of-speech tagging task .







Unsupervised Sense Disambiguation Using Bilingual Probabilistic Models

We describe two probabilistic models for unsupervised word-sense disambiguation using parallel corpora . The first model , which we call the Sense model , builds on the work of Diab and Resnik (2002) that uses both parallel text and a sense inventory for the target language , and recasts their approach in a probabilistic framework . The second model , which we call the Concept model , is a hierarchical model that uses a concept latent variable to relate different language specific sense labels. We show that both models improve performance on the word sense disambiguation task over previous unsu-pervised approaches , with the Concept model showing the largest improvement . Furthermore, in learning the Concept model , as a by-product , we learn a sense inventory for the parallel language .







Adaptive Chinese Word Segmentation

This paper presents a Chinese word segmentation system which can adapt to different domains and standards . We first present a statistical framework where domain-specific words are identified in a unified approach to word segmentation based on linear models . We explore several features and describe how to create training data by sampling . We then describe a transformation-based learning method used to adapt our system to different word segmentation standards . Evaluation of the proposed system on five test sets with different standards shows that the system achieves stateof-the-art performance on all of them.







Multimodal Database Access On Handheld Devices


We present the final MIAMM system , a multimodal dialogue system that employs speech , haptic interaction and novel techniques of information visualization to allow a natural and fast access to large multimedia databases on small handheld devices .






Corpus- Oriented Development Of Japanese HPSG Parsers


This paper reports the corpus-oriented development of a wide-coverage Japanese HPSG parser . We first created an HPSG treebank from the EDR corpus by using heuristic conversion rules , and then extracted lexical entries from the tree-bank . The grammar developed using this method attained wide coverage that could hardly be obtained by conventional manual development . We also trained a statistical parser for the grammar on the tree-bank , and evaluated the parser in terms of the accuracy of semantic-role identification and dependency analysis .







Man-Assisted Machine Construction Of A Semantic Dictionary For Natural Language Processing

This is a report on the semantic dictionary for natural language processing we are constructing now.   This paper explains how to obtain the semantic information for the dictionary from an ordinary Japanese language dictionary with about 60,000 items (which had already been put into machine readable form ) and also explains what should be the frame for the representation of meaning of each item ( word ).   Then a man-assisted machine procedure that embeds the semantic graph with respect to the head word of the ordinary dictionary into the frame of a head word is discussed.






Strategies For Interactive Machine Translation : The Experience And Implications Of The UMIST Japanese Project

At the Contre for Computational Linguistics , we are designing and implementing an Eng.lish-to- Japanese interactive machine translation system . The project is funded jointly by the Alvey Directorate and International Computers Limited (ICL). The prototype system runs on the ICL PERQ, though much of the development work has been done on a VAX 11/750. It is implemented in Prolog, in the interests of rapid prototyping , but intended for optimization . The informing principles are those of modern comp1 ex-feature-based linguistic theories , in particular Lexical- Functional Grammar ( Bresnan (ed.) 1982, Kaplan and  Bresnan 1982 ), and Generalized Phrase Structure Grammar ( Gazdar et al. 1985 ). For development purposes we are using an existing corpus of 10,000 words of continuous prose from the PERQ's graphics documentation ; in the long term , the system will be extended for use by technical writers in fields other than software . At the time of writing, we have well-developed system development software , user interface , and grammar and dictionary handling utilities . The English analysis grammar handles most of the syntactic structures of the corpus , and we have a range of formats for output of linguistic representations and Japanese text . A transfer grammar for English- Japanese has been prototyped, but is not not yet fully adequate to handle all constructions in the corpus ; a facility for dictionary entry in kanji is incorporated. The aspect of the system we will focus on in the present paper is its interactive nature , discussing the range of different types of interaction which are provided or permitted for different types of user .







Exploiting Lexical Regularities In Designing Natural Language Systems

This paper presents the lexical component of the START Question Answering system developed at the MIT Artificial Intelligence Laboratory . START is able to interpret correctly a wide range of semantic relationships associated with alternate expressions of the arguments of verbs . The design of the system takes advantage of the results of recent linguistic research into the structure of the lexicon , allowing START to attain a broader range of coverage than many existing systems while maintaining modular organization .






Reasons Why I Do Not Care Grammar Formalism

has borrowed a lot of ideas from We could not have developed even a simple parser without the research results in It is obviously nonsense to claim that we, computational linguists , do not care research results in However, the researchers in it seems to me, are very fond of especially, those who are called They always fight with each other by asserting that their are superior to the others'. They are oversensitive and tend to distinguish people into two groups, and A computational linguist using LFG (or LFG) as a small part in his total system is taken as the ally of LFG, and is certainly accused by the other groups. They promptly demonstrate that LFG is wrong, by showing a lot of peculiar sentences which rarely appear in real texts .Formalisms are prepared for accomplishing specific purposes . The formalisms in have been proposed , roughly speaking, for describing the of distinguishing from arbitrary /ramraa/ica/ sequences , and of relating the grammatical sequences with the other representational levels .On the other hand , a formalism we need in CL is for different purposes . That is, we need a formalism for describing the rules of distinguishing the most feasible grammatical structures from other less feasible but still ones of the same sentences We also need a formalism in which we can manage systematically a large amount of knowledge of various sorts necessary for NLP.Formalisms for different purposes , of course, should be evaluated based on different standards . The current discussions of different formalisms in TL are irrelevant to our standards , though they may be important for their The following is a list of the reasons why I think so.







Directing The Generation Of Living Space Descriptions


We have developed a Computational model of the process of describing the layout of an apartment or house, a much-studied discourse task first characterized linguistically by Linde (1974). The model is embodied in a program , APT, that can reproduce segments of actual tape-recorded descriptions , using organizational and discourse strategies derived through analysis of our corpus .






Topic / Focus Articulation And Intensional Logic

A semantic analysis of topic and focus as two parts of tectograamatical representation by means of transparent intensional logic (TIL) ia presented. It is pointed out that two sentences (more precisely, their tectograamatical representations ) differing just in the topic / focus articulation (TFA) denote different propositions, i.e. that TFA has an effect upon the semantic content of the sentence . An inforaal short description of an algorithm handling the TFA in the translation of tectogrammatical representations into the constructions of TIL is added. The TFA algorithm divides a representation into two parts corresponding to the topic and focus ; every part is analyzed ( translated ) in isolation and then the resulting construction is put together. The TIL construction discussed here







Bilingual Generation Of Weather Forecasts In An Operations Environment

In  1986  the first experiments in text generation applied to weather forecasts resulted in a prototype system (RAREAS[6,3]) for producing English marine bulletins from forecast data . Subsequent work in 1987 added French output to make the initial system bilingual (RAREAS-2[llj). During 1988 -1989 a full-scale operational system was created to meet the needs of daily marine forecast production for three regional centres in the Canadian Atmospheric Environment Service







Syllable- Based Morphology

This paper presents a language for the description of morphological alternations which is based on syllable structure . The justification for such an approach is discussed with reference to examples from a variety of languages and the approach is compared to Koskenniemi 's two-level account of morphonoiogy. Keywords: morphology , phonology , syllables.






Knowledge Acquisition And Chinese Parsing Based On Corpus

In Natural Language Processing (NLP), one key problem is how to design a robust and effective parsing system . In this paper , we will introduce a corpus based Chinese parsing system . Our efforts arc concctrated on: (1) knowledge acquisition and representation ; and (2) the parsing scheme . The knowledge of this system is principally extracted from analyzed corpus , others are a few grammatical principles , i.e. the four axioms of the Dependency Grammar (DG). In addition , we also propose the fifth axiom of DG to support the parsing of Chinese sentences .






Automatic Model Refinement - With An Application To Tagging 
Statistical NLP models usually only consider coarse information and very restricted context to make the estimation of parameters feasible. To reduce the modeling error introduced by a simplified probabilistic model , the Classification and Regression Tree (CART) method was adopted in this paper to select more discriminative features for automatic model refinement . Because the features are adopted dependently during splitting the classification tree in CART, the number of training data in each terminal node is small, which makes the labeling process of terminal nodes not robust . This over-tuning phenomenon cannot be completely removed by cross-validation process (i.e., pruning process ). A probabilistic classification model based on the selected discriminative features is thus proposed to use the training data more efficiently. In tagging the Brown Corpus , our probabilistic classification model reduces the error rate of the top 10 error dominant words from 5.71% to 4.35%, which shows 23.82% improvement over the unrefined model .






Generating Multilingual Documents From A Knowledge Base The TECHDOC Project

TECHDOC is an implemented system demonstrating the feasibility of generating multilingu.il technical documents on the basis of a language-independent knowledge base . Its application domain is use]- and maintenance instructions , which are produced from underlying plan structures representing the activities, the participating objects with their properties , relations , and so on. This paper gives a brief outline of the system architecture and discusses some recent developments in the project : the addition of actual event simulation in the KM, steps towards a document authoring tool , and a multimodal user interface .







A Reestimation Algorithm For Probabilistic Ttecursive Transition Network

Probabilistic. Recursive Transition Network (I'KTN) is an elevated version of RTN to model and process languages in stochastic, parameters . The representation is a direct derivation from the KTN and keeps much the spirit of Hidden Markov Model at the same time . We present a reestimation algorithm for PHTN that is a variation of Inside Outside algorithm that computes the values of the probabilistic parameters from sample sentences (parsed or uuparsed).







DCKR - Knowledge Representation In Prolog And Its Application To Natural Language Processing

"important tasks for natural language processing . Basin to semantic processing is descriptions of lexical items . The most frequently used form of description of lexical items is probably Frames or Objects. Therefore in what form Frames or Objects are expressed is a key issue for natural language processing . A method of the Object representation in Prolog called DCKR will be introduced. It will be seen that if part of general knowledge and a dictionary are described in DCKR, part of context-processing and the greater part of semantic processing can be left  to the functions built in Prolog. 09) se
( animal ,age : X,_) :- bottomof(S,B), sem(B,birthYear:Y,_), X is  1986 - Y. 10) seni(face,P,S) :- hasa(eye,P,[face I S]) ; hasa(nose,P,[face IS] ) ; has a(mou th,P,[face !S]). Now the meanings of the gem, isa and hasa predicates, which are important to descriptions in DCKR, are explained later using the DCKR examples given above. 1. I n troduct i on Relationships    between   knowledge represented predicate  logic formulas and knowledge represented Frames    or    Siruciured objects are clarified by [Hayes 80],     [ Nilsson 80],     CGoebel  85],[ Bowen 85], al,      but      their      methods    requires separately interpreter for  their representation . et an The authors have developed a knowledge representation form called DCKR (Definite Clause Knowledge Representation ) [ Koyama 85]. In DCKR, each of the sip_ts composing of a Structured Object (hereinafter simply called an object ) is represented by a Horn clause (a Prolog statement) with the ""sen "" predicate (to be explained in Section 2) as its head. Therefore, an Object can be regarded as a set of Horn clauses (slots) headed by the sen predicate with the same first argument . From the foregoing it follows that almost all of a program for performing semantic intepretations relative to lexical items described in DCKR can be replaced by functions built in Prolog. That is, most of programming efforts of semantic processing can be left to the functions built in Pro 1og. DCKR will be described in detail in Section 2. Section 3 will discuss applications of DCKR to semantic processing of natural languages . "







Improving Search Strategies An Experiment In Best-First Parsing

"Viewing the syntactic analysis of natural language as a search problem , the right choice of parsing strategy plays an important role in the performance of natural language parsers . After a motivation of the use of various heuristic criteria , a framework for defining and testing ' parsing strategies is presented. On this basis systematic tests on different parsing strategies have been performed , the results of which are dicussed. Generally ;hese tests show that a ""guided"" depth-oriented strategy gives a considerable reduction of search effort compared to the classical depth first strategy . "






Generating Multimodal Output- Conditions, Advantages And Problems

"In natural communication situations , multimodal referent specification is frequent and efficient. The linguistic component are deictic expressions , e.g. 'this' and 'here'. Extralinguistic devices in dialogs are different body movements, mainly pointing gestures . Their functional equivalent in texts are means like arrows and indices . This paper has two intentions. First, it discusses the advantages of multimodal reference in interhuman communication which motivate the integration of extralinguistic ""pointing"" devices into NL dialog systems . The generation of multimodal output poses specific problems , which have no counterpart in the analysis of multimodal input . The second part presents the strategy for generating multimodal output which has been developed within the framework of the XTRA system (a NL access system to expert systems ). XTRA allows the combination of verbal descriptions and pointing gestures in order to specify elements of the given visual context , i.e. a form displayed on the screen . The component POPEL generates referential expressions which may be accompanied by a pointing gesture . The appearance of these gestures depends on several factors , e.g. the type of referent (whether it is a region or an entry of the form ) and its complexity . "







Generalizing Local and Non-Local Word- Reordering Patterns for Syntax- Based Machine Translation 
Syntactic word reordering is essential for translations across different grammar structures between syntactically distant language-pairs . In this paper , we propose to embed local and non-local word reordering decisions in a synchronous context free grammar , and leverages the grammar in a chart-based decoder . Local word-reordering is effectively encoded in Hiero-like rules ; whereas non-local word-reordering , which allows for long-range movements of syntactic chunks , is represented in tree-based reordering rules , which contain variables correspond to source-side syntactic constituents . We demonstrate how these rules are learned from parallel corpora . Our proposed shallow Tree-to- String rules show significant improvements in translation quality across different test sets .






Automatic Acquisition of Basic Katakana Lexicon from a Given Corpus 
Abstract .







A Twin- Candidate Model of Coreference Resolution with Non-Anaphor Identification Capability 
Abstract .






The Influence of Data Homogeneity on NLP System Performance

In this work we study the influence of corpus homogeneity on corpus-based NLP system performance . Experiments are performed on both stochastic language models and an EBMT system translating from Japanese to English with a large bicorpus, in order to reassess the assumption that using only homogeneous data tends to make system performance go up. We describe a method to represent corpus homogeneity as a distribution of similarity coefficients based on a cross-entropic measure investigated in previous works. We show that beyond minimal sizes of training data the excessive elimination of heterogeneous data proves prejudicial in terms of both perplexity and translation quality : excessively restricting the training data to a particular domain may be prejudicial in terms of In- Domain system performance , and that heterogeneous, Out-of- Domain data may in fact contribute to better sytem performance .







Using Maximum Entropy to Extract Biomedical Named Entities without Dictionaries

Current NER approaches include : dictionary-based , rule-based , or machine learning . Since there is no consolidated nomenclature for most biomedical NEs, most NER systems relying on limited dictionaries or rules do not perform satisfactorily. In this paper , we apply Maximum Entropy (ME) to construct our NER framework . We represent shallow linguistic information as linguistic features in our ME model . On the GENIA 3.02 corpus , our system achieves satisfactory F-scores of 74.3% in protein and 70.0% overall without using any dictionary . Our system performs significantly better than dictionary-based systems . Using partial match criteria , our system achieves an F-score of 81.3%. Using appropriate domain knowledge to modify the boundaries , our system has the potential to achieve an F-score of over 80%.







Resolving Pronominal References in Chinese with the Hobbs Algorithm

"This study addresses pronominal anaphora resolution , including zero pronouns, in Chinese . A syntactic , rule-based pronoun resolution algorithm , the ""Hobbs algorithm "" was run on ""gold standard "" hand parses from the Penn Chinese Treebank. While first proposed for English , the algorithm counts for its success on two characteristics that Chinese and English have in common . Both languages are SVO, and both are fixed word order languages . No changes were made to adapt the algorithm to Chinese . The accuracy of the algorithm on overt, third-person pronouns at the matrix level was 77.6%, and the accuracy for resolving matrix-level zero pronouns was 73.3%. In contrast , the accuracy of the algorithm on pronouns that appeared in subordinate constructions was only 43.3%, providing support for Miltsakaki 's two-mechanism proposal for resolving inter- vs. "






Chinese Word Segmentation Based On Direct Maximum Entropy Model

	Och, Franz Josef ; Ney , Hermann ,Discriminative Training And Maximum Entropy Models For Statistical Machine Translation ,Annual Meeting Of The Association For Computation al Linguistics ,2002 *** Gao , Jianfeng ; Li , Mu; Huang , Changning,Improved Source- Channel Models For Chinese Word Segmentation ,Annual Meeting Of The Association For Computation al Linguistics ,2003






Linguistic Variation And Computation (Invited Talk)

Language variationists study how languages vary along geographical or social lines or along lines of age and gender . Variationist data is available and challenging , in particular for dialectology, the study of geographical variation , which will be the focus of this paper , although we present approaches we expect to transfer smoothly to the study of variation correlating with other extralinguistic variables . Techniques from computational linguistics on the one hand , and standard statistical data reduction techniques on the other, not only shed light on this classic linguistic problem , but they also suggest avenues for exploring the question at more abstract levels , and perhaps for seeking the determinants of variation .







A Method For Abstracting Newspaper Articles By Using Surface Clues


This paper describes a system which automatically creates an abstract of a newspaper article by selecting important sentences of a given text . To determine the importance of a sentence , several superficial features are considered, and weights for features are determined by multiple-regression analysis of a hand processed corpus .







XML And Multilingual Document Authoring: Convergent Trends


Typical approaches to XML authoring view a XML document as a mixture of structure (the tags ) and surface ( text between the tags ). We advocate a radical approach where the surface disappears from the XML document altogether to be handled exclusively by rendering mechanisms . This move is based on the view that the author's choices when authoring XML documents are best seen as language-neutral semantic decisions , that the structure can then be viewed as interlingual content , and that the textual output should be derived from this content by language-specific realization mechanisms , thus assimilating XML authoring to Multilingual Document Authoring. However, standard XML tools have important limitations when used for such a purpose : (1) they are weak at propagating semantic dependencies between different parts of the structure , and, (2) current XML rendering tools are ill-suited for handling the grammatical combination of textual units . We present two related proposals for overcoming these limitations : one (GF) originating in the tradition of mathematical proof editors and constructive type theory , the other (IG), a specialization of Definite Clause Grammars strongly inspired by GF.






Automatic Semantic Grouping In A Spoken Language User Interface Toolkit


With the rapid growth of real application domains for NLP systems , there is a genuine demand for a general toolkit from which programmers with no linguistic knowledge can build specific NLP systems . Such a toolkit should provide an interface to accept sample sentences and convert them into semantic representations so as to allow programmers to map them to domain actions . In order to reduce the workload of managing a large number of semantic forms individually, the toolkit will perform what we call semantic grouping to organize the forms into meaningful groups. In this paper , we present three semantic grouping methods : similarity-based , verb-based and category-based grouping, and their implementation in the SLUI toolkit. We also discuss the pros and cons of each method and how they can be utilized according to the different domain needs.







Paradigmatic Morphology

We present a notation for the declarative statement of morphological relationships and lexical rules , based on the traditional notion of Word and Paradigm Elsewhere Condition, string equations






Selective Magic HPSG Parsing

We propose a parser for constraint-logic grammars implementing HPSG that combines the advantages of dynamic bottom-up and advanced top-down control . The parser allows the user to apply magic compilation to specific constraints in a grammar which as a result can be processed dynamically in a bottom-up and goal-directed fashion . State of the art top-down processing techniques are used to deal with the remaining constraints . We discuss various aspects concerning the implementation of the parser as part of a grammar development system .







Representing Text Chunks


"Dividing sentences in chunks of words is a useful preprocessing step for parsing , information extraction and information retrieval . ( Ramshaw and  Marcus, 1995 ) have introduced a ""convenient""data representation for chunking by converting it to a tagging task . In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks . We will show that the the data representation choice has a minor influence on chunking performance . However, equipped with the most suitable data representation , our memory-based learning chunker was able to improve the best published chunking results for a standard data set."






Detection Of Japanese Homophone Errors By A Decision List Including A Written Word As A Default Evidence

In this paper , we propose a practical method to detect Japanese homophone errors in Japanese texts . It is very important to detect homophone errors in Japanese revision systems because Japanese texts suffer from homophone errors frequently. In order to detect homophone errors , we have only to solve the homophone problem . We can use the decision list to do it because the homophone problem is equivalent to the word sense disambiguation problem . However, the homophone problem is different from the word sense disambiguation problem because the former can use the written word but the latter cannot. In this paper , we incorporate the written word into the original decision list by obtaining the identifying strength of the written word . The improved decision list can raise the F-measure of error detection .







New Models For Improving Supertag Disambiguation

In previous work, supertag disambiguation has been presented as a robust partial parsing technique . In this paper we present two approaches : contextual models , which exploit a variety of features in order to improve supertag performance , and class-based models , which assign sets of supertags to words in order to substantially improve accuracy with only a slight increase in ambiguity .






Japanese Dependency Structure Analysis Based On Maximum Entropy Models

This paper describes a dependency structure analysis of Japanese sentences based on the maximum entropy models . Our model is created by learning the weights of some features from a training corpus to predict the dependency between bunsetsus or phrasal units . The dependency accuracy of our system is 87.2% using the Kyoto University corpus . We discuss the contribution of each feature set and the relationship between the number of training data and the accuracy .







Reranking And Self- Training For Parser Adaptation

"Statistical " parsers trained and tested on the Penn Wall Street Journal (wsj) treebank have shown vast improvements over the last 10 years. Much of this improvement , however, is based upon an ever-increasing number of features to be trained on (typically) the wsj treebank data . This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres . Such worries have merit. The standard ""Charniak parser "" checks in at a labeled precision-recall f-measure of 89.7% on the Penn WSJ test set , but only 82.9% on the test set from the Brown treebank corpus . This paper should allay these fears. In particular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%. Furthermore, use of the self-training techniques described in ( McClosky et al., 2006 ) raise this to 87.8% (an error reduction of 28%) again without any use of labeled Brown data . This is remarkable since training the parser and reranker on labeled Brown data achieves only 88.4%. "






Automatic Classification Of Verbs In Biomedical Texts 
Lexical classes , when tailored to the application and domain in question , can provide an effective means to deal with a number of natural language processing (nlp) tasks . While manual construction of such classes is difficult, recent research shows that it is possible to automatically induce verb classes from cross-domain corpora with promising accuracy . We report a novel experiment where similar technology is applied to the important, challenging domain of biomedicine. We show that the resulting classification , acquired from a corpus of biomedical journal articles, is highly accurate and strongly domain-specific . It can be used to aid bio-nlp directly or as useful material for investigating the syntax and semantics of verbs in biomedical texts .






Word Alignment In English- Hindi Parallel Corpus Using Recency- Vector Approach : Some Studies 
Word alignment using recency-vector based approach has recently become popular. One major advantage of these techniques is that unlike other approaches they perform well even if the size of the parallel corpora is small. This makes these algorithms worth-studying for languages where resources are scarce. In this work we studied the performance of two very popular recency-vector based approaches , proposed in ( Fung and  McKeown, 1994 ) and ( Somers, 1998 ), respectively, for word alignment in English - Hindi parallel corpus . But performance of the above algorithms was not found to be satisfactory. However, subsequent addition of some new constraints improved the performance of the recency-vector based alignment technique significantly for the said corpus . The present paper discusses the new version of the algorithm and its performance in detail .







Semi-Supervised Training For Statistical Word Alignment

We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus . We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality .







You Can't Beat Frequency (Unless You Use Linguistic Knowledge ) - A Qualitative Evaluation Of Association Measures For Collocation And Term Extraction

In the past years, a number of lexical association measures have been studied to help extract new scientific terminology or general-language collocations . The implicit assumption of this research was that newly designed term measures involving more sophisticated statistical criteria would outperform simple counts of cooccurrence frequencies . We here explicitly test this assumption . By way of four qualitative criteria , we show that purely statistics-based measures reveal virtually no difference compared with frequency of occurrence counts, while linguistically more informed metrics do reveal such a marked difference .







A Rote Extractor With Edit Distance- Based Generalisation And Multi- Corpora Precision Calculation
In this paper , we describe a rote extractor that learns patterns for finding semantic relationships in unrestricted text , with new procedures for pattern generalization and scoring. These include the use of part-of-speech tags to guide the generalization , Named Entity categories inside the patterns , an edit-distance-based pattern generalization algorithm , and a pattern accuracy calculation procedure based on evaluating the patterns on several test corpora . In an evaluation with 14 entities , the system attains a precision higher than 50% for half of the relationships considered.







An Empirical Study Of Chinese Chunking


In this paper , we describe an empirical study of Chinese chunking on a corpus , which is extracted from UPENN Chinese Treebank-4 (CTB4). First, we compare the performance of the state-of-the-art machine learning models . Then we propose two approaches in order to improve the performance of Chinese chunking . 1) We propose an approach to resolve the special problems of Chinese chunking . This approach extends the chunk tags for every problem by a tag-extension function . 2) We propose two novel voting methods based on the characteristics of chunking task . Compared with traditional voting methods , the proposed voting methods consider long distance information . The experimental results show that the SVMs model outperforms the other models and that our proposed approaches can improve performance significantly.







Soft Syntactic Constraints For Word Alignment Through Discriminative Training 
Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases specified by a monolingual dependency tree . However, this hard constraint can also rule out correct alignments , and its utility decreases as alignment models become more complex . We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint . The resulting aligner is the first, to our knowledge , to use a discriminative learning method to train an ITG bitext parser .







A Bio-Inspired Approach For Multi- Word Expression Extraction
This paper proposes a new approach for Multi-word Expression (MWE) extraction on the motivation of gene sequence alignment because textual sequence is similar to gene sequence in pattern analysis . Theory of Longest Common Subsequence (LCS) originates from computer science and has been established as affine gap model in Bioinformatics. We perform this developed LCS technique combined with linguistic criteria in MWE extraction . In comparison with traditional n-gram method , which is the major technique for MWE extraction , LCS approach is applied with great efficiency and performance guarantee. Experimental results show that LCS-based approach achieves better results than n-gram .







Graph Branch Algorithm : An Optimum Tree Search Method For Scored Dependency Graph With Arc Co- Occurrence Constraints


"Various kinds of scored dependency graphs are proposed as packed shared data structures in combination with optimum dependency tree search algorithms . This paper classifies the scored dependency graphs and discusses the specific features of the ""Dependency Forest "" (DF) which is the packed shared data structure adopted in the ""Preference Dependency Grammar "" (PDG), and proposes the ""Graph Branch Algorithm "" for computing the optimum dependency tree from a DF. This paper also reports the experiment showing the computational amount and behavior of the graph branch algorithm . "







Translating HPSG-Style Outputs Of A Robust Parser Into Typed Dynamic Logic

The present paper proposes a method by which to translate outputs of a robust HPSG parser into semantic representations of Typed Dynamic Logic (TDL), a dynamic plural semantics defined in typed lambda calculus . With its higher-order representations of contexts , TDL analyzes and describes the inherently inter-sentential nature of quantification and anaphora in a strictly lexicalized and compositional manner . The present study shows that the proposed translation method successfully combines robustness and descriptive adequacy of contemporary semantics . The present implementation achieves high coverage , approximately 90%, for the real text of the Penn Treebank corpus .







Adding Syntax To Dynamic Programming For Aligning Comparable Texts For The Generation Of Paraphrases


Multiple sequence alignment techniques have recently gained popularity in the Natural Language community , especially for tasks such as machine translation , text generation , and paraphrase identification . Prior work falls into two categories , depending on the type of input used: (a) parallel corpora (e.g., multiple translations of the same text ) or (b) comparable texts (non-parallel but on the same topic ). So far, only techniques based on parallel texts have successfully used syntactic information to guide alignments . In this paper , we describe an algorithm for incorporating syntactic features in the alignment process for non-parallel texts with the goal of generating novel paraphrases of existing texts . Our method uses dynamic programming with alignment decision based on the local syntactic similarity between two sentences . Our results show that syntactic alignment outrivals syntax-free methods by 20% in both grammatically and fidelity when computed over the novel sentences generated by alignment-induced finite state automata.







Clavius: Bi-Directional Parsing For Generic Multimodal Interaction

We introduce a new multi-threaded parsing algorithm on unification grammars designed specifically for multimodal interaction and noisy environments . By lifting some traditional constraints , namely those related to the ordering of constituents , we overcome several difficulties of other systems in this domain . We also present several criteria used in this model to constrain the search process using dynamically loadable scoring functions . Some early analyses of our implementation are discussed.







Is It Correct? - Towards Web- Based Evaluation Of Automatic Natural Language Phrase Generation

This paper describes a novel approach for the automatic generation and evaluation of a trivial dialogue phrases






A Model Of Natural Language Processing Of Time - Related Expressions









A Machine Translation System From Japanese Into English - Another Perspective Of MT Systems

A machine translation system from Japanese into English is described.    The system aims at translation of computer manuals , and basically follows to the transfer approach .    The design principles of the system are discussed in detail , together with the overall constructions of the system . Especially, the effectiveness of lexicon-based procedures , i.e. lexicon-based analysis , transfer , and synthesis , is emphasized.    Most of the linguistic phenomena are treated by using lexical descriptions and lexical rules , instead of by general syntactic rules .    Because Japanese and English belong to quite different language families, much more structural transfers are necessary than in other MT systems among European languages .    Special cares have been paid for designing the transfer component .    Some translation results are also given to illustrate the current abilities of the system .







Referential Nets With Attributes


One of the essential problems in natural language production and understanding is the problem of processing referential relations. In this paper I describe a model for representing and processing referential relations : referential nets with attributes. Both processes (analyzing and generating referential expressions ) are controlled by attributes. There are two types of attributes, on one hand , the ones to the internal substitutes of the objects spoken about, on the other hand , the ones to the descriptions of these objects .







A Multilayered Approach To The Handling Of Word Formation

The treatment of word formations has until recently been a neglected topic in natural  language AI research . This paper proposes a multilayered approach to word formation which treats derivatives and compounds on several different levels of processing within a natural language dialogue system . Analysis and generation strategies being developed for the dialogue system HAM-ANS are described. Identification of word format ions , semantic interpretation , and evaluation in the context of a dialogue are the main levels of analysis on which the system successively attempts to infer the implicit relations between word formation components . Generation of word formations is viewed as a process comparable to the generation of elliptical utterances .







The Anatomy Of A Systemic Choice 
Choice is one of the most prominent organizing concepts in systemic linguistics . Languages are described in terms of the choices available to the speaker and the relationships of those choices to each other and to the language produced. This paper addresses the problems of






Parsing German


The first part of this paper is dedicated to an overview of the parser of the system VIE-LANG (Viennese Language Understanding System ). The parser is a production system which uses an interleaved method that combines syntax and semantics . It parses directly into the internal representation of the system , without producing an. intermediate syntactic structure . The last part discusses the relationship between some special features of the German language , and properties of the parser that originate in the language .







Definite Noun Phrases And The Semantics Of Discourse 
Discourse Representation Theory (DRT), developed by Hans Kamp several years ago ( Kamp 1981 ), belongs, together with Irene Heims narrowly related File Change Semantics ( Heim 1982 ) and Situation Semantics (Barwise/ Perry 1983 ), to a group of theoretical approaches which in the early Eighties introduced a dynamic, context-oriented perspective into the semantics of natural language . This recent development in theoretical semantics indicates a shift of interest towards topics that have been familiar in natural language processing research for the last decade: among others, the interpretation of new utterances with respect to a given context , and integration of the utterance information into that context ; the step-by-step construction of representations for larger pieces of discourse ; the investigation of text coherence phenomena ; and the description of referential processes . The core of DRT (and File Change Semantics ) is the treatment of indefinite noun phrases as reference establishing terms (as opposed to their standard truth-conditional quantifier analysis , but in accordance with the treatmant of indefinites in NLP research ) and definite noun phrases (pronouns as well as full NPs) as anaphoric expressions . It is one of the theoretically most appealing features of these theories that they provide simple unified accounts for all indefinites, and for all definites, respectively. This theoretical simplicity stands however in sharp contrast to the complexity of the process of etablishing reference observed in NLP research , and the variety of phenomena and linguistic levels involved. On the one hand , this contrast is quite natural : As a semantically motivated theory , DRT should not be expected to incorporate every detail of inferencing necessary to come up with an interpretation for a specific utterance in a given context ; it can better be thought of as an interface relating theoretical, truth-conditional semantics and the genuinely pragmatic work of text understanding . On the other hand , if DRT is seriously intended to bridge the gap between theoretical linguistics and the NLP approach , it should take into consideration as many factual restrictions on NP reference , and distinctions among subtypes of referential expressions , as is possible in a systematic and descriptive way. Several extensions of the standard system are at work, e.g. for the treatment of plural and temporal anaphora. Little, however, has yet been done to arrive at a closer view of the analysis of (singular) definite noun phrases , once the basic concepts had been established. The only attempt I know about is by Kamp himself, described in Kamp (1983), an unpublished fragment . In this talk I will first give a short overview of the basic DRT system , and sketch Kamp 's proposal for the treatment of definite noun phrases . Then I will indicate how the basic reference establishing function and the 'side-effects 'of different types of definite NPs can be described in more detail . In doing this, I will refer to the work about anaphora done in the NLP area (esp. by Barbara Grosz , Candy Sidner , and Bonnie Webber ), integrating some of their assumptions into the DRT framework , and critically commenting on some others.







Towards The Automatic Acquisition Of Lexical Data

"Creating a knowledge base has always been a bottleneck in the implementation of AI systems . This is also true for Natural Language Understanding (NEU) systems , particularly for data-driven ones. While a perfect system for automatic acquisition of all sorts of knowledge is still feir from being realized, partial solutions are possible. This holds especially for lexical data . Nevertheless, the task is not trivial, in particular when dealing with languages rich in inflectional Horms like German. Our: system is to be used by persons with no specific linguistic knowledge , thus linguistic expertise has been put into the system to ascertain correct classification of words. Classification is done by means of a small rule based system with Lexical  knowledge and language-specific heuristics. The key idea is the identification of three sorts of knowledge which are processed distinctly and the optimal use of knowledge already contained in the existing Lexicon . 1
 Introduction in this paper we introduce a system for the semi-automatic enlargement of a morphological 1 ex icon . If forms part of VIE-BANG, a German Language dialogue system ( Buchberger et al. 1982 ) . VI K-l. ANG serves not only as an ob ject but as a meta system as we 11 : i ts knowledge base is to be enlarged, and its facilities are used L:o support that process : the parsor serves to analyze the i nput to the acquisj ti on system , the generator  i. s used  to provide exampl es. Ln contrast to English the morphological analysis of German words  is no trivial task ,  due to two causes: - First, there is a rich inflectional system , consisting of about 60 different endings (where most endings have various different interpretations ), some prefixes ('go-PPP, "






The PSI/PHI Architecture For Prosodic Parsing

In is in as a a






Optimization Algorithms Of Deciphering As The Elements Of A Linguistic Theory

This paper presents an outline of the linguistic theory which may be identified with the partially ordered set of optimization algorithms of deciphering. An algorithm of deciphering is the operational definition of a given linguistic phenomenon which han the following three components : a set of admissible solutions , an objective function and a procedure which finds out the ministurn or the maximum of the objective function . The paper contains the description of the four algorithms of the proposed type : 1. The algorithm which classifies the letters into vowels and consonants . 2. The algorithm which identifies the morphemes in the text without the boundaries between words . 3. The algorithm whioh finds out the dependency tree of a sentence .







Tenets For An Interlingual Representation Of Definite NPs


The main goal of this paper (as in Keenan and  Stavi 1986 ) is to characterize the possible determiner denotations in order to develop a computational approach that makes explicit use of this information . To cope with the constraints that languages impose when generating determiners , a computational model has to follow the laws that map d
finiteness to structures and strings and viceversa. In the following proposal I distantiate from K. B
hlers Deixis Theory and Weinrichs (76) proposal where indefinites suggest subsequent information , while definite point out facts from the previous information . This very general position is insufficient if we want to formalize NP-definiteness. The semantics of NP defmiteness must be captured adequately in computational frameworks for such tasks as answering quantified NL-- questions , or in a MT system to convert NPs from one language into another. In the first part of this paper I draw a typology of defmiteness ; later I reflect on the defmiteness of NPs in an IL-representation . The major result is given by the determiner generators . Defmiteness should be evaluated in a Q-A system and in MT. The extensive functionality    of   defmiteness    is first elaborated in the parsing and results in an IL-representation ; finally the determiner generators create correct morphological determiners and right determiner structures .






Lexical Gaps And Idioms In Machine Translation

This paper describes the treatment of lexical gaps , collocation information and idioms in the English to Portuguese machine translation system PORTUGA. The perspective is strictly bilingual, in the sense that all problems referenced above are considered to belong to the transfer phase , and not, as in other systems , to analysis or generation . The solution presented invokes a parser for the target language (Portuguese) that analyses , producing the corresponding graph structure , the multiword expression selected as the result of lexical transfer . This process seems to bring considerable advantage in what readability and ease of bilingual dictionary development is concerned , and to furnish maximal flexibility together with minimal storage requirements . Finally, it also provides complete independence between dictionary and grammar formalisms .







Intelligent Handling Of Weather Forecasts


Some typical cases of intelligent handling of weather forecasts such as translation , visualization , etc. are decomposed into two subprocesses analysis and synthesis . Specific techniques are presented for analysis and synthesis of weather forecast texts as well as for generation of weather maps . These techniques deal with the weather forecasts at different levels syntactic , discourse and semantic . They are based on a conceptual model underlying weather forecasts as well as on formal descriptions of the means of expression used in particular natural and cartographic sublanguages.







Genetic NPs And Habitual VPs


We propose a simple , intuitively satisfying treatment of the semantics of bare plural NPs . This treatment avoids the use of nonstandard logics , and avoids the need for systematic ambiguity of verb semantics .







Using Linguistic, World, And Contextual Knowledge In A Plan Recognition Model Of Dialogue
This paper presents a plan-based model of dialogue that combines world, linguistic, and contextual knowledge in order to recognize complex communicative actions such as expressing doubt. Linguistic knowledge suggests certain discourse acts, a speaker's beliefs, and the strength of those beliefs; contextual knowledge suggests the most coherent continuation of the dialogue ; and world knowledge provides evidence that the applicability conditions hold for those discourse acts that capture the relationship of the current utterance to the discourse as a whole.







Temporal Structure Of Discourse

In this paper discourse segments are defined and a method for discourse segmentation primarily based on abduction of temporal relations between segments is proposed . This method is precise and computationally feasible and is supported by previous work in the area of temporal anaphora resolution .







Organizing Dialogue From An Incoherent Stream Of Goals


their reasoning for structure their dialogues . Instead, computer-generated conversation must rely on some other mechanism for its organisation. In this paper , we discuss such mechanism . We describe provides a guide for conversation . The template is built from schemata representing discourse convention . As goals arrive from the problem solver they are added to the template . Because accepted discourse structures are used to connect a new goal to the existing template , goals are organised into sub-groups that follow conventional, coherent patterns of discourse . We present JUDIS , an interface to distributed problem solver that uses this approach to organise dialogues from incoherent of goals .







Preventing False Temporal Implicatures: Interactive Defaults For Text Generation

Given the causal and temporal relations between events in a knowledge base , what are the ways they can be described in text ? Elsewhere, we have argued that during interpretation , the reader-hearer // must infer certain temporal information from knowledge about the world, language use and pragmatics. It is generally agreed that processes of Gricean implicature help determine the interpretation of text in context . But without a notion of logical con se quo ii ce to underwrite them, the inferences
-often defeasible in nature - will appear arbitrary, and unprincipled. Hence, we have explored the requirements on a formal model of temporal implicature, and outlined one possible nonmonotonic framework for discourse interpretation (Lascarides &amp;; Asher [1991], Lascarides     Oberlander [1992a]). Here, we argue that if the writer-speaker 22077.






Design Tool Combining Keyword Analyzer And Case- Based Parser For Developing Natural Language Database Interfaces


We have designed and experimentally implemented a tool for developing a natural language systems that can accept extra-grammatical expressions , keyword sequences , and linguistic fragments , as well as ordinary natural language queries . The key to this tool 's efficiency is its effective use of a simple keyword analyzer in combination with a conventional case-based parser . The keyword analyzer performs a majority of those queries which are simple data retrievals . Since it uses only keywords in any query , this analyzer is robust with regard to extra-grammatical expressions . Since little labor is required of the application designer in using the keyword analyzer portion of the tool , and since the case-based parser processes only those queries which the keyword analyzer fails to interpret, total labor required of the designer is less than that for a tool which employs a conventional case-based parser alone.







A Robust Approach For Handling Oral Dialogues


Present limits of speech recognition and understanding in the context of free spoken language (altlwugh with a limited vocabulary ) have perverse effects on the flow of the dialogue with a system . Typically a non robust dialogue manager will fail to face with these limits and conversations will often be a failure. This paper presents some possibilities of a structural approach for handling communication failures in task-oriented oral dialogues . Several types of communication failures are presented and explained. They must be dealt with by the dialogue manager if we strike to have a robust system . The exposed strategies for handling these failures are based on a structural approach of the conversation and are implemented in the SUNDIAL system . We first recall some aspects of the model and then describe the strategies for preventing and repairing communication failure in oral conversations with a system .







Causal Ambiguity In Natural Language : Conceptual Representation Of'parce Que/because' And 'puisque/since'


This research deals with the representation of causal relations found in texts written in natural language , in order for KALIPSOS [1], an NL-understanding and question-answering system , to encode causal information in conceptual graphs so as to handle causal information and reasoning . Natural languages such as French or English have many ways to express a causal relation . It can be syntactic (parce que/because) {provoquer/to produce), (Je me suis cass
e la jambe el je n'ai pas pu venir/1 broke my leg and I couldn't come), parce que/because puisque/since parce que/because puisque/since







B-SURE: A Believed Situation And Uncertain- Action Representation Environment

Tliis paper presents a system that is capable of representing situations , states, and nondeterniinistic nonmonotonic-outcome actions occurring in multiple possible worlds. The system supports explicit representations of actions and situations used in intentional action theory and situation theory , l
oth types and instances are supported . Situations and states before and after nonmonotonic actions can be represented simultaneously. Agents have free will as to whether to choose to perform an action or not. Situations mid actions can have expected values, allowing the system to support decision-making and decision-based plan inferencing . The system can perform global reasoning simultaneously across multiple possible worlds, without being forced to extend each world explicitly. The resulting system is useful for Biich natural language tasks as plan recognition , intentions modeling , and parallel task scheduling.







An Architecture For A Universal Lexicon A Case Study On Shared Syntactic Information In Japanese , Hindi, Bengali , Greek , And English 
Pustejovsky , James ,The Generative Lexicon , Computation al Linguistics ,1991





Logic Compression Of Dictionaries For Multilingual Spelling Checkers


"To provide practical spelling checkers on micro-computers , good compression algorithms are essential. Current techniques used to compress lexicons for indo-Europcan languages provide efficient spelling checker. Applying the same methods to languages which have a different morphological system (Arabic, Turkish,...) gives insufficient results . To get better results , we apply other ""logical"" compression mechanisms based on the structure of the language itself. Experiments with multilingual dictionaries show a significant reduction rate attributable to our logic compression alone and even better results when using our method in conjunction with existing methods . KEY WORDS : "







CLAWS4: The Tagging Of The British National Corpus

The main purpose of this paper is to describe the CLAWS4 general-purpose grammatical tagger, used for the tagging of the 100- million-word British National Corpus , of which c.70 million words have been tagged at the time of writing (April  1994 ).tagsets input formats . output formats :






Thesaurus- Based Efficient Example Retrieval By Generating Retrieval Queries From Similarities


In example-based NLP, the problem of computational cost of example retrieval is severe , since the retrieval time increases in proportion to the number of examples in the database . This paper proposes a novel example retrieval method for avoiding full retrieval of examples . The proposed method has the following three features , 1) it generates






Discovering The Sounds Of Discourse Structure Extended Abstract

It is widely accepted that discourses are composed of segments and that the recognition of segment boundaries is essential to a determination of discourse meaning ( Grosz and  Sidner, 1986 ). Written language has orthographic cues such as section headings, paragraph boundaries , and punctuation which can assist in identifying discourse structure . In spoken language , into-national variation provides essential information about discourse structure . For instance , it may be used to mark structural features of discourse at the global level , such as segment boundaries . Intonation also provides more local information about relations among utterances within a segment , for example indicating whether phrases are parenthetical. It can also help distinguish between different interpretations of phrases that can function either as cue phrases that indicate discourse segment boundaries or sentcntially to convey domain information . Finally, variations in intonational prominence may be used to convey information about the discourse status of entities referred to by definite noun phrases and pronouns. An understanding of intonational variation and the ways in which it carries information about discourse characteristics of spoken language is important for computer-based interpretation and generation of speech . From the interpretation perspective , this understanding may provide new techniques for identifying discourse structure . From the generation perspective , it would lead to more natural synthetic speech , making it possible to produce computer speech that is easier for people to understand and less susceptible to misinterpretation. Three major challenges have faced researchers attempting to discover the relationship between intonational features and the structure of spoken discourse . First, the collection of corpora of spontaneous speech has required the development of * The research described in this presentation was supported by the National Science Foundation, Grant IRI 94-04756. The research has been done collaboratively with Julia Hirschberg and Christine Nakatani . David Ahn provided invaluable technical assistance. new experimental methodologies . Whereas it is straightforward to have the same text read by many speakers, it is much more difficult to obtain similar samples of spontaneous speech from multiple speakers. Second, techniques must be developed to obtain reliable segmentations and labelings of the corpora. Because discourse structure is rooted in semantics rather than syntax , this has proved more difficult than tagging corpora for sentence structure . Third, measures of agreement among segmentations must be designed . In this area too, the semantic nature of discourse structure leads to a more complex problem than comparing sentence parse structures . This talk will begin with a summary of pilot studies that demonstrated reliable correlations of discourse structure and intonational features ( Grosz and  Hirschberg, 1992 ; Hirschbcrg and  Grosz, 1992 ; Hirschbcrg and  Grosz, 1994 ). It will then focus on a new corpus of direction-giving monologues, the Boston Directions Corpus ( Nakatani et al., 1995a ; Hirschberg and  Nakatani, 1996 ). I will describe the methodology we developed to elicit fluent spontaneous direction-giving monologues ranging over a spectrum of planning complexity . Next I will describe the development of annotation instructions used to train labelers to segment spoken discourses ( Nakatani et ah, 1995b) and will discuss agreement among segmentations on the Boston Directions Corpus obtained using these instructions . Then I will describe results of our analyses of the correlation between discourse structure and intonational features . Finally, I will present a list of challenges for future research in this area .






Parsing Spoken Language Without Syntax 
Parsing spontaneous speech is a difficult task because of the ungrammatical nature of most spoken utterances . To overpass this problem , we propose in this paper to handle the spoken language without considering syntax . We describe thus a microsemantic parser which is uniquely based on an associative network of semantic priming. Experimental results on spontaneous speech show that this parser stands for a robust alternative to standard ones.






Processing Word Order Variation Within A Modified ID/LP Framework

"From a ""well represented sample of world languages Steele (1978) shows that about "









A Compositional Approach To The Translation Of Temporal Expressions In The ROSETTA System

This paper discusses the translation of temporal expressions , in the framework of the machine translation system Rosetta. The translation method of Rosetta, the 'isomorphic grammar method ', is based on Montague 's Compositionality Principle . It is shown that a compositional approach leads to a transparent account of the complex aspects of time in natural language and can be used for the translation of temporal expressions .







Pragmatics In Machine Translation

TEXAN is a system of transferi-oriented text analysis . Its linguistic concept is based on a communicative approach within the framework of speech act theory . In this view texts are considered to be the result of linguistic actions . It is assumed that they control the selection of translation equivalents. The transition of this concept of linguistic actions ( text acts) to the model of computer analysis is performed by a context-free il locution grammar processing categories of actions and a propositional structure of states of affairs. The grammar which is related to a text lexicon provides the connection of these categories and the linguistic surface units of a single language .







From Structure To Process Computer- Assisted Teaching Of Various Strategies For Generating Pronoun Constructions In French


This paper describes an implemented tutoring system (2), designed to help students to generate clitic-constructions in French. While showing various ways of converting a given meaning structure into its corresponding surface expression , the system helps not only to discover what







Divided And Valency- Oriented Parsing In Speech Undstanding


A parsing scheme for spoken utterances is proposed that deviates from traditional 'one go' left to right sentence parsing in that it d
vides the parsing process first into two aeperate parallel processes . Verbal constituents and nominal phrases ( including prepositonal phrases ) are treated seperately and only brought together in an utterance parser . This allows especially the utterance parser to draw on valency information right from beginning when amalgamating the nominal constituents to the verbal core by means of binary sentence rules . The paper also discusses problems of representing the valency information in case-frames arising in a spoken language environment .






Computational Phonology : Merged, Not Mixed

Research into text-to-speech systems has become a rather important topic in the areas of linguistics and phonetics. Particularly for English , several text-to-speech systems have been established (cf. for example Hertz (1982), Klatt (1976)). For Dutch, text-to-speech systems are being developed at the University of Nijmegen (cf. Wester (1984)) and at the Universities of Utrecht and Leyden and the Institute of Perception Research (IPO) Eindhoven as well. In this paper we will be concerned with the grapheme-to-phoneme conversion component as part of the Dutch text-to-speech system which is being developed in Utrecht, Leyden and Eindhoven. One of our primary interests is that the grapheme-to-phoneme system not only has to generate the input for speech synthesis , either in allophone or diphone form , but that it had to be used for other purposes as well. Thus, the system has to satisfy the following demands: - its output must form a proper and flexible input for diphone as well as allophone synthesis ; - it must be possible to easily generate phonematized lists on the basis of orthographic input ; - it must be possible to automatically obtain information regarding the relation between graphemes and phonemes in texts ; - the system has to be user-friendly , so that it can be addressed by linguists without computer training (for example to test their phonological rules ). In our view, there are two aspects to a grapheme-to-phoneme conversion system : a linguistic and a computational one. The linguist , in fact, provides the grammar necessary for the conversion and the engineer implements this grammar into a computer system . Thus, knowledge about spelling and linguistics are separated







Expressing Quantifier Scope In French Generation

In this paper we propose a new method to express quantification and especially quantifier scope in French generation . Our approach is based on two points: the identification of the sentence components between which quantifier scope can indeed be expressed and a mechanism to reinforce the expression of quantifier scope . This approach is being integrated in a written French generator , called Herm
s, which will become the generator of a portable natural language interface .






Constituent Coordination In Lexical- Functional Grammar


"Abstract : This paper outlines a theory of constituent coordination for Lexical- Functional Grammar. On this theory LFG's flat, unstructured sets arc used as the functional representation of coordinate constructions . Function-application is extended to sets by treating a sot formally as the generalization of its functional elements. This causes properties attributed externally to a coordinate structure to be uniformly distributed across its elements, without requiring additional grammatical specifications . Introduction A proper treatment of coordination has long been an elusive goal of both theoretical and computational approaches to language . The original transformational formulation in terms of the Coordinate Reduction rule (e.g. / Dougherty 1970 /) was quickly shown to have many theoretical and empirical inadequacies, and only recently have linguistic theories (e g, GPSG / Gazdar et al. 1985 /, Categorial grammar (e.g. / Steedman 1985 /) made substantial progress on characterizing the complex restrictions on coordinate constructions and also on their semantic interpretations . Coordination has also presented descriptive problems for computational approaches. Typically these have been solved by special devices that are added to the parsing algorithms to analyze coordinate constructions that cannot easily be characterized in explicit rules of grammar. The best known examples of this kind of approach are SYSCONJ / Woods 1973 /, ESP / Sager 1981 /, and MSG / Dahl and  McCord 1983 /. Coordination phenomena are usually divided into two classes , the so-called constituent coordinations where the coordinated elements look like otherwise well-motivated phrasal constituents II), and nonconstituent coordination where the coordinated elements look like fragments of phrasal constituents (2). (1) (a)  A girl saw Mary and ran to Iiill. (Coordinated verb phrases ) (b)  A girl saw and hoard Mary . (Coordinated verbs ) (2) Iiill went to Chicago on Wednesday and New York on Thursday. Of course, what is or is not a well-motivated constituent depends on the details of the particular grammatical theory . Constituents in transformationally-oriented theories , for example , are units that simplify the feeding relations of transformational rules , whereas ""constituents "" in categorial grammars merely reflect the order of binary combinations and have no other special motivation . In lexical-functional grammar, surface constituents are taken to be the units of phonological interpretation . These may differ markedly from the units of functional or semantic interpretation , as shown in the analysis of Dutch cross serial dependencies given by/ Bresnan et al. 1982 /. N'onconstituent coordination , of course, presents a wide variety of complex and difficult descriptive problems , but constituent coordination also raises important linguistic issues . It is the latter that we focus on in this brief paper . To a first approximation , constituent coordinations can be analyzed as the result of taking two independent clauses and factoring out their common subparts. The verb coordination in (lb) is thus related to the fuller sentence coordination in (3). This intuition , which was the basis of the Coordinate Reduction Transformation , accounts for more complex patterns of acceptability such as (4) illustrates. The coordination in (4c) is acceptable because both (4a) and (4b) are, while (4e) is bad because of the independent subcategorization violation in (4d). (3) A girl saw Mary and a girl heard Mary . (4) (a) A gir l dedicated a pie to Bill . (b) A girl gave a pie to Bill . (c) A girl dedicated and gave a pie to Bill . (d) *A girl ate a pie to Bill . (e) *A girl dedicated and ate a pie to Bill . This first approximation is frought with difficulties . It ensures that constituents of like categories can be conjoined only if they share some finer details of specification , but there are more subtle conditions that it does not cover. For example , even though (5a) and (5b) are both independently grammatical, the coordination in (5c) is unacceptable: (5) (a) The girl promised John to go. (b) The girl persuaded John to go. (c) ""The girl promised and persuaded John to go, (Hint: Who is going"







An Integrated Model For The Treatment Of Time In MT-Systems

One of the ways to achieve a good translation of verbal foras is the morphosyntactic approach , which consists in a function pairing the different morphological tenses that occur in a given language with the tenses of the other language . Complicated rules must be established to calculate the right pair for an expression , because of the amount of discrepancies that different languages show with respect to each other. 
he way wa have chosen to deal with this problem is, conversely, the projection of the different values coming from verbs ( type , processivity, morftense, morfaspect, sioodrequirement), from adverbs , prepositional phrases and temporal M.?s (deixis, aspect , iteration ), and from subordinate conjunctions ( aspect , moodrequirement). All this information permits to obtain a final value for aspect and tense for the whole sentence , which later on is percolated, not only to the verb node , but also to the the rest of elements conveying information . Our proposal relies on the fact that tense/ aspect calculation is relevant not only for a good translation of verbs , but also for a good translation of adverbs , PPs, temporal Ni-n and conjunctions , as we have intended to demonstrate in this paper . I. Introduction Thin article deals with a methodology to achieve the right translation of temporal expressions by giving account of the temporal reference and temporal relations in/ between sentences . The task to accomplish is to translate syntactic marks into semantic values that decide/ reflect the aspectunl value of the sentences . For our treatment of time and aspect we draw on the work of Kamp [1979] and Partee [1984] who have argued for taking status and events as primitives and relations of precedence und overlapping between them. The ordering relation between events is crucial for deciding about the aspect of the sentences involved. The present proposal presumes an analysis and a generation component that deliver a set of S- trees whose leaves correspond to words . The pre-terminals have morphosyntatitic and relational information . As usual, features am percolated and nodes get features assigned . She tine/ aspectual problem ia dealt with under the perspectiv
 of MT with the aim of sketching a system that can be implemented independently of the particular formalisms of different MT-systems . To outline a general model for the time / aspect calculation in MT we subsume a system with PSG rules that obtain some sentence structure with no regard to a specific grammar type ; it could be an augmented PSG, as in METAL, or some kind of deep syntactic structure , as it is the case in Eurotra. The problem is the well known fact that translations of temporal expressions in Ni does not involve a simple mapping of tenses and adverbials. We could just compare Spanish, rich in aspect and tenses vs. German or English . That is, a MT dealing with Germanic and Romance languages is concerned with different parameters for each language ; the whole practice in MT systems is to translate morphological tenses, and syntactical values into reference times that include events or slates







Improved Sentence Alignment on Parallel Web Pages Using a Stochastic Tree Alignment Model

"Parallel web pages are important source of training data for statistical machine translation . In this paper , we present a new approach to sentence alignment on parallel web pages . Parallel web pages tend to have parallel structures , and the structural correspondence can be indicative information for identifying parallel sentences . In our approach , the web page is represented as a tree , and a stochastic tree alignment model is used to exploit the structural correspondence for sentence alignment . Experiments show that this method significantly enhances alignment accuracy and robustness for parallel web pages which are much more diverse and noisy than standard parallel corpora such as ""Hansard"". With improved sentence alignment performance , web mining systems are able to acquire parallel sentences of higher quality from the web. "







A Japanese Predicate Argument Structure Analysis using Decision Lists


This paper describes a new automatic method for Japanese predicate argument structure analysis . The method learns relevant features to assign case roles to the argument of the target predicate using the features of the words located closest to the target predicate under various constraints such as dependency types , words , semantic categories , parts of speech , functional words and predicate voices. We constructed decision lists in which these features were sorted by their learned weights . Using our method , we integrated the tasks of semantic role labeling and zero-pronoun identification , and achieved a 17% improvement compared with a baseline method in a sentence level performance analysis .







Joint Unsupervised Coreference Resolution with Markov Logic 
Machine learning approaches to coreference resolution are typically supervised, and require expensive labeled data . Some unsuper-vised approaches have been proposed (e.g., Haghighi and Klein (2007)), but they are less accurate. In this paper , we present the first un-supervised approach that is competitive with supervised ones. This is made possible by performing joint inference across mentions , in contrast to the pairwise classification typically used in supervised methods , and by using Markov logic as a representation language , which enables us to easily express relations like apposition and predicate nominals. On MUC and ACE datasets, our model outperforms Haghigi and Klein 's one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models .







The GENIA Project : Corpus- Based Knowledge Acquisition And Information Extraction From Genome Research Papers

We present an outline of the genome information acquisition (GENIA) project for automatically extracting biochemical information from journal papers and abstracts . GENIA will be available over the Internet and is designed to aid in information extraction , retrieval and visualisation and to help reduce information overload on researchers . The vast repository of papers available online in databases such as MEDLINE is a natural environment in which to develop language engineering methods and tools and is an opportunity to show how language engineering can play a key role on the Internet.







Interactive Word Alignment For Language Engineering

In this paper we report ongoing work on developing an interactive word alignment environment that will assist a user to quickly produce accurate full-coverage word alignment in bitexts for different language engineering tasks , such as MT lexicons and gold standards for evaluation . The system uses a graphical interface , static and dynamic resources as well as machine learning techniques . We also sketch how the system is being integrated with an automatic word aligner.







Re- Evaluation The Role Of Bleu In Machine Translation Research

We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric . We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality , and give two significant counterexamples to Bleu 's correlation with human judgments of quality . This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.







Automatically Constructing A Lexicon Of Verb Phrase Idiomatic Combinations


We investigate the lexical and syntactic flexibility of a class of idiomatic expressions . We develop measures that draw on such linguistic properties , and demonstrate that these statistical , corpus-based measures can be successfully used for distinguishing idiomatic combinations from non-idiomatic ones. We also propose a means for automatically determining which syntactic forms a particular idiom can appear in, and hence should be included in its lexical representation .









An Education And Research Tool For Computational Semantics

This paper describes an interactive graphical environment for computational semantics . The system provides a teaching tool , a stand alone extendible grapher, and a library of algorithms together with test suites . The teaching tool allows users to work step by step through derivations of semantic representations , and to compare the properties of various semantic formalisms such as Intensional Logic , DRT, and Situation Semantics . The system is freely available on the Internet.







The Effects Of Word Order And Segmentation On Translation Retrieval Performance

This research looks at the effects of word order and segmentation on translation retrieval performance for an experimental Japanese- English translation memory system . We implement a number of both bag-of-words and word order-sensitive similarity metrics , and test each over character-based and word-based indexing . The translation retrieval performance of each system configuration is evaluated empirically through the notion of word edit distance between translation candidate outputs and the model translation . Our results indicate that character-based indexing is consistently superior to word-based indexing , suggesting that segmentation is an unnecessary luxury in the given domain . Word order-sensitive approaches are demonstrated to generally outperform bag-of-words methods , with source language segment-level edit distance proving the most effective similarity metric .







Reusing An Ontology To Generate Numeral Classifiers


In this paper , we present a solution to the problem of generating Japanese numeral classifiers using semantic classes from an ontology . Most nouns must take a numeral classifier when they are quantified in languages such as Chinese , Japanese , Korean, Malay and Thai. In order to select an appropriate classifier , we propose an algorithm which associates classifiers with semantic classes and uses inheritance to list only those classifiers which have to be listed . It generates sortal classifiers with an accuracy of 81%. We reuse the ontology provided by Goi-Taikei  a Japanese lexicon , and show that it is a reasonable choice for this task , requiring information to be entered for less than 6% of individual nouns .







Aspects Of Pattern- Matching In Data- Oriented Parsing 
Data- Oriented Parsing (dop) ranks among the best parsing schemes , pairing state-of-the art parsing accuracy to the psycholinguistic insight that larger chunks of syntactic structures are relevant grammatical and probabilistic units . Parsing with the dop-model , however, seems to involve a lot of CPU cycles and a considerable amount of double work, brought on by the concept of multiple derivations , which is necessary for probabilistic processing , but which is not convincingly related to a proper linguistic backbone . It is however possible to reinterpret the dop-model as a pattern-matching model , which tries to maximize the size of the substructures that construct the parse , rather than the probability of the parse . By emphasizing this memory-based aspect of the dop-model , it is possible to do away with multiple derivations , opening up possibilities for efficient Viterbi-style optimizations , while still retaining acceptable parsing accuracy through enhanced context-sensitivity .







A Hybrid Japanese Parser With Hand- Crafted Grammar And Statistics

This paper describes a hybrid parsing method for Japanese which uses both a hand-crafted grammar and a statistical technique . The key feature of our system is that in order to estimate likelihood for a parse tree , the system uses information taken from alternative partial parse trees generated by the grammar. This utilization of alternative trees enables us to construct a new statistical model called Triplet/Quadruplet Model .We






Learning Semantic- Level Information Extraction Rules By Type- Oriented ILP


This paper describes an approach to using semantic representations information extraction (IE) inductive logic programming (ILP)






Robust German Noun Chunking With A Probabilistic Context- Free Grammar


We present a noun chunker for German which is based on a head-lexicalised probabilistic context-free grammar. A manually developed grammar was semi-automatically extended with robustness rules in order to allow parsing of unrestricted text . The model parameters were learned from unlabelled training data by a probabilistic context-free parser . For extracting noun chunks , the parser generates all possible noun chunk analyses , scores them with a novel algorithm which maximizes the best chunk sequence criterion , and chooses the most probable chunk sequence . An evaluation of the chunker on 2,140 hand-annotated noun chunks yielded 92% recall and 93% precision .







The Week At A Glance - Cross- Language Cross- Document Information Extraction And Translation

"Work on the production of texts in English describing instances of a particular event type from multiple news sources will be described. A system has been developed which extracts events , such as meetings , from texts in English , Russian, Spanish, and Japanese . The extraction is currently carried out using only ontological information . The results of a set of such extractions were combined to produce a table of event instances , date stamped, with links back to the original documents . The original documents can then be summarized and translated by the system on demand. By using techniques from information retrieval , information extraction , summarization , and machine translation , in a multi-lingual environment , new documents can be produced which provide ""at a glance"" access to news on events from multiple sources . The paper concludes with a discussion of the key resources which need to be developed to enhance the accuracy and coverage of the techniques used in our experiment . "







Querying Temporal Databases Using Controlled Natural Language

Recent years have shown a surge in interest in temporal database systems , which allow users to store time-dependent information . We present a novel controlled natural language interface to temporal databases , based on translating natural language questions into SQL/Temporal, a temporal database query language . The syntactic analysis is done using the Type- Logical Grammar framework , highlighting its utility not only as a theoretical framework but also as a practical tool . The semantic analysis is done using a novel theory of the semantics of temporal questions , focusing on the role of temporal preposition phrases rather than the more traditional focus on tense and aspect . Our translation method is considerably simpler than previous attempts in this direction . We present a prototype software implementation .






Learning Chinese Bracketing Knowledge Based On A Bilingual Language Model

This paper proposes a new method for automatic acquisition of Chinese bracketing knowledge from English- Chinese sentence-aligned bilingual corpora . Bilingual sentence pairs are first aligned in syntactic structure by combining English parse trees with a statistical bilingual language model . Chinese bracketing knowledge is then extracted automatically. The preliminary experiments show automatically learned knowledge accords well with manually annotated brackets . The proposed method is particularly useful to acquire bracketing knowledge for a less studied language that lacks tools and resources found in a second language more studied . Although this paper discusses experiments with Chinese and English , the method is also applicable to other language pairs .







The Computation Of Word Associations: Comparing Syntagmatic And Paradigmatic Approaches

It is shown that basic language processes such as the production of free word associations and the generation of synonyms can be simulated using statistical models that analyze the distribution of words in large text corpora . According to the law of association by contiguity, the acquisition of word associations can be explained by Hebbian learning. The free word associations as produced by subjects on presentation of single stimulus words can thus be predicted by applying first-order statistics to the frequencies of word co-occurrences as observed in texts . The generation of synonyms can also be conducted on co-occurrence data but requires second-order statistics . The reason is that synonyms rarely occur together but appear in similar lexical neighborhoods. Both approaches are systematically compared and are validated on empirical data . It turns out that for both tasks the performance of the statistical system is comparable to the performance of human subjects .







A Chart- Parsing Algorithm For Efficient Semantic Analysis

"In some contexts , well-formed natural language cannot be expected as input to information or communication systems . In these contexts , the use of grammar-independent input ( sequences of uninflected semantic units like e.g. language-independent icons ) can be an answer to the users ' needs. However, this requires that an intelligent system should be able to interpret this input with reasonable accuracy and in reasonable time . Here we propose a method allowing a purely semantic-based analysis of sequences of semantic units . It uses an algorithm inspired by the idea of ""chart parsing "" known in Natural Language Processing , which stores intermediate parsing results in order to bring the calculation time down. "







A Generative Probability Model For Unification- Based Grammars


A generative probability model for unification-based grammars is presented in which rule probabilities depend on the feature structure of the expanded constituent . The presented model is the first model which requires no normalization and allows the application of dynamic programming algorithms for disambiguation ( Viterbi ) and training (Inside-Outside). Another advantage is the small number of parameters .







Augmenting Noun Taxonomies By Combining Lexical Similarity Metrics


This paper presents a method for augmenting taxonomies with domain information using a simple combination of three existing lexical similarity metrics . The combined approach is evaluated by comparing their results against the annotated SEMCOR corpus . An implementation is described in which WordNet is augmented with thesaural information from the CIDE+ machine readable dictionary .







A Method Of Cluster- Based Indexing Of Textual Data

This paper presents a framework for clustering in text-based information retrieval systems . The prominent feature of the proposed method is that documents , terms , and other related elements of textual information are clustered simultaneously into small overlapping clusters . In the paper , the mathematical formulation and implementation of the clustering method are briefly introduced, together with some experimental results .






Processing Japanese Self- Correction In Speech Dialog Systems 
Speech dialog systems need to deal with various kinds of ill-formed speech inputs that appear in natural human-human dialog . Self-correction (or speech-repair ) is a particularly problematic phenomenon . Although many ways of dealing with self-correction have been proposed , these have limitations in both detecting and correcting for this phenomenon . In this paper , we propose a method to overcome these problems in Japanese speech dialog . We evaluate the proposed method using our speech dialog corpus and discuss its limitations and the work that remains to be done.







Fertilization Of Case Frame Dictionary For Robust Japanese Case Analysis

This paper proposes a method of fertilizing a Japanese case frame dictionary to handle complicated expressions : double nominative sentences , non-gapping relation of relative clauses , and case change. Our method is divided into two stages. In the first stage, we parse a large corpus and construct a Japanese case frame dictionary automatically from the parse results . In the second stage, we apply case analysis to the large corpus utilizing the constructed case frame dictionary , and upgrade the case frame dictionary by incorporating newly acquired information .







Open- Domain Voice-Activated Question Answering

Voice-Activated Question Answering (VAQA) systems represent the next generation capability for universal access by integrating state-of-the-art in question answering Q&amp;A and automatic speech recognition (ASR) in such a way that the performance of the combined system is better than the individual components . This paper presents an implemented VAQA system and describes the techniques that enable the terative refinement of both Q&amp;A and ASR. The results of our experiments show that spoken questions can be processed with surprising accuracy when using our VAQA implementation .






Generating Discourse Structures For Written Text

This paper presents a system for automatically generating discourse structures from written text . The system is divided into two levels : sentence-level and text-level . The sentence-level discourse parser uses syntactic information and cue phrases to segment sentences into elementary discourse units and to generate discourse structures of sentences . At the text-level , constraints about textual adjacency and textual organization are integrated in a beam search in order to generate best discourse structures . The experiments were done with documents from the RST Discourse Treebank. It shows promising results in a reasonable search space compared to the discourse trees generated by human analysts .







Improving Japanese Zero Pronoun Resolution By Global Word Sense Disambiguation

This paper proposes unsupervised word sense disambiguation based on automatically constructed case frames and its incorporation into our zero pronoun resolution system . The word sense disambiguation is applied to verbs and nouns . We consider that case frames define verb senses and semantic features in a thesaurus define noun senses, respectively, and perform sense disambiguation by selecting them based on case analysis . In addition , according to the one sense per discourse heuristic, the word sense disambiguation results are cached and applied globally to the subsequent words . We integrated this global word sense disambiguation into our zero pronoun resolution system , and conducted experiments of zero pronoun resolution on two different domain corpora . Both of the experimental results indicated the effectiveness of our approach .







BiFrameNet: Bilingual Frame Semantics Resource Construction By Cross- Lingual Induction

"We present a novel automatic approach to constructing a bilingual semantic network
the BiFrameNet, to enhance statistical and transfer-based machine translation systems . BiFrameNet is a frame semantic representation , and contains semantic structure transfers between English and Chinese . The English FrameNet and the Chinese HowNet provide us with two different views of the semantic distribution of lexicon by linguists . We propose to induce the mapping between the English lexical entries in FrameNet to Chinese word senses in HowNet, furnishing a bilingual semantic lexicon which simulates the ""concept lexicon "" supposedly used by human translators , and which can thus be beneficial to machine translation systems . BiFrameNet also contains bilingual example sentences that have the same semantic roles . We automatically induce Chinese example sentences and their semantic roles , based on semantic structure alignment from the first stage of our work, as well as shallow syntactic structure . In addition to its utility for machine-aided and machine translations , our work is also related to the spatial models proposed by cognitive scientists in the framework of artifactual simulations of the translation process . "







A Computational Theory Of Prose Style For Natural Language Generation

"1. Where in the generation process style is taken into account. 2. How a particular prose style is represented; what ""stylistic rules "" look like; 3. What modifications to a generation algorithm are needed; what the decision is that evaluates stylistic alternatives ; 4. What elaborations to the normal description of surface structure are necessary to make it usable as a plan for the text and a reference for these decisions ; 5. What kinds of information decisions about style have access to. Our theory emerged out of design experiments we have made over the past year with our natural language generation system , the Zetalisp program MUMBLE. In the process we have extended MUMBLE through the addition of an additional process that now mediates between content planning and linguistic realization . This new process , which we call ""attachment "", provides the further significant benefit that text structure is no longer dictated by the structure of the message : the sequential order and dominance relationships of concepts in the message no longer force one form onto the words and phrases in the text . Instead, rhetorical and intentional directives can be interpreted flexibly in the context of the ongoing discourse and stylistic preferences . The text is built up through composition under the direction of linguistic organizing principles , rather than having to follow conceptual principles in lockstep. We will begin by describing what we mean by prose style and then introducing the generation task that lead us to this theory , the reproduction of short encyclopedia articles on African tribes. We will then use that task to outline the parts of our theory and the operations of the attachment process . Finally we will compare our techniques to the related work of Davey , McKeown and Derr , and Gabriel , and consider some of the possible psycholinguistic hypotheses that it may lead to. "






Prediction In Chart Parsing Algorithms For Categorial Unification Grammar

Natural language systems based on Categorial Unification Grammar (CUG) have mainly employed bottom-up parsing algorithms for processing . Conventional prediction techniques to improve the efficiency of the parsing process , appear to fall short when parsing CUG. Nevertheless, prediction seems necessary when parsing grammars with highly ambiguous lexicons or with non-canonical categorial rules . In this paper we present a lexicalist prediction technique for CUG and show that this may lead to considerable gains in efficiency for both bottom-up and top-down parsing .






